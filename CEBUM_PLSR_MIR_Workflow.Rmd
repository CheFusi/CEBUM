---
title: "CEBUM_PLSR_Workflow"
output: html_document
date: "2024-10-14"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries

```{r, include=FALSE}

# Define a list of required packages
required_packages <- c("writexl","readxl","randomForest", "caret", "pls", "data.table", "ithir", 
                       "dplyr", "tidyr", "prospectr", "globals", "stringr", 
                       "ggplot2", "here", "tidymodels", "parsnip", "plsmod", 
                       "mixOmics", "yardstick", "purrr", "furrr", "tibble","ranger","VIM","shiny","FNN","lightgbm", "xgboost", "dials", "tune", "brnn", "recipes")

#VIM for KNN imputation
# Load all required packages quietly without outputting any list of packages
suppressPackageStartupMessages(
  invisible(lapply(required_packages, library, character.only = TRUE))
)

# Ensure dplyr functions are prioritized
conflicted::conflict_prefer("select", "dplyr")
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflicts_prefer(purrr::map)
conflicted::conflicts_prefer(tune::tune)

plan(multisession)
```

## Load reference data

```{r}

# treated spectra
spec_trt <- readRDS("spec_trt_MIR.RDS")
spec_trt_MIR_lab <- readRDS("spec_trt_MIR_lab.RDS")
spec_trt_NIR <- readRDS("/Users/soliverchefusi/Library/CloudStorage/OneDrive-Personal/R/ICRAF&Lehmann/ICRAF_IR/ICRAF_IR_R_Project/ICRAF_IR/CEBUM/CEBUM_NIR/CEBUM_NIR/spec_trt_NIR.RDS")
spec_trt_NIR_lab <- readRDS("/Users/soliverchefusi/Library/CloudStorage/OneDrive-Personal/R/ICRAF&Lehmann/ICRAF_IR/ICRAF_IR_R_Project/ICRAF_IR/CEBUM/CEBUM_NIR/CEBUM_NIR/spec_trt_NIR_lab.RDS")

spec_trt <- spec_trt[,-2] #removing the data for the first wavenumber since it's all NAN's
spec_trt_MIR_lab <- spec_trt_MIR_lab[,-2]

spec_trt_MIR_NIR <- merge(spec_trt,spec_trt_NIR, by ="SSN")
spec_trt_MIR_NIR_lab <- merge(spec_trt_MIR_lab,spec_trt_NIR_lab, by ="SSN")
rownames(spec_trt_MIR_NIR_lab) <- spec_trt_MIR_NIR_lab$SSN

# biochar property data
#df1<-readRDS("df1.RDS")
df1<-readRDS("df.f.RDS")

# df.f: full dataset;
# spec_trt: treated spectra

#Available properties to predict
slprptr<-names(df1[-c(2:9)]) #FUSI EDIT: removing metadata columns 

# defining identity object with names of biochar samples
pred<-as.data.frame(spec_trt[,1])
colnames(pred)<-"SSN"
 
```

## PLSR, RF, GBDT, BRNN Model Workflow

Models based on minimizing RMSEP (objective function)

High-Level Structure:

```         
1.  Preprocessing

2.  Setting up models 

3.  Running models (map() over models to fit them)

4.  Calculating metrics (for each model)

5.  Plotting results (for each model)
```

### Initializing Repository for Outputs

```{r}

# Initialize the workspace by creating directories for storing outputs and 
# setting up an empty tibble to accumulate performance metrics 
initialize_workspace <- function(identifier) {
  #dir.create(paste0("Plots_Validation_MIR_", identifier), showWarnings = FALSE)
  dir.create(paste0("Plots_WellFitted_Properties_", identifier), showWarnings = FALSE)
  all_metrics <<- tibble()  # Global tibble to store metrics for each property
  
  return(identifier)  # Pass the identifier for later use
}
```

### Selecting Property to then iterate over

```{r}

# slprptr: list of properties; .x: index of property in slprptr
select_property_data <- function(slprptr, .x, df1, spec_trt) {
  # Filter df1 to create df.f based on SSN present in spec_trt
    df.f <- df1 %>%
    filter(SSN %in% spec_trt$SSN) # Select only samples present in both predictor and response datasets
  # Check if the property exists in df1
  if (!slprptr[.x] %in% colnames(df.f)) {
    stop(paste("Property", slprptr[.x], "not found in df1"))
  }
  
  # Columns to select, ensuring required columns are present
  cols_to_select <- c("SSN", "Source", slprptr[.x])
  cols_to_select <- intersect(cols_to_select, colnames(df.f))
  
  if (!all(cols_to_select %in% colnames(df.f))) {
    stop(paste("Columns", paste(cols_to_select, collapse = ", "), "not found in df.f"))
  }
  
  # Filter and preprocess data, including removing outliers for the selected property
    df.sel <- df.f %>%
    dplyr::select(dplyr::all_of(cols_to_select)) %>%
    inner_join(spec_trt, by = "SSN") %>%   # Combine predictor and response variables into one df
    na.omit()#%>%
    #filter(data.table::between(!!sym(slprptr[.x]), quantile(!!sym(slprptr[.x]), 0.01), quantile(!!sym(slprptr[.x]), 0.99))) # Filter out extremes
  
  # Set SSN as rownames but keep the column
  rownames(df.sel) <- df.sel$SSN
  # Check if df.sel has rows
  if (nrow(df.sel) == 0) {
    stop(paste("No matching rows in df.sel for property:", slprptr[.x]))
  }
  
  return(list(df.f = df.f, df.sel = df.sel))
}
```

### Defining Training/Test Split

### Bootstrapped Training/Test Split

```{r}

generate_bootstrap_splits <- function(df.sel, num_bootstrap, use_source = FALSE, source_name = NULL) {
  set.seed(123)

  # If a specific lab is selected, filter data
  if (!is.null(source_name)) {
    df.sel <- df.sel %>% filter(Source == source_name)
  }

  # Ensure we have enough samples
    # If there aren't enough samples, return NULL instead of stopping
  if (nrow(df.sel) < 20) {
    message("Skipping property due to insufficient data after filtering.")
    return(NULL)
  }

  # Generate bootstrap resamples with or without stratification
  if (use_source) {
    boot_splits <- bootstraps(df.sel, times = num_bootstrap, strata = "Source")
  } else {
    boot_splits <- bootstraps(df.sel, times = num_bootstrap)
  }

  return(boot_splits)
}

# Function to split bootstrap resample into train/test
split_bootstrap_sample <- function(boot_sample) {
  train_data <- analysis(boot_sample)
  test_data <- assessment(boot_sample)
  
  #Remove Source column before model training
  train_data <- train_data %>% dplyr::select(-Source)
  test_data <- test_data %>% dplyr::select(-Source)
  
  return(list(train_data = train_data, test_data = test_data))
}
```

**NOTE: prepare_model_data and preprocess_data functions are a bit redundant ... consider combining one you solidfy script**

```{r}

prepare_model_data <- function(boot_data, slprptr, .x) {
  # Map over all bootstrap splits to prepare train/test data for the specific property
  #processed_splits <- map(boot_data, function(split) {
  safe_map <- function(x, f) {
  tryCatch(map(x, f), error = function(e) {
    message("Error in map() inside prepare_model_data(): ", e$message)
    stop(e)
  })
}

processed_splits <- safe_map(boot_data, function(split) {
  
    train_data <- split$train_data # split refers to the index [[i]]
    test_data <- split$test_data
 
    # Ensure the property exists in the dataset
    if (!slprptr[.x] %in% colnames(train_data)) {
      return(list(train_data = NA, test_data = NA))  # Return NA instead of stopping or returning NULL
    }
# 
#     # Select relevant columns: SSN + response variable
#     selected_cols <- c("SSN", slprptr[.x])
#     train_data <- train_data %>% dplyr::select(all_of(selected_cols)) %>% na.omit()
#     test_data <- test_data %>% dplyr::select(all_of(selected_cols)) %>% na.omit()

    # Ensure we have enough valid data after selection
    if (nrow(train_data) <= 10 || nrow(test_data) <= 10) {
      return(list(train_data = NA, test_data = NA))  # Maintain NA return instead of NULL
    }
    
    # Append row number to SSN with three-digit formatting
    train_data <- train_data %>%
      dplyr::mutate(SSN = paste0(SSN, "_", sprintf("%03d", row_number())))
    
    test_data <- test_data %>%
      dplyr::mutate(SSN = paste0(SSN, "_", sprintf("%03d", row_number())))


    # Preserve SSN as rownames
    rownames(train_data) <- train_data$SSN
    rownames(test_data) <- test_data$SSN

    return(list(train_data = train_data, test_data = test_data))
  })

  # # Remove invalid splits (but keep as NA, not NULL)
  # valid_splits <- map(processed_splits, function(split) {
  #   if (is.na(split$train_data) || is.na(split$test_data)) {
  #     return(list(train_data = NA, test_data = NA))
  #   } else {
  #     return(split)
  #   }
  # })
  
  valid_splits <- compact(processed_splits)

  return(valid_splits)  # Return list of processed train-test pairs, ensuring all outputs are NA-safe
}
```

### Prepping Model/ Defining Recipe

```{r}

preprocess_data <- function(valid_splits, slprptr, p, #recipe_steps = NULL
                            include_zv = FALSE,
                            include_pca = FALSE,
                            #include_pls = FALSE,
                            #include_rf = FALSE,
                            num_pca_comp = NULL 
                            #variance_threshold = NULL,
                            #variance_threshold_pls = NULL,
                            #forest_threshold = NULL) 
                            ){

    # Use safe_map() to prevent failures in mapping
    safe_map <- function(x, f) {
        tryCatch(map(x, f), error = function(e) {
            message("Error in map() inside preprocess_data(): ", e$message)
            stop(e)
        })
    }

    processed_splits <- safe_map(valid_splits, function(split) {
        
        # Ensure train and test data exist
        if (!"train_data" %in% names(split) || !"test_data" %in% names(split)) {
            message("🚨 Warning: Missing train/test data in this split!")
            return(NULL)  # Explicitly return NULL so we can track it
        }
        
        train_data <- split$train_data
        test_data <- split$test_data
        

        if (nrow(train_data) == 0 || nrow(test_data) == 0) {
            warning("⚠️ Skipping split due to empty train/test data.")
            return(NULL)
        }


        formula <- as.formula(paste(slprptr[p], "~ ."))
        
        recipe_base <- recipe(formula, data = train_data) %>%
            update_role(SSN, new_role = "ID")
        

        if (include_zv) {
            recipe_base <- recipe_base %>% step_zv() %>% step_nzv()
        }

        # # Initialize feature reduction variables
        n_comp_pca <- NA_integer_
        #n_comp_pls <- NA_integer_
        #n_features_rf <- NA_integer_

        # =========================== PCA ===========================
        if (include_pca) {
            if (!is.null(num_pca_comp)) {
                num_pca_comp_set <- min(
                    num_pca_comp,
                    ncol(train_data),
                    nrow(train_data) - 1,
                    max(10, floor(nrow(train_data) * 0.2))
                )
                    # *** Check whether the result is length zero
    if (length(num_pca_comp_set) == 0 || is.na(num_pca_comp_set)) {
      message("Skipping property due to inability to determine a valid PCA component count.")
      return(NULL)
    }
                if (num_pca_comp_set > (nrow(train_data) - 1) || num_pca_comp_set > (ncol(train_data) - 1)) {
                    message("Warning: num_pca_comp (", num_pca_comp, ") too high. Using fallback value of 5 components.")
                    num_pca_comp_set <- 5
                }

                recipe_base <- recipe_base %>%
                    step_pca(all_numeric_predictors(), num_comp = num_pca_comp_set)
                 # Wrap prep() in tryCatch so that if PCA fails, we return NULL
    prepped_recipe <- tryCatch({
      prep(recipe_base, training = train_data, retain = TRUE)
    }, error = function(e) {
      message("⚠️ PCA prep failed: ", e$message)
      return(NULL)
    })
    
    if (is.null(prepped_recipe)) {
      message("Skipping property due to PCA failure.")
      return(NULL)
    }
                n_comp_pca <- num_pca_comp_set
            } else if (!is.null(variance_threshold)) {
                recipe_base_pca_variance <- recipe_base %>%
                    step_pca(all_numeric_predictors(), threshold = variance_threshold)

                prepped_recipe <- prep(recipe_base_pca_variance, training = train_data, retain = TRUE)
                n_comp_pca_variance <- prepped_recipe$steps[[1]]$num_comp

                num_pca_comp_set <- max(
                    min(n_comp_pca_variance, ncol(train_data), nrow(train_data) - 1),
                    max(min(10, floor(nrow(train_data) * 0.2)), 2)
                )

                recipe_base <- recipe_base %>%
                    step_pca(all_numeric_predictors(), num_comp = num_pca_comp_set)

                n_comp_pca <- num_pca_comp_set
            } else {
                warning("PCA enabled but neither num_pca_comp nor variance_threshold provided. PCA not applied.")
            }
        }

        if (include_pca && (length(n_comp_pca) == 0 || is.na(n_comp_pca))) {
            stop("ERROR: n_comp_pca is NA despite PCA being enabled!")
        }

        # # =========================== PLS ===========================
        # if (include_pls) {
        #     if (!is.null(variance_threshold_pls)) {
        #         recipe_base <- recipe_base %>%
        #             step_pls(all_numeric_predictors(), outcome = slprptr[p])  
        #     } else {
        #         max_possible_comp <- min(ncol(train_data), nrow(train_data) - 1)
        #         temp_recipe <- recipe_base %>%
        #             step_pls(all_numeric_predictors(), outcome = slprptr[p], num_comp = max_possible_comp)
        # 
        #         prepped_temp <- prep(temp_recipe, training = train_data, retain = TRUE)
        #         pls_results <- prepped_temp$steps[[1]]$res$Xvar  
        #         cumulative_variance <- cumsum(pls_results) / sum(pls_results)
        # 
        #         n_comp_pls <- which(cumulative_variance >= variance_threshold_pls)[1]
        # 
        #         if (!is.na(n_comp_pls) && n_comp_pls < 2) {
        #             warning("PLS variance threshold too high, using fallback of 2 components.")
        #             n_comp_pls <- 2
        #         }
        # 
        #         recipe_base <- recipe_base %>%
        #             step_pls(all_numeric_predictors(), outcome = slprptr[p], num_comp = n_comp_pls)
        #     }
        # }
        # 
        # # =========================== Feature Selection (RF) ===========================
        # if (include_rf && !is.null(forest_threshold)) {
        #     message("Applying step_select_forests() with threshold: ", forest_threshold)
        #     recipe_base <- recipe_base %>% step_select_forests(all_numeric_predictors(), threshold = forest_threshold)
        # }

        return(list(
            recipe = recipe_base,
            train_data = train_data,
            test_data = test_data,
            n_comp_pca = n_comp_pca
            #n_comp_pls = n_comp_pls,
            #n_features_rf = n_features_rf
        ))
    })


    if (length(processed_splits) == 0 || all(map_lgl(processed_splits, is.null))) {
        stop("🚨 ERROR: All processed_splits are NULL!")
    }

    return(processed_splits)
}
```

### Tuning Models

```{r}

# # Set up PLS or Random Forest model specification, including tuning parameters
# setup_model <- function(model_type, preprocessed_splits, response_var = NULL) {
#   
#     # Extract train_data from preprocessed_splits
#   train_data_list <- preprocessed_splits$train_data
#   
# 
#   # Iterate over all bootstrap splits to set up models
#   models <- map(train_data_list, function(train_data) {
# 
#     # Skip if train_data is NA
#     if (all(is.na(train_data))) {
#       return(list(model_spec = NA, mtry_range = NA))
#     }
#   
#   
#   
#   if (model_type == "pls") {
#     # Configure a PLS model specification
#     model_spec <- parsnip::pls() %>%
#       set_mode("regression") %>%
#       set_engine("mixOmics") %>%
#       set_args(num_comp = tune())
#     mtry_range <- NULL
#   } else if (model_type == "rf") {
#     # Configure a Random Forest model with tuning for mtry and min_n
#     if (is.null(train_data)) stop("train_data must be provided for Random Forest.")
#     if (is.null(response_var)) stop("response_var must be specified for Random Forest.")
#     
#     # Remove SSN and response columns to get only predictor columns
#     predictors_only <- train_data %>%
#       dplyr::select(-SSN, -all_of(response_var))
#         # Check if predictors are sufficient
#     if (ncol(predictors_only) < 2) stop("Random Forest requires at least 2 predictor variables.")
#     
#     
#     # Finalize mtry_range
#     # mtry_range <- dials::finalize(dials::mtry(), predictors_only) # predictors_only is used to set the max value for the mtry range - so here mtry ranges up to the total number of predictors (e.g. number of wavenumbers).
#     mtry_range <- dials::mtry(range = c(2, round(sqrt(ncol(predictors_only)))))
#     
#     if (is.null(mtry_range)) stop("Failed to initialize mtry_range in setup_model(). Check predictor columns in train_data.")
#     
#     model_spec <- parsnip::rand_forest(mtry = tune(), min_n = tune()) %>%
#       set_mode("regression") %>%
#       set_engine("ranger")
#   } else if (model_type == "gbdt") {
#     # Gradient-Boosted Decision Tree setup
#     predictors_only <- train_data %>%
#       dplyr::select(-SSN, -all_of(response_var))
#     model_spec <- parsnip::boost_tree(
#       trees = tune(),  # Number of trees to tune
#       tree_depth = tune(),  # Depth of each tree
#       learn_rate = tune(),  # Learning rate
#       loss_reduction = tune(),  # Minimum gain for split
#       min_n = tune(),  # Tune minimal node size
#       #sample_size = tune(),  # Tune row sampling
#       mtry = tune()  # Tune predictor sampling
#     ) %>%
#       set_mode("regression") %>%
#       set_engine("xgboost")
#                  #early_stopping_rounds = 10)  #
#     mtry_range <- NULL  # Not applicable for GBDT
#   } else if (model_type == "brnn") {
#     model_spec <- parsnip::mlp(hidden_units = tune(), penalty = tune()) %>%
#       set_mode("regression") %>%
#       set_engine("nnet")
#     mtry_range <- NULL
#   } else {
#     stop("Unsupported model type: ", model_type)
#   }
#   return(list(model_spec = model_spec, mtry_range = mtry_range))
#   })
# 
#   return(models)  # Return list of models, one per bootstrap split
# }
```

```{r}

# Set up PLS or Random Forest model specification, including tuning parameters
setup_model <- function(model_type, preprocessed_splits, response_var = NULL) {
  
    train_data_list <- map(preprocessed_splits, "train_data")
    test_data_list <- map(preprocessed_splits, "test_data")
    
    # Iterate over all bootstrap splits to set up models
    models <- map(train_data_list, function(train_data) {

        #  Check if train_data exists before proceeding
        if (is.null(train_data)) {
            warning("Skipping model setup due to NULL train_data.")
            return(list(model_spec = NA, mtry_range = NA))
        }

        if (nrow(train_data) == 0 || ncol(train_data) == 0) {
            warning("Skipping model setup due to empty train_data.")
            return(list(model_spec = NA, mtry_range = NA))
        }
    
        if (model_type == "pls") {
            # Configure a PLS model specification
            model_spec <- parsnip::pls() %>%
                set_mode("regression") %>%
                set_engine("mixOmics") %>%
                set_args(num_comp = tune())
            mtry_range <- NULL
        } else if (model_type == "rf") {
            # Configure a Random Forest model with tuning for mtry and min_n
            if (is.null(response_var)) stop("response_var must be specified for Random Forest.")
            
            # Remove SSN and response columns to get only predictor columns
            predictors_only <- train_data %>%
                dplyr::select(-SSN, -all_of(response_var))
            
            # Check if predictors are sufficient
            if (ncol(predictors_only) < 2) stop("Random Forest requires at least 2 predictor variables.")
            
            # Finalize mtry_range
            mtry_range <- dials::mtry(range = c(2, round(sqrt(ncol(predictors_only)))))
            if (is.null(mtry_range)) stop("Failed to initialize mtry_range in setup_model(). Check predictor columns in train_data.")
            
            model_spec <- parsnip::rand_forest(mtry = tune(), min_n = tune()) %>%
                set_mode("regression") %>%
                set_engine("ranger")
        } else if (model_type == "gbdt") {
            # Gradient-Boosted Decision Tree setup
            predictors_only <- train_data %>%
                dplyr::select(-SSN, -all_of(response_var))
            
            model_spec <- parsnip::boost_tree(
                trees = tune(),  # Number of trees to tune
                tree_depth = tune(),  # Depth of each tree
                learn_rate = tune(),  # Learning rate
                loss_reduction = tune(),  # Minimum gain for split
                min_n = tune(),  # Tune minimal node size
                mtry = tune()  # Tune predictor sampling
            ) %>%
                set_mode("regression") %>%
                set_engine("xgboost")
            mtry_range <- NULL  # Not applicable for GBDT
        } else if (model_type == "brnn") {
            model_spec <- parsnip::mlp(hidden_units = tune(), penalty = tune()) %>%
                set_mode("regression") %>%
                set_engine("nnet")
            mtry_range <- NULL
        } else {
            stop("Unsupported model type: ", model_type)
        }

        return(list(model_spec = model_spec, mtry_range = mtry_range))
    })

    #return(models)  # Return list of models, one per bootstrap split
    return(list(
  models = models
))
  
    
}
```

### Defining/ Calculating Model Metrics

```{r}

calculate_metrics <- function(results_list, data_type, slprptr, .x, model_type) {

  response_var <- slprptr[.x]  # Define response variable
  
    if (is.null(results_list) || length(results_list) == 0) {
    message("⚠️ No valid results for ", response_var, " in ", data_type)
    return(tibble(
      Mean_R2 = NA, SD_R2 = NA, 
      Mean_RMSE = NA, SD_RMSE = NA, 
      Mean_Bias = NA, SD_Bias = NA, 
      Property = response_var, 
      Data_Type = data_type, 
      Model = model_type
    ))
  }
  
   # Extract metrics for each bootstrap iteration
  metrics_list <- map(results_list, function(result) {
    
    # Assign correct truth and prediction sets based on data_type
    if (data_type == "Calibration") {
      truth <- result$model$pre$mold$outcomes[[response_var]]  # Training data
      predictions <- result$train_predictions
    } else if (data_type == "Validation") {
      truth <- result$test_truth[[response_var]]  # Use explicitly stored test values
      predictions <- result$test_predictions
    } else {
      stop("Invalid data_type specified.")
    }

    # Ensure truth and predictions are aligned
    if (length(truth) != nrow(predictions)) {
      message("Skipping metric calculation: Length mismatch between truth and predictions")
      return(tibble(R_sq = NA, RMSE = NA, bias = NA))
    }

    # Safe computation function with error handling
    safe_compute <- function(expression) {
      tryCatch(eval(expression), error = function(e) NA)
    }

    # Calculate performance metrics
    r2_value <- safe_compute(rsq_vec(truth = truth, estimate = predictions$.pred))
    rmse_value_plain <- safe_compute(rmse_vec(truth = truth, estimate = predictions$.pred))
    bias_value <- safe_compute(mean(predictions$.pred - truth, na.rm = TRUE) / mean(truth, na.rm = TRUE) * 100)
    
      # Calculate the range of the response variable from the test_data and RMSEP/range
  response_range <- safe_compute(max(truth, na.rm = TRUE) - min(truth, na.rm = TRUE))
  
  rmse_value <- if (!is.na(response_range) && response_range > 0) rmse_value_plain / (response_range / 10) else NA

    return(tibble(R_sq = r2_value, RMSE = rmse_value, bias = bias_value))
  })
    # 🔍 **Remove NULL results before summarization**
  metrics_list <- compact(metrics_list)  
  # Combine results and calculate mean & standard deviation across bootstrap iterations
  # Check if all iterations failed
   # Convert list of tibbles into a single tibble
  metrics_df <- bind_rows(metrics_list)  # Merge all iterations
  
    # Compute means and standard deviations across bootstrap iterations
  
    # **Check if we have valid metrics**
  if (length(metrics_list) == 0) {
    message("🚨 No valid results for ", response_var, " in ", data_type)
    return(tibble(
      Property = response_var, 
      Data_Type = data_type, 
      Model = model_type,
      Mean_R2 = NA, SD_R2 = NA, 
      Mean_RMSE = NA, SD_RMSE = NA, 
      Mean_Bias = NA, SD_Bias = NA 
      
    ))
  }
  
  metrics_summary <- metrics_df %>%
    summarise(
      Mean_R2 = mean(R_sq, na.rm = TRUE),
      SD_R2 = sd(R_sq, na.rm = TRUE),
      Mean_RMSE = mean(RMSE, na.rm = TRUE),
      SD_RMSE = sd(RMSE, na.rm = TRUE),
      Mean_Bias = mean(bias, na.rm = TRUE),
      SD_Bias = sd(bias, na.rm = TRUE)
    ) %>%
    mutate(Property = response_var, Data_Type = data_type, Model = model_type)

  return(metrics_summary)
}
```

### Defining Models (PLSR, RF, GBDT, BRNN)

```{r}

# Wrapper function to run analysis for each property in slprptr with specified model_type
run_all_properties <- function(slprptr, df1, spec_trt, model_type, identifier,
                               num_bootstrap = 50, use_source = FALSE, source_name = NULL, 
                               tuning_goal = "rmsep", property_range = 2:length(slprptr),
                                include_zv = FALSE, 
                                include_pca = FALSE, num_pca_comp = NULL 
                               #variance_threshold = NULL,
                               # include_pls = FALSE, variance_threshold_pls = NULL,
                               # include_rf = FALSE, forest_threshold = NULL
                               ) {

  output_dir <- paste0("Plots_WellFitted_Properties_", identifier)

  # Store model results for debugging and future analysis
  model_results <- list()

  # Loop through each property and collect metrics
  all_metrics <- furrr::future_map_dfr(property_range, function(.x) {
    tryCatch({
      message("Processing property: ", .x, " : ", slprptr[.x])
    
     library(plsmod) 
     set.seed(345)

    # Extract property-specific data 
    property_data <- select_property_data(slprptr, .x, df1, spec_trt)
    df.f <- property_data$df.f
    df.sel <- property_data$df.sel

    # Check if there are enough observations to proceed 
    if (nrow(df.sel) <= 20) {
      message(paste("Skipping property:", slprptr[.x], "due to insufficient data"))
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = model_type, R_sq = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
  # Generate bootstrap resamples
    boot_splits <- generate_bootstrap_splits(df.sel, num_bootstrap, use_source, source_name)
    
    
    # If boot_splits is NULL, skip this property
if (is.null(boot_splits)) {
  message("Skipping property due to insufficient data.")
  return(tibble(
    Property = slprptr[.x],
    Data_Type = "Skipped",
    Model = model_type,
    R_sq = NA,
    RMSE = NA,
    bias = NA,
    ncomp = NA
  ))
}

      boot_data <- tryCatch({
  map(boot_splits$splits, split_bootstrap_sample)  # Pass rsplit objects directly
}, error = function(e) {
  message("Error in mapping split_bootstrap_sample: ", e$message)
  NULL
}) 
      
          # If boot_data is NULL, skip processing
    if (is.null(boot_data)) {
      message("Skipping property due to bootstrap failure: ", slprptr[.x])
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = model_type, R_sq = NA, RMSE = NA, bias = NA, ncomp = NA))
    }

    # Prepare bootstrap train/test splits
    valid_splits <- prepare_model_data(boot_data, slprptr, .x)
    
    if (is.null(valid_splits)) {
    stop("🚨 Error: valid_splits is NULL for property ", slprptr[.x])
    }
    
   preprocessed_splits <- tryCatch({
    preprocess_data(valid_splits, slprptr, .x,
                    include_zv = include_zv,
                    include_pca = include_pca, num_pca_comp = num_pca_comp 
                    # variance_threshold = variance_threshold,
                    # include_pls = include_pls, variance_threshold_pls = variance_threshold_pls,
                    # include_rf = include_rf, forest_threshold = forest_threshold
                    )
}, error = function(e) {
    message(" Error in preprocess_data() for ", slprptr[.x], ": ", e$message)
    return(NULL)
})
   if (is.null(preprocessed_splits)) {
      return(tibble(Property = slprptr[.x],
                    Data_Type = "Skipped",
                    Model = model_type,
                    R_sq = NA, RMSE = NA, bias = NA, ncomp = NA))
    }


# 🔍 **Check if preprocessed_splits is NULL**
# if (is.null(preprocessed_splits)) {
#     stop(" ERROR: preprocessed_splits is NULL after preprocess_data()!")
# }
    
    # # Ensure train/test data is not empty after preprocessing 
    # if (nrow(preprocessed_splits$train_data) == 0 || nrow(preprocessed_splits$test_data) == 0) {
    #   message(paste("Skipping property:", slprptr[.x], "due to empty train/test data after preprocessing."))
    #   return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = model_type, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    # }

    # # Skip properties with insufficient train/test data
    # if (nrow(valid_splits$train_data) <= 10 || nrow(valid_splits$test_data) <= 10) {
    #   message(paste("Skipping property:", slprptr[.x], "due to insufficient train/test data"))
    #   return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = model_type, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    # }
  
    # Set up the model specification 
    if (model_type == "pls") {
      model_spec_list <- setup_model(model_type, preprocessed_splits, response_var = slprptr[.x])
    } else if (model_type == "rf") {
      model_spec_list <- setup_model(model_type, preprocessed_splits, response_var = slprptr[.x])
    } else if (model_type == "gbdt") {
      model_spec_list <- setup_model(model_type, preprocessed_splits, response_var = slprptr[.x])
    } else if (model_type == "brnn") {
      model_spec_list <- setup_model(model_type, preprocessed_splits, response_var = slprptr[.x])
    } else {
      stop(paste("Unsupported model type:", model_type))
    }
   
   # Compute the global minimum PCA component count across iterations,
# ignoring any NAs
n_comp_pca_list <- purrr::map_dbl(preprocessed_splits, "n_comp_pca")
#DEBUG 
message("n_comp_pca values: ", paste(n_comp_pca_list, collapse = ", "))
min_n_comp_pca <- min(n_comp_pca_list, na.rm = TRUE)
min_n_comp_pca

   
   tuned_models <- tryCatch({
     fit_model(
  model_spec_list = map(model_spec_list$models, "model_spec"), 
  train_data_list = map(preprocessed_splits, "train_data"),
  test_data_list = map(preprocessed_splits, "test_data"),
  recipe_list = map(preprocessed_splits, "recipe"),
  model_type = model_type,
  tuning_goal = tuning_goal,
  property_name = slprptr[.x],
           include_zv = include_zv,
           include_pca = include_pca,
           num_pca_comp = num_pca_comp,
          # variance_threshold = variance_threshold,
          # include_pls = include_pls,
          # variance_threshold_pls = variance_threshold_pls,
          # n_comp_pca = map(preprocessed_splits, "n_comp_pca"),
          # n_comp_pca = preprocessed_splits[[1]]$n_comp_pca
          n_comp_pca = min_n_comp_pca
          # n_comp_pls = map(preprocessed_splits, "n_comp_pls"),
          # include_rf = include_rf,
          # forest_threshold = forest_threshold
        )
   }, error = function(e) {
       message("❌ Model fitting failed for property: ", slprptr[.x],
          "\nError: ", e$message)
              return(list(model = NA, train_predictions = NA, test_predictions = NA, best_params = NA))
            })
   
   if (is.null(tuned_models) || 
    !("model" %in% names(tuned_models)) || 
    length(tuned_models$model) == 0 || 
    is.na(tuned_models$model[1])) {
  message("Skipping property: ", slprptr[.x], " due to model fitting failure.")
  return(tibble(
    Property  = slprptr[.x],
    Data_Type = "Skipped",
    Model     = model_type,
    R_sq      = NA,
    RMSE      = NA,
    bias      = NA,
    ncomp     = NA
  ))
}

            
    # Compute predictions for training and test data
    cal_predictions <- map(tuned_models, "train_predictions")
    val_predictions <- map(tuned_models, "test_predictions")
    

  
    # # Calculate performance metrics 
    
  cal_metrics <- calculate_metrics(
  results_list = tuned_models, 
  data_type = "Calibration", 
  slprptr = slprptr, 
  .x = .x, 
  model_type = model_type
)
   
  val_metrics <- calculate_metrics(
  results_list = tuned_models, 
  data_type = "Validation", 
  slprptr = slprptr, 
  .x = .x, 
  model_type = model_type
  # Extract test data for all bootstrap iterations
)

    # Combine Calibration and Validation metrics
    return(bind_rows(cal_metrics, val_metrics))
  }, error = function(e) {
    message("Error processing property ", slprptr[.x], ": ", e$message)
    tibble(Property = slprptr[.x],
           Data_Type = "Skipped",
           Model = model_type,
           R_sq = NA, RMSE = NA, bias = NA, ncomp = NA)
  })
  
  output_file <- paste0(output_dir, "/bootstrap_metrics_", identifier, ".xlsx")
  writexl::write_xlsx(all_metrics, path = output_file)
  message("Metrics saved to: ", output_file)
})
  return(all_metrics)

}
```

#### Aside: Cumulative Variance for PLSR

```{r}

# Function to calculate cumulative explained variance
calculate_explained_variance <- function(train_data, response_var, max_comps) {
  # Prepare X and Y matrices
  X <- train_data %>%
    dplyr::select(-c(SSN, all_of(response_var))) %>%
    dplyr::mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  Y <- train_data %>%
    dplyr::select(all_of(response_var)) %>%
    dplyr::mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  # Explained variance calculation
  variance_list <- purrr::map_dbl(1:max_comps, function(num_comp) {
    model <- tryCatch(
      mixOmics::pls(X = X, Y = Y, ncomp = num_comp),
      error = function(e) {
        message("Error in fitting PLS model: ", e)
        return(NULL)
      }
    )
    
    if (is.null(model)) {
      return(0)
    }
    
    # Calculate cumulative explained variance for X up to num_comp
    cumulative_variance_X <- sum(model$prop_expl_var$X[1:num_comp])
    cumulative_variance_X
  })
  
  return(variance_list)
}
```

### Fitting Model (tuning hyperparameters)

```{r}

fit_model <- function(model_spec_list, train_data_list, test_data_list, recipe_list, model_type,
                      tuning_goal = "rmsep", cumulative_variance_threshold = 0.85, property_name = NULL,
                       include_zv = FALSE, 
                      # variance_threshold = NULL, 
                       include_pca = FALSE, 
                      # include_pls = FALSE,  
                      # n_comp_pca_variance = NULL,
                       num_pca_comp = NULL, 
                       n_comp_pca = NULL 
                      # n_comp_pls = NULL, 
                      # variance_threshold_pls = NULL, 
                      # include_rf = FALSE, 
                      # forest_threshold = NULL
                      ) { 
    
  # Iterate over bootstrap samples
  results_list <- map2(seq_along(train_data_list), train_data_list, function(iter, train_data) {
    ## assigns each element of train_data_list to train_data, iterativel
    
    test_data <- test_data_list[[iter]]
    recipe <- recipe_list[[iter]]
    
     # Skip if train_data or test_data is NULL
    if (is.null(train_data) || is.null(test_data) || nrow(train_data) == 0 || nrow(test_data) == 0) {
      message("Skipping iteration ", iter, " due to missing or empty data")
      return(NULL)
    }

    model_spec <- model_spec_list[[iter]] 
    #View(model_spec)
    #mtry_range <- model_spec_list[[1]]$mtry_range  # Only for RF
##WILL CAUSE ERROR 
    
    # Set up workflow with recipe and model
    biochar_workflow <- workflow() %>%
      add_recipe(recipe) %>%
      add_model(model_spec)

    # Set default CV folds
    set.seed(123)
    cv_folds <- if (nrow(train_data) <= 10) {
      vfold_cv(train_data, v = nrow(train_data))  
    } else {
      vfold_cv(train_data, v = 10)  
    }

    if (model_type == "pls") {
      # Determine max_comps dynamically
      if (isTRUE(include_pca) && !is.null(n_comp_pca)) {
        max_comps <- min(25, nrow(train_data) - 1,  ncol(train_data), n_comp_pca)
      } else {
         max_comps <- min(25, nrow(test_data) - 4,  ncol(test_data))
      }
      
        # NEW: check if max_comps is valid
  if (max_comps < 1) {
  message("Skipping property ", slprptr[.x], " because max_comps is less than 1.")
  return(tibble(Property = slprptr[.x],
                Data_Type = "Skipped",
                Model = model_type,
                R_sq = NA, RMSE = NA, bias = NA, ncomp = NA))
}

      if (tuning_goal == "rmsep") {
        # Perform CV tuning for RMSEP
        num_comp_grid <- tibble(num_comp = 1:max_comps)
        # tune_results <- tune_grid(
        #   biochar_workflow,
        #   resamples = cv_folds,
        #   grid = num_comp_grid,
        #   metrics = metric_set(rmse),
        #   control = control_grid(save_pred = TRUE)
        # )
        
        tune_results <- tryCatch({
  tune_grid(
    biochar_workflow,
    resamples = cv_folds,
    grid = num_comp_grid,
    metrics = metric_set(rmse),
    control = control_grid(save_pred = TRUE)
  )
}, error = function(e) {
  warning("⚠️ Error in PLS tuning: ", e$message)
  return(NULL)
})

  # NEW: Check if tune_results is empty before proceeding.
  if (is.null(tune_results) || nrow(tune_results) == 0) {
    message("⚠️ Tuning yielded no results for property ", property_name, ". Skipping.")
    return(list(model = NA, train_predictions = NA, test_predictions = NA, best_params = NA, test_truth = test_data))
  }
  

        #best_params <- tune_results %>% select_best(metric = "rmse") %>% dplyr::select(num_comp)
          best_params <- tryCatch({
    tune_results %>% select_best(metric = "rmse") %>% dplyr::select(num_comp)
  }, error = function(e) {
    warning("⚠️ select_best failed: ", e$message)
    return(NULL)
  })
  
  if (is.null(best_params) || nrow(best_params) == 0) {
    message("⚠️ No valid best parameters found for property ", property_name, ". Skipping.")
    return(list(model = NA, train_predictions = NA, test_predictions = NA, best_params = NA, test_truth = test_data))
  }
        final_workflow <- finalize_workflow(biochar_workflow, best_params)

      } else if (tuning_goal == "variance") {
        # Calculate explained variance
        response_var <- colnames(train_data)[[2]]  # Adjust index if response var is elsewhere
        explained_variance <- calculate_explained_variance(train_data, response_var, max_comps)

        # Find min num_comp meeting variance threshold
        num_comp <- which(explained_variance >= cumulative_variance_threshold)[1]
        if (is.na(num_comp)) {
          message(paste("Skipping property:", response_var, "due to insufficient cumulative variance."))
          return(NULL)
        }

        # Validate num_comp with CV
        num_comp_grid <- tibble(num_comp = num_comp)
        tune_results <- tune_grid(
          biochar_workflow,
          resamples = cv_folds,
          grid = num_comp_grid,
          metrics = metric_set(rmse),
          control = control_grid(save_pred = TRUE)
        )

        best_params <- tibble(num_comp = num_comp)
        final_workflow <- finalize_workflow(biochar_workflow, best_params)
      }
    } else if (model_type == "rf") {
      set.seed(123)
      cv_folds <- vfold_cv(train_data, v = min(10, nrow(train_data) - 1))
      
      # Tune Random Forest model
      rf_grid <- grid_random(
        mtry_range,
        min_n(range = c(1, min(5, floor(nrow(train_data) / 3)))),
        size = min(nrow(train_data) / length(cv_folds$splits), 5)
      )

      tune_results <- tune_grid(
        biochar_workflow,
        resamples = cv_folds,
        grid = rf_grid,
        metrics = metric_set(rmse),
        control = control_grid(save_pred = TRUE)
      )

      best_params <- tune_results %>% select_best(metric = "rmse")
      final_workflow <- finalize_workflow(biochar_workflow, best_params)

    } else if (model_type == "brnn") {
      num_predictors <- ncol(train_data) - 2
      max_hidden_units <- min(20, floor(log2(num_predictors)))

      brnn_grid <- grid_random(
        hidden_units(range = c(1, max_hidden_units)),
        penalty(range = c(0.001, 0.1)),
        size = 10
      )

      tune_results <- tune_grid(
        biochar_workflow,
        resamples = cv_folds,
        grid = brnn_grid,
        metrics = metric_set(rmse),
        control = control_grid(save_pred = TRUE)
      )

      best_params <- tune_results %>% select_best(metric = "rmse")
      final_workflow <- finalize_workflow(biochar_workflow, best_params)

    } else if (model_type == "gbdt") {
      set.seed(123)
      cv_folds <- vfold_cv(train_data, v = ifelse(nrow(train_data) < 30, 5, min(10, nrow(train_data) - 1)))

      gbdt_grid <- grid_random(
        trees(range = c(50, 200)),
        tree_depth(range = c(2, min(4, round(log2(nrow(train_data)))))),
        learn_rate(range = c(0.005, 0.03)),
        loss_reduction(range = c(0, 5)),
        min_n(range = c(2, min(10, round(0.05 * nrow(train_data))))),
        mtry(range = c(ceiling(0.05 * ncol(train_data)), min(15, floor(0.4 * ncol(train_data))))),
        size = 10
      )

      tune_results <- tryCatch({
        tune_grid(
          biochar_workflow,
          resamples = cv_folds,
          grid = gbdt_grid,
          metrics = metric_set(rmse),
          control = control_grid(save_pred = TRUE)
        )
      }, error = function(e) {
        warning("Error during GBDT tuning: ", e$message)
        return(NULL)
      })

       # If tuning failed, skip
    if (is.null(tune_results) || nrow(tune_results) == 0) {
      message("⚠️ Skipping iteration ", iter, " due to tuning failure")
      return(NULL)
    }
      
      # Check if tune_results is valid
if (!is.null(tune_results) && nrow(tune_results) > 0) {
  
  # Ensure .metrics exists and is not NULL
  if (is.null(tune_results$.metrics) || nrow(tune_results$.metrics[[1]]) == 0) {
    message("🚨 No valid metrics in `tune_results` for iteration ", iter, ". Skipping...")
    return(NULL)
  }

  
} else {
  message("⚠️ `tune_results` contains no valid results for iteration ", iter, ". Skipping...")
  return(NULL)
}

      best_params <- tune_results %>% select_best(metric = "rmse")
      
      best_params <- tryCatch({
  tune_results %>% select_best(metric = "rmse")
}, error = function(e) {
  message("⚠️ Skipping iteration ", iter, " as no best parameters could be selected")
  return(NULL)
})
      # If best_params is NULL, skip this iteration
if (is.null(best_params) || nrow(best_params) == 0) {
  message("⚠️ No valid parameters found for iteration ", iter, ". Skipping model fitting.")
  return(NULL)
}
      
      final_workflow <- finalize_workflow(biochar_workflow, best_params)
    }

    # Fit final model
   # model_fit <- fit(final_workflow, data = train_data)
    
        # Try-Catch for model fitting
    model_fit <- tryCatch({
      fit(final_workflow, data = train_data)
    }, error = function(e) {
      message("🚨 Model fitting failed for iteration ", iter, ": ", e$message)
      return(NULL)
    })
    
      if (is.null(model_fit)) {
      return(NULL)
    }

    # Generate predictions for train and test sets
    train_predictions <- tryCatch({
      predict(model_fit, new_data = train_data) %>%
        as_tibble()
    }, error = function(e) {
      message("Error in train_predictions: ", e)
      return(NULL)
    })
    

    test_predictions <- tryCatch({
      predict(model_fit, new_data = test_data) %>%
        as_tibble()
    }, error = function(e) {
      message("Error in test_predictions: ", e)
      return(NULL)
    })

    return(list(
      model = model_fit,
      train_predictions = train_predictions,
      test_predictions = test_predictions,
      best_params = best_params,
      test_truth = test_data  # Store actual test response values

    ))
  })
  
  if (all(map_lgl(results_list, is.null))) {  
  message("🚨 All bootstrap iterations failed for property: ", property_name, ". Skipping model training.")
  return(NULL)  # Return NULL to indicate failure
}
  
  return(results_list)  # Return results for all bootstrap iterations
}
```

## PLSR

#### Tune to minimize RMSEP - MIR

selecting the number of components that minimizes the RMSEP

Total split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 100) # , property_range = 50:51)
```

Source-based split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep")
```

Cornell only Predictions (focused on pH)

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep")
```

#### Tune to minimize RMSEP - MIR 80/20 Split

Need to change 0.7 to 0.8

```         
set.seed(123) 
# Split ensuring Source is represented in both subsets
trainIndex <- createDataPartition(df.sel$Source, p = 0.7, list = FALSE)

} else { set.seed(123) # Split without considering Source trainIndex <- sample(seq_len(nrow(df.sel)), size = 0.7 * nrow(df.sel))
```

selecting the number of components that minimizes the RMSEP

Total split

```{r}
# 
# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
#                    use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

Source-based split

```{r}
# 
# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_sourcesplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
#                    use_source = TRUE, tuning_goal = "rmsep")
```

Cornell only Predictions (focused on pH)

```{r}

# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_CornellOnlysplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
#                    use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep")
```

#### Lab Covariate - MIR

Zero-variance predictors don’t contribute to PLS components since they hold no information.

```         
•   Removing them has a marginal effect because PLSR naturally assigns zero weight to irrelevant predictors.

•   Edge case: If your dataset is very small, leaving zero-variance predictors might slightly affect component selection due to noise in variance calculations.
```

Total split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_Lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep") #, property_range = 49:51)
```

Source-based split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_sourcesplit_Lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep")
```

Cornell only Predictions (focused on pH)

\*\* don't make a difference

#### PCA Components as features

```{r}

identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_PCAfeat")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", include_pca = TRUE, num_pca_comp =500, num_bootstrap = 5) #, property_range = 50:55) 
```

\*\* Sparse Generalized Canonical Correlation Analysis (SGCCA) algorithm used in pls() modeling failed to find a stable solution within the allowed iterations.

```{r}

identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_PCAfeat_variance")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", include_pca = TRUE, variance_threshold = 0.95) #, property_range = 50:52) 
```

#### RF Top Features as features

```{r}

identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_RFfeat")  # For RMSEP

run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", include_rf = TRUE, forest_threshold = 0.02) #, property_range = 2:5) 
```

```{r}
identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_RFfeat_pretune")

run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", 
                   include_rf = TRUE, forest_threshold = 0.02, rf_tuning_file = "Plots_WellFitted_Properties_RF_RMSEP_MIR_totsplit_99perc/all_metrics_RF_RMSEP_MIR_totsplit_99perc.xlsx")
```

#### Tune to minimize RMSEP - NIR

Total split - NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_NIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep")
```

Source-based split - NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep")#, property_range = 3:4)
```

Cornell only Predictions - NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_NIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", property_range = 59:60)
```

#### Tune to minimize RMSEP - MIR + NIR

Total split - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep")
```

Source-based split - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep")
```

Cornell only Predictions - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep")
```

#### Lab Covariate - MIR + NIR

Total split - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_totsplit_Lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

Source-based split - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_sourcesplit_Lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep")
```

Cornell only Predictions - MIR + NIR

\*\* doesn't make sense to run

## RANDOM FOREST

#### Tune to minimize RMSEP - MIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

```{r}
identifier <- initialize_workspace("RF_RMSEP_MIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep") #,property_range = 25:55)

```

#### Tune to minimize RMSEP - MIR 80/20 Split

```{r}
# 
# identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
#                    use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_sourcesplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
#                    use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

```{r}
# identifier <- initialize_workspace("RF_RMSEP_MIR_CornellOnlysplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
#                    use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep") #,property_range = 25:55)

```

#### Lab Covariate - MIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

#### No Zero Variance - MIR

-   Identifies **columns (wavenumbers) where all values are identical** (e.g., every row has 0.5 at a certain wavenumber). These predictors provide **no information** because their standard deviation is **zero**.

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 59:63)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_sourcesplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_CornellOnlysplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)

```

#### Lab Covariate & No Zero Variance- MIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

#### PCA Components as features

Number of components

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_PCAfeat")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_pca = TRUE, num_pca_comp =20) #, property_range = 58:65) ˆ
```

Captured variance

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_PCAfeat_Var")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_pca = TRUE, variance_threshold = 0.95) #, property_range = 39:41 ) #property_range = 49:51,
```

#### PLSR Components as features

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totspslit_PLSfeat_RMSEP")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", include_pls = TRUE) #, property_range = 2:5) 
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_PLSfeat_Var") 

run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", include_pls = TRUE, variance_threshold_pls = 0.95)
```

#### RF Top Features as features

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_RFfeat")  # For RMSEP

run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", include_rf = TRUE, forest_threshold = 0.02) #, property_range = 2:5) 
```

#### 

#### (Cont. from here 13/02/2025) Tune to minimize RMSEP - NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

```{r}
identifier <- initialize_workspace("RF_RMSEP_NIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep") #,property_range = 25:55)

```

#### Lab Covariate - NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

#### No Zero Variance - NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_totsplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_sourcesplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_CornellOnlysplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)

```

#### Lab Covariate & No Zero Variance - NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

#### Tune to minimize RMSEP - MIR + NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
                   use_source = FALSE, tuning_goal = "rmsep")
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep")
```

ERRORRR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell", tuning_goal = "rmsep")


#Note: skips testing of all properties - insufficient data. 
```

#### Lab Covariate - MIR + NIR8

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

#### No Zero Variance - MIR +NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_totsplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_sourcesplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_CornellOnlysplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

#### Lab Covariate & No Zero Varaiance - MIR + NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

## GBDT

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("GBDT_RMSEP_MIR_totsplit")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "gbdt",  # Specify GBDT
  use_source = FALSE,
  tuning_goal = "rmsep",
  identifier,
  property_range = 52:55#length(slprptr)
)
```

```{r}

# # Run analysis for all properties
# identifier <- initialize_workspace("GBDT_RMSEP_MIR_sourcesplit") 
# 
# run_all_properties(
#   slprptr = slprptr,
#   df1 = df1,
#   spec_trt = spec_trt,
#   model_type = "gbdt",  # Specify GBDT
#   use_source = TRUE,
#   tuning_goal = "rmsep",
#   identifier,
#   property_range = 2:length(slprptr)
# )
```

```{r}

# # Run analysis for all properties
# identifier <- initialize_workspace("GBDT_RMSEP_MIR_CornellOnlysplit") 
# 
# run_all_properties(
#   slprptr = slprptr,
#   df1 = df1,
#   spec_trt = spec_trt,
#   model_type = "gbdt",  # Specify GBDT
#   use_source = FALSE,
#   source_name = "Cornell ",
#   tuning_goal = "rmsep",
#   identifier,
#   property_range = 2:length(slprptr)
# )
```

## BRNN

#### Tune to minimize RMSEP - MIR

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_totsplit")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  property_range = 52:56, #length(slprptr),
  #include_pca = TRUE,
  identifier#,
  #num_pca_comp = 20
  #variance_threshold = 0.95
)
```

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_sourcesplit")
  
run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "brnn",
  use_source = TRUE,
  tuning_goal = "rmsep",
  #property_range = 52:56, #length(slprptr),
  include_pca = TRUE,
  identifier,
  variance_threshold = 0.95
)
```

#### Lab Covariate - MIR

```{r}
identifier <- initialize_workspace("BRNN_RMSEP_MIR_totsplit_lab")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_lab,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  include_pca = TRUE,
  identifier,
  variance_threshold = 0.95
)
```

Stopped HERE

```{r}
identifier <- initialize_workspace("BRNN_RMSEP_MIR_sourcesplit_lab")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_lab,
  model_type = "brnn",
  use_source = TRUE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  identifier,
  include_pca = TRUE,
  variance_threshold = 0.95
)
```

#### Tune to minimize RMSEP - NIR

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_NIR_totsplit")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_NIR,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  #property_range = 52:56, #length(slprptr),
  include_pca = TRUE,
  identifier,
  num_pca_comp = 20
  #variance_threshold = 0.95
)
```

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_NIR_sourcesplit")
  
run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_NIR,
  model_type = "brnn",
  use_source = TRUE,
  tuning_goal = "rmsep",
  #property_range = 52:56, #length(slprptr),
  include_pca = TRUE,
  identifier,
  variance_threshold = 0.95
)
```

#### 

#### Tune to minimize RMSEP - MIR + NIR

```{r}
identifier <- initialize_workspace("BRNN_RMSEP_MIR_NIR_totsplit")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_NIR,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  identifier,
  include_pca = TRUE,
  variance_threshold = 0.95
)
```

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_NIR_sourcesplit")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_NIR,
  model_type = "brnn",
  use_source = TRUE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  identifier,
  include_pca = TRUE,
  variance_threshold = 0.95
)
```

#### Lab Covariate - MIR

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_NIR_totsplit_lab")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_NIR_lab,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  identifier,
  include_pca = TRUE,
  variance_threshold = 0.95
)
```

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_NIR_sourcesplit_lab")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_NIR_lab,
  model_type = "brnn",
  use_source = TRUE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  identifier,
  include_pca = TRUE,
  variance_threshold = 0.95
)
```

#### PLSR Components as features

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_totsplit_PLSfeat")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  #property_range = 52:56, #length(slprptr),
  include_pls = TRUE,
  identifier
  #variance_threshold = 0.95
)
```

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_totsplit_PLSfeat_Var")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  #property_range = 52:56, #length(slprptr),
  include_pls = TRUE,
  identifier,
  variance_threshold_pls = 0.95
)
```

#### RF Top Features as features

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_totsplit_RFfeat")  # For RMSEP

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  #property_range = 52:56, #length(slprptr),
  include_rf = TRUE,
  identifier,
  forest_threshold = 0.99 #try 0.10 if 'too many weights' error is persistent
)
```

#### 
