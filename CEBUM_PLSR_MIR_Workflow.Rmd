---
title: "CEBUM_PLSR_Workflow"
output: html_document
date: "2024-10-14"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries

```{r, include=FALSE}

# Define a list of required packages
required_packages <- c("writexl","randomForest", "caret", "pls", "data.table", "ithir", 
                       "dplyr", "tidyr", "prospectr", "globals", "stringr", 
                       "ggplot2", "here", "tidymodels", "parsnip", "plsmod", 
                       "mixOmics", "yardstick", "purrr", "tibble","ranger","VIM","shiny","FNN","lightgbm", "xgboost", "dials", "tune", "brnn")

#VIM for KNN imputation
# Load all required packages quietly without outputting any list of packages
suppressPackageStartupMessages(
  invisible(lapply(required_packages, library, character.only = TRUE))
)
```

## Load reference data

```{r}

# treated spectra
spec_trt <- readRDS("spec_trt_MIR.RDS")
spec_trt_MIR_lab <- readRDS("spec_trt_MIR_lab.RDS")

spec_trt_NIR <- readRDS("/Users/soliverchefusi/Library/CloudStorage/OneDrive-Personal/R/ICRAF&Lehmann/ICRAF_IR/ICRAF_IR_R_Project/ICRAF_IR/CEBUM/CEBUM_NIR/CEBUM_NIR/spec_trt_NIR.RDS")
spec_trt_NIR_lab <- readRDS("/Users/soliverchefusi/Library/CloudStorage/OneDrive-Personal/R/ICRAF&Lehmann/ICRAF_IR/ICRAF_IR_R_Project/ICRAF_IR/CEBUM/CEBUM_NIR/CEBUM_NIR/spec_trt_NIR_lab.RDS")

spec_trt <- spec_trt[,-2] #removing the data for the first wavenumber since it's all NAN's
spec_trt_MIR_lab <- spec_trt_MIR_lab[,-2]

spec_trt_MIR_NIR <- merge(spec_trt,spec_trt_NIR, by ="SSN")
spec_trt_MIR_NIR_lab <- merge(spec_trt_MIR_lab,spec_trt_NIR_lab, by ="SSN")
rownames(spec_trt_MIR_NIR_lab) <- spec_trt_MIR_NIR_lab$SSN

# biochar property data
#df1<-readRDS("df1.RDS")
df1<-readRDS("df.f.RDS")

# df.f: full dataset;
# spec_trt: treated spectra

#Available properties to predict
slprptr<-names(df1[-c(2:9)]) #FUSI EDIT: removing metadata columns 

# defining identity object with names of biochar samples
pred<-as.data.frame(spec_trt[,1])
colnames(pred)<-"SSN"
 
```

## PLSR ( & RF) Base model

1.  definine a 'base' model based on minimizing RMSEP

High-Level Structure:

```         
1.  Preprocessing

2.  Setting up models (PLS and Random Forest)

3.  Running models (map() over models to fit them)

4.  Calculating metrics (for each model)

5.  Plotting results (for each model)

6.  Comparing the models (optional step for comparing PLS vs Random Forest)
```

```{r}

# Initialize the workspace by creating directories for storing outputs and 
# setting up an empty tibble to accumulate performance metrics 
initialize_workspace <- function(identifier) {
  #dir.create(paste0("Plots_Validation_MIR_", identifier), showWarnings = FALSE)
  dir.create(paste0("Plots_WellFitted_Properties_", identifier), showWarnings = FALSE)
  all_metrics <<- tibble()  # Global tibble to store metrics for each property
  
  return(identifier)  # Pass the identifier for later use
}

# slprptr: list of properties; .x: index of property in slprptr
select_property_data <- function(slprptr, .x, df1, spec_trt) {
  # Filter df1 to create df.f based on SSN present in spec_trt
    df.f <- df1 %>%
    filter(SSN %in% spec_trt$SSN) # Select only samples present in both predictor and response datasets
  # Check if the property exists in df1
  if (!slprptr[.x] %in% colnames(df.f)) {
    stop(paste("Property", slprptr[.x], "not found in df1"))
  }
  
  # Columns to select, ensuring required columns are present
  cols_to_select <- c("SSN", "Source", slprptr[.x])
  cols_to_select <- intersect(cols_to_select, colnames(df.f))
  
  if (!all(cols_to_select %in% colnames(df.f))) {
    stop(paste("Columns", paste(cols_to_select, collapse = ", "), "not found in df.f"))
  }
  
  # Filter and preprocess data, including removing outliers for the selected property

    
    df.sel <- df.f %>%
    dplyr::select(dplyr::all_of(cols_to_select)) %>%
    inner_join(spec_trt, by = "SSN") %>%   # Combine predictor and response variables into one df
    na.omit()#%>%
    #filter(data.table::between(!!sym(slprptr[.x]), quantile(!!sym(slprptr[.x]), 0.01), quantile(!!sym(slprptr[.x]), 0.99))) # Filter out extremes
  
  # Set SSN as rownames but keep the column
  rownames(df.sel) <- df.sel$SSN
  # Check if df.sel has rows
  if (nrow(df.sel) == 0) {
    stop(paste("No matching rows in df.sel for property:", slprptr[.x]))
  }
  
  return(list(df.f = df.f, df.sel = df.sel))
}

# Split data into calibration (training) and validation (test) sets
# Allows splitting based on a specific Source if source_name is specified
split_data <- function(df.sel, use_source = FALSE, source_name = NULL) {
  # If a specific source_name is provided, filter data for that source
  
  if (!is.null(source_name)) {
    df.sel <- df.sel %>% filter(Source == source_name)
  }

  if (use_source) {
    set.seed(123) 
    # Split ensuring Source is represented in both subsets
    trainIndex <- createDataPartition(df.sel$Source, p = 0.7, list = FALSE)
  } else {
    set.seed(123) 
    # Split without considering Source
    trainIndex <- sample(seq_len(nrow(df.sel)), size = 0.7 * nrow(df.sel))
  }

  # Remove Source column
    df.sel <- df.sel %>% dplyr::select(-Source)
  
  cal_ids <- rownames(df.sel)[trainIndex]
  val_ids <- rownames(df.sel)[-trainIndex]
    
    # Filter and assign rownames directly
  cal_df <- df.sel %>%
    filter(rownames(.) %in% cal_ids) %>%
    arrange(rownames(.)) 
  
  val_df <- df.sel %>%
    filter(rownames(.) %in% val_ids) %>%
    arrange(rownames(.))

  # Save objects in the global environment
assign("cal_df_global", cal_df, envir = .GlobalEnv)
assign("val_df_global", val_df, envir = .GlobalEnv)

  return(list(cal_df = cal_df, val_df = val_df))
}

prepare_model_data <- function(cal_df, val_df, slprptr, .x) {
  if (nrow(cal_df) <= 10 || nrow(val_df) <= 10) {
    warning("Insufficient rows in cal_df or val_df. Returning NA placeholders.")
    train_data <- cal_df
    test_data <- val_df
    train_data[, ] <- NA
    test_data[, ] <- NA
  } else {
    train_data <- cal_df
    test_data <- val_df
    rownames(train_data) <- train_data$SSN  # Set SSN as rownames but keep the column
    rownames(test_data) <- test_data$SSN
  }
  
  # Save rownames of training and test data for debugging
  debug_rows <- tibble(
    Property = slprptr[.x],
    Dataset = c(rep("Training", nrow(train_data)), rep("Testing", nrow(test_data))),
    SSN = c(rownames(train_data), rownames(test_data))
  )
  
  # Save debugging dataframe globally to inspect later
  if (!exists("debug_row_summary")) {
    assign("debug_row_summary", debug_rows, envir = .GlobalEnv)
  } else {
    debug_row_summary <<- bind_rows(debug_row_summary, debug_rows)
  }
  
  return(list(train_data = train_data, test_data = test_data))
}

# Create a recipe for data preprocessing, with optional custom steps
preprocess_data <- function(train_data, test_data, slprptr, p, recipe_steps = NULL,include_zv = FALSE,
                            include_pca = FALSE, 
                            num_pca_comp = NULL, 
                            variance_threshold = NULL) {
  # Check if train/test data is all NA
  if (all(is.na(train_data)) || all(is.na(test_data))) {
    warning("Preprocessing skipped due to all NA in train/test data.")
    return(list(
      recipe = NULL,
      train_data = data.frame(),  # Return an empty data frame as a placeholder
      test_data = data.frame(),
      n_comp_pca = NA
    ))
  }
  
  formula <- as.formula(paste(slprptr[p], "~ ."))  # Define formula for model
  
  # Define base recipe with ID role (column) for SSN
  recipe_base <- recipe(formula, data = train_data) %>%
    update_role(SSN, new_role = "ID")  # Designate SSN column as an identifier
  
    # Conditionally add step_zv() only if include_zv is TRUE
  if (include_zv) {
    recipe_base <- recipe_base %>% step_zv()%>%
step_nzv()
  }
  
  # Conditionally add PCA only if include_pca is TRUE
  
    # Check available numeric predictors **before** applying PCA

  # Handle PCA selection
  n_comp_pca <- NA  # Default to NA if PCA is not applied
  
  if (include_pca) {
    if (!is.null(num_pca_comp)) {
      
      # Ensure num_pca_comp does not exceed the number of samples - 1
      # num_pca_comp_set <- min(num_pca_comp, ncol(train_data), nrow(train_data) - 1)
      
    num_pca_comp_set <- min(
    num_pca_comp, 
    ncol(train_data), 
    nrow(train_data) - 1,  
    max(10, floor(nrow(train_data) * 0.2))
  )  

  # Fallback to 5 components if the matrix is singular
  if (num_pca_comp_set > (nrow(train_data) - 1) || num_pca_comp_set > (ncol(train_data) - 1)) {
    message("Warning: num_pca_comp (", num_pca_comp, ") too high. Using fallback value of 5 components.")
    num_pca_comp_set <- 5
  }
      

    recipe_base <- recipe_base %>%
      step_pca(all_numeric_predictors(), num_comp = num_pca_comp_set)
     n_comp_pca <- num_pca_comp_set  # Store the manually set component count

         # Extract actual number of components chosen dynamically
    prepped_recipe <- prep(recipe_base, training = train_data, retain = TRUE)
    
    # Extract the actual number of PCA components chosen based on variance
    n_comp_pca_test <- prepped_recipe$steps[[1]]$num_comp 
    
    # Debugging messages to check values
View(prepped_recipe)
message(paste("Final num_pca_comp_set:", n_comp_pca_test))
     
     
  } else if (!is.null(variance_threshold)) {
    recipe_base_pca_variance <- recipe_base %>%
      step_pca(all_numeric_predictors(), threshold = variance_threshold)
    
    # Extract actual number of components chosen dynamically
    prepped_recipe <- prep(recipe_base_pca_variance, training = train_data, retain = TRUE)
    
    # Extract the actual number of PCA components chosen based on variance
    n_comp_pca_variance <- prepped_recipe$steps[[1]]$num_comp  # Extracts the selected num_comp

    # Ensure the number of PCA components does not exceed nrow(train_data) - 1
    #num_pca_comp_set <- min(n_comp_pca_variance, ncol(train_data), nrow(train_data) - 1)
numeric_cols <- train_data %>%
  dplyr::select(where(is.numeric)) %>%
  ncol()

num_pca_comp_set <- max(
  min(n_comp_pca_variance, numeric_cols, nrow(train_data) - 1, ncol(train_data) - 1),  
  max(min(10, floor(nrow(train_data) * 0.2)), 2)  # Dynamic lower bound  #
)

    # Apply PCA step with the selected number of components
    recipe_base <- recipe_base %>%
      step_pca(all_numeric_predictors(), num_comp = num_pca_comp_set)

    # Store the number of components for later use
    n_comp_pca <- num_pca_comp_set
    
  } else {
    warning("PCA was enabled but neither num_pca_comp nor variance_threshold was provided. PCA will not be applied.")
  }
  }
  
  # Ensure valid PCA selection
  if (!is.na(n_comp_pca) && n_comp_pca < 2) {
    warning(paste("Skipping PCA for", slprptr[p], "- not enough valid components after preprocessing."))
    return(list(
      recipe = NULL,
      train_data = train_data,
      test_data = test_data,
      n_comp_pca = NA
    ))
  }
  
  message("DEBUG: Inside preprocess_data()")
message(paste("DEBUG: include_pca =", include_pca))
message(paste("DEBUG: num_pca_comp =", num_pca_comp))
#message(paste("DEBUG: variance_threshold =", variance_threshold))
message(paste("DEBUG: Final n_comp_pca =", n_comp_pca))
message(paste("DEBUG: Final n_comp_pca_set =", num_pca_comp_set))

if (is.null(n_comp_pca) || is.na(n_comp_pca)) {
    stop("ERROR: n_comp_pca is NULL in preprocess_data()!")
}
  
  # Apply additional preprocessing steps as needed
  if (!is.null(recipe_steps)) {
    for (step in recipe_steps) {
      recipe_base <- recipe_base %>% step
    }
  }
  
  return(list(recipe = recipe_base, train_data = train_data, test_data = test_data, n_comp_pca = n_comp_pca))
}

# Set up PLS or Random Forest model specification, including tuning parameters
setup_model <- function(model_type, train_data = NULL, response_var = NULL) {
  if (model_type == "pls") {
    # Configure a PLS model specification
    model_spec <- parsnip::pls() %>%
      set_mode("regression") %>%
      set_engine("mixOmics") %>%
      set_args(num_comp = tune())
    mtry_range <- NULL
  } else if (model_type == "rf") {
    # Configure a Random Forest model with tuning for mtry and min_n
    if (is.null(train_data)) stop("train_data must be provided for Random Forest.")
    if (is.null(response_var)) stop("response_var must be specified for Random Forest.")
    
    # Remove SSN and response columns to get only predictor columns
    predictors_only <- train_data %>%
      dplyr::select(-SSN, -all_of(response_var))
        # Check if predictors are sufficient
    if (ncol(predictors_only) < 2) stop("Random Forest requires at least 2 predictor variables.")
    
    
    # Finalize mtry_range
    # mtry_range <- dials::finalize(dials::mtry(), predictors_only) # predictors_only is used to set the max value for the mtry range - so here mtry ranges up to the total number of predictors (e.g. number of wavenumbers).
    mtry_range <- dials::mtry(range = c(2, round(sqrt(ncol(predictors_only)))))
    
    if (is.null(mtry_range)) stop("Failed to initialize mtry_range in setup_model(). Check predictor columns in train_data.")
    
    model_spec <- parsnip::rand_forest(mtry = tune(), min_n = tune()) %>%
      set_mode("regression") %>%
      set_engine("ranger")
  } else if (model_type == "gbdt") {
    # Gradient-Boosted Decision Tree setup
    predictors_only <- train_data %>%
      dplyr::select(-SSN, -all_of(response_var))
    model_spec <- parsnip::boost_tree(
      trees = tune(),  # Number of trees to tune
      tree_depth = tune(),  # Depth of each tree
      learn_rate = tune(),  # Learning rate
      loss_reduction = tune(),  # Minimum gain for split
      min_n = tune(),  # Tune minimal node size
      #sample_size = tune(),  # Tune row sampling
      mtry = tune()  # Tune predictor sampling
    ) %>%
      set_mode("regression") %>%
      set_engine("xgboost")
                 #early_stopping_rounds = 10)  #
    mtry_range <- NULL  # Not applicable for GBDT
  } else if (model_type == "brnn") {
    model_spec <- parsnip::mlp(hidden_units = tune(), penalty = tune()) %>%
      set_mode("regression") %>%
      set_engine("nnet")
    mtry_range <- NULL
  } else {
    stop("Unsupported model type: ", model_type)
  }
  return(list(model_spec = model_spec, mtry_range = mtry_range))
}

calculate_metrics <- function(data, predictions, data_type, slprptr, .x, best_params, model_type, test_data) {
  response_var <- slprptr[.x]  # Define response variable

  # Check if response variable is present and .pred column exists in predictions
  if (!response_var %in% colnames(data) || !".pred" %in% colnames(predictions)) {
    message(paste("Skipping metrics calculation due to missing columns."))
    return(tibble(Property = response_var, Data_Type = data_type, Model = model_type, R2 = NA, RMSE = NA))
  }

  # Safe computation function with error handling for each metric
  safe_compute <- function(expression) {
    tryCatch(eval(expression), error = function(e) NA)
  }

  # Calculate performance metrics
  r2_value <- safe_compute(rsq_vec(truth = data[[response_var]], estimate = predictions$.pred))
  rmse_value <- safe_compute(rmse_vec(truth = data[[response_var]], estimate = predictions$.pred))
  bias_value <- safe_compute(mean(predictions$.pred - data[[response_var]], na.rm = TRUE) / mean(data[[response_var]], na.rm = TRUE))

  # Calculate the range of the response variable from the test_data and RMSEP/range
  response_range <- safe_compute(max(test_data[[response_var]], na.rm = TRUE) - min(test_data[[response_var]], na.rm = TRUE))
  rmse_range_ratio <- if (!is.na(response_range) && response_range > 0) rmse_value / (response_range / 10) else NA

  metrics_tibble <- tibble(
    Property = response_var,
    Data_Type = data_type,
    Model = model_type,
    Comps = if (model_type == "pls") best_params$num_comp else NA,
    #mtry = if (model_type == "rf") best_params$mtry else NA,
    #min_n = if (model_type == "rf") best_params$min_n else NA,
    mtry = if (model_type %in% c("rf", "gbdt")) best_params$mtry else NA,  
    min_n = if (model_type %in% c("rf", "gbdt")) best_params$min_n else NA,  
    N = nrow(data),
    R2 = r2_value,
    RMSE = rmse_value,
    RMSEP_range_ratio = rmse_range_ratio,
    bias = bias_value
    #n_comp_pca = if (include_pca) n_comp_pca else NA  # Only assign if PCA was applied
  )

  return(metrics_tibble)
}

# Define properties for core and related metrics
get_property_groups <- function() {
  core_properties <- c(
    "Temp.", "pH", "EC_uS", "Ash_avg", "exchg_Ca_mmol_kg", 
    "exchg_K_mmol_kg", "exchg_Mg_mmol_kg", "exchg_Na_mmol_kg", "exchg_P_mmol_kg", 
    "Ca_mg_kg", "K_mg_kg", "Mg_mg_kg", "C_tot_avg", "N_tot_avg",
    "C_org_wt", "H_tot_wt", "O_ult", "P_avg_mg_kg", "Fe_avg_mg_kg", "Zn_avg_mg_kg", 
    "Cu_avg_mg_kg", "Ni_avg_mg_kg", "As_avg_mg_kg", "Cd_avg_mg_kg"
  )
  
  related_properties <- c(
    "Naphthalin_mg_kg", "X2_Methylnaphthalin", "X1_Methylnaphthalin", "Sum_Naphthaline", 
    "Acenaphthylen", "Acenaphthen", "Fluoren", "Phenanthren", "Anthracen", 
    "Fluoranthen", "Pyren", "Chrysen", "Benzo_a_anthracen", 
    "Benzo_b_plus_k_fluoranthen", "Benzo_a_pyren", "Dibenzo_a_h_anthracen", 
    "Indeno_c_d_pyren", "Benzo_g_h_i_perylen", "Sum_PAH_defined_EPA_mg_kg", 
    "Bulk.Density_mg_m3", "C_tot_to_N_wt", "O_less_Al_Si_Fe_to_C", "O_less_Al_Si_Fe_Ca_to_C","O_less_Al_Si_Fe_C_inorg_to_C", "O_less_C_inorg_to_C", "C_org_to_N_wt", 
    "C_tot_to_N_molar", "Htot_to_C_tot_molar", "Oult_to_C_tot_molar", 
    "Htot_to_C_org_molar", "Oult_to_C_org_molar", "O_less_Al_Si_Fe_to_C_org_molar", 
    "O_less_Al_Si_Fe_Ca_to_C_org_molar", "O_less_Al_Si_Fe_C_inorg__to_C_org_molar", 
    "O_less_C_inorg_to_C_org_molar"
  )
  
  return(list(core_properties = core_properties, related_properties = related_properties))
}

filter_metrics_by_group <- function(all_metrics, property_groups) {
  core_metrics <- all_metrics %>% filter(Property %in% property_groups$core_properties)
  related_metrics <- all_metrics %>% filter(Property %in% property_groups$related_properties)
  return(list(core_metrics = core_metrics, related_metrics = related_metrics))
}


# Function to plot measured vs. predicted values and save in the default directory
plot_predictions <- function(val_plot_data, val_df, slprptr, .x, combined_metrics, model_type, identifier) {
  # Check if the property exists in the validation plot data
  if (!(slprptr[.x] %in% colnames(val_plot_data))) return(NULL)

  # Extract metrics for validation
  val_metrics <- combined_metrics %>% filter(Data_Type == "Validation", Model == model_type)
  
  # Add label text with components and RÂ²
  label_text <- paste("Comps:", val_metrics$Comps, "RÂ²:", round(val_metrics$R2, 3))

  # Generate the plot
  # validation_plot <- ggplot(val_plot_data, aes_string(x = slprptr[.x], y = ".pred")) +
  #   geom_point(color = "blue", size = 2) +
  #   geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  #   labs(
  #     title = paste("Measured vs Predicted for", slprptr[.x], "-", model_type),
  #     x = paste("Measured", slprptr[.x]),
  #     y = "Predicted"
  #   ) +
  #   theme_minimal() +
  #   annotate("text", x = Inf, y = -Inf, hjust = 1.1, vjust = -0.5, label = label_text, size = 4)

  # Save the plot
  # ggsave(
  #   filename = paste0("Plots_Validation_PLSR_MIR_", identifier, "/", slprptr[.x], "_", model_type, ".png"),
  #   plot = validation_plot, width = 7, height = 7, dpi = 300
  #)
}

# Plot RÂ² values and differences for well-fitted properties based on model criteria
plot_well_fitted_r2 <- function(all_metrics, identifier) {
  all_r2_values <- all_metrics %>%
    group_by(Property) %>%
    reframe(
      R2_cal = R2[Data_Type == "Calibration"],
      R2_val = R2[Data_Type == "Validation"],
      R2_diff = abs((R2_cal - R2_val) / R2_cal)
    )
  
  well_fitted_properties <- all_r2_values %>%
    filter(R2_diff <= 0.15, R2_cal > 0.6, R2_val > 0.6) %>%
    pull(Property)
  
  plot_data <- all_metrics %>%
    filter(Property %in% well_fitted_properties, Data_Type %in% c("Calibration", "Validation")) %>%
    dplyr::select(Property, Data_Type, R2, N, RMSE, bias,RMSEP_range_ratio) %>%
    dplyr::rename(Value = R2)
  
  well_fitted_plot <- ggplot(plot_data, aes(x = Property, y = Value, fill = Data_Type)) +
    geom_bar(stat = "identity", position = "dodge") +
    scale_fill_manual(values = c("Calibration" = "purple", "Validation" = "darkgreen")) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
      panel.grid = element_blank(),
      panel.border = element_rect(color = "black", fill = NA)
    ) +
    labs(title = "RÂ² Comparison for Well-Fitted Properties",
         subtitle = "Purple: Calibration | Green: Validation",
         y = "RÂ² Value",
         x = "Property") +
    geom_text(
      aes(label = ifelse(Data_Type == "Calibration", paste0("N:", N),
                         paste0("N:", N, "\nRMSE%:\n", round(RMSEP_range_ratio, 2), "\nRel_Bias:\n", round(bias, 2)))),
      position = position_dodge(width = 0.9), 
      vjust = -0.5, size = 3
    ) +
    ylim(0, 1.35)  # Set y-axis limit to allow space for annotations
  
  print(well_fitted_plot)
  
  ggsave(filename = paste0("Plots_WellFitted_Properties_", identifier, "/well_fitted_properties_plot.png"), 
         plot = well_fitted_plot, width = 10, height = 7, dpi = 300)
}


# Wrapper function to run analysis for each property in slprptr with specified model_type (no default)
run_all_properties <- function(slprptr, df1, spec_trt, model_type, identifier, 
                                use_source = FALSE, source_name = NULL, tuning_goal = "rmsep",
                                property_range = 2:length(slprptr), include_zv = FALSE, include_pca = FALSE, num_pca_comp = NULL, variance_threshold = NULL) {
  #all_metrics <<- tibble()  # Initialize global metrics storage
  model_results <- list()  # Store model fits for later use
  output_dir <- paste0("Plots_WellFitted_Properties_", identifier)  # Use existing directory

  
  # Validate the property_range parameter
  if (any(property_range > length(slprptr) | property_range < 1)) {
    stop("property_range contains indices outside the valid range of slprptr.")
  }
  
  # Loop through each property in the specified range and collect metrics
  all_metrics <- purrr::map_dfr(property_range, function(.x) {
    message("Processing property: ", .x, " : ", slprptr[.x])
    # Select and filter the property data
    property_data <- select_property_data(slprptr, .x, df1, spec_trt)
    df.f <- property_data$df.f
    df.sel <- property_data$df.sel
    
    # Split the data into calibration (training) and validation (test) sets
    split <- split_data(df.sel, use_source = use_source, source_name = source_name)
    cal_df <- split$cal_df
    val_df <- split$val_df
    
    # Skip if the calibration or validation datasets have insufficient rows
    if (nrow(cal_df) <= 10 || nrow(val_df) <= 10) {
      message(paste("Skipping property:", slprptr[.x], "due to insufficient data"))
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
    # Prepare train/test data and preprocess (e.g., removing outliers)
    data_prep <- prepare_model_data(cal_df, val_df, slprptr, .x)  # Converts calibration and validation datasets to train/test format
    recipe_data <- preprocess_data(data_prep$train_data, data_prep$test_data, slprptr, .x, 
      include_zv=include_zv, include_pca = include_pca,  num_pca_comp = num_pca_comp, variance_threshold = variance_threshold)  # Handles preprocessing steps
    
    #Ensure train_data and test_data are updated after additional step e.g. step_pca
    train_data <- recipe_data$train_data
    test_data <- recipe_data$test_data
    
    n_comp_pca <- recipe_data$n_comp_pca 
    message(paste("DEBUG: Extracted n_comp_pca from preprocess_data():", n_comp_pca))
    
    # Ensure train/test data is not empty after preprocessing
    if (nrow(recipe_data$train_data) == 0 || nrow(recipe_data$test_data) == 0) {
      message(paste("Skipping property:", slprptr[.x], "due to empty train/test data after preprocessing."))
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
    # Step 3: Skip properties with insufficient train/test data
    if (nrow(data_prep$train_data) <= 10 || nrow(data_prep$test_data) <= 10) {
      message(paste("Skipping property:", slprptr[.x], "due to insufficient train/test data"))
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
    # Set up the model specification (PLS or RF) based on model_type
    if (model_type == "pls") {
      model_spec_list <- setup_model(model_type, train_data = data_prep$train_data)  # Set up PLS model
    } else if (model_type == "rf") {
      model_spec_list <- setup_model(model_type, train_data = data_prep$train_data, response_var = slprptr[.x])  # Set up RF model
    } else if (model_type == "gbdt") {
      model_spec_list <- setup_model(model_type, train_data = data_prep$train_data)  # Set up GBDT model
    } else if (model_type == "brnn") {  # ðŸ”¥ Fix: Ensure BRNN is handled
  model_spec_list <- setup_model(model_type, train_data = data_prep$train_data)  # Set up BRNN model
  } else {
      stop(paste("Unsupported model type:", model_type))  # Error for unsupported models
    }
    
    
    # Fit the model using the specified tuning goal (e.g., RMSEP) and handle errors gracefully
    
    message(paste("DEBUG: Passing n_comp_pca to fit_model():", n_comp_pca))

  
    model_fit <- tryCatch(
      {
        fit_model(
          model_spec_list, 
          train_data = recipe_data$train_data, 
          test_data = recipe_data$test_data, 
          recipe = recipe_data$recipe, 
          model_type = model_type, 
          tuning_goal = tuning_goal,
          property_name = slprptr[.x],
          include_zv = include_zv,
          include_pca = include_pca,
          variance_threshold = variance_threshold,
          num_pca_comp = num_pca_comp,
          n_comp_pca = n_comp_pca 
        )
      },
      error = function(e) {
        # Log detailed debugging info
        warning(paste(
          "Error with property:", slprptr[.x], "\n",
          "Training data dimensions:", dim(recipe_data$train_data), "\n",
          "Test data dimensions:", dim(recipe_data$test_data), "\n",
          "Error message:", e$message
        ))
        return(tibble(Property = slprptr[.x], Data_Type = "Error", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
      }
    )
    
    # Handle skipped properties due to model fitting issues
    if ("Data_Type" %in% colnames(model_fit) && model_fit$Data_Type[1] == "Skipped") {
      return(model_fit)
    }
    
    # Generate predictions for calibration and validation datasets
    cal_predictions <- predict(model_fit$model, new_data = recipe_data$train_data)
    val_predictions <- predict(model_fit$model, new_data = recipe_data$test_data)
    
    # Calculate performance metrics (RÂ², RMSE, Bias, etc.) for both datasets
    cal_metrics <- calculate_metrics(
      data = recipe_data$train_data, 
      predictions = cal_predictions, 
      data_type = "Calibration", 
      slprptr = slprptr, 
      .x = .x, 
      best_params = model_fit$best_params, 
      model_type = model_type, 
      test_data = recipe_data$test_data
    )
    val_metrics <- calculate_metrics(
      data = recipe_data$test_data, 
      predictions = val_predictions, 
      data_type = "Validation", 
      slprptr = slprptr, 
      .x = .x, 
      best_params = model_fit$best_params, 
      model_type = model_type, 
      test_data = recipe_data$test_data
    )
    
    # Combine metrics for calibration and validation
    combined_metrics <- bind_rows(cal_metrics, val_metrics)
    
    # Plot predictions (commented out for now; enable as needed)
    # plot_predictions(
    #   val_plot_data = recipe_data$test_data %>% dplyr::select(slprptr[.x]) %>% bind_cols(val_predictions),
    #   val_df = val_df,
    #   slprptr = slprptr,
    #   p = .x,
    #   combined_metrics = combined_metrics,
    #   model_type = model_type,
    #   identifier = identifier
    # )

    return(combined_metrics)
  })
  
  # Retrieve property groups for filtering metrics
  property_groups <- get_property_groups()
  
  # Filter metrics into core and related property groups
  filtered_metrics <- filter_metrics_by_group(all_metrics, property_groups)
  
  # Save filtered metrics as separate tabs in an Excel file
  output_data <- list(
    "All Metrics" = all_metrics,
    "Core Properties" = filtered_metrics$core_metrics,
    "Related Properties" = filtered_metrics$related_metrics
  )
  output_file <- paste0("Plots_WellFitted_Properties_", identifier, "/all_metrics_", identifier, ".xlsx")
  writexl::write_xlsx(output_data, path = output_file)
  

  # Plot well-fitted RÂ² results
  plot_well_fitted_r2(all_metrics, identifier)
}
```

## Base model 1

```{r}

# Function to calculate cumulative explained variance
calculate_explained_variance <- function(train_data, response_var, max_comps) {
  # Prepare X and Y matrices
  X <- train_data %>%
    dplyr::select(-c(SSN, all_of(response_var))) %>%
    dplyr::mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  Y <- train_data %>%
    dplyr::select(all_of(response_var)) %>%
    dplyr::mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  # Explained variance calculation
  variance_list <- purrr::map_dbl(1:max_comps, function(num_comp) {
    model <- tryCatch(
      mixOmics::pls(X = X, Y = Y, ncomp = num_comp),
      error = function(e) {
        message("Error in fitting PLS model: ", e)
        return(NULL)
      }
    )
    
    if (is.null(model)) {
      return(0)
    }
    
    # Calculate cumulative explained variance for X up to num_comp
    cumulative_variance_X <- sum(model$prop_expl_var$X[1:num_comp])
    cumulative_variance_X
  })
  
  return(variance_list)
}
```

```{r}

# Main fit_model function
fit_model <- function(model_spec_list, train_data, test_data, recipe, model_type,
                      tuning_goal = "rmsep", cumulative_variance_threshold = 0.85,property_name = NULL,include_zv = FALSE, variance_threshold = NULL, include_pca = FALSE,  n_comp_pca_variance = NULL,num_pca_comp = NULL,  n_comp_pca = NULL) { 
  message("DEBUG: n_comp_pca inside fit_model(): ", n_comp_pca)
  
  model_spec <- model_spec_list$model_spec
  mtry_range <- model_spec_list$mtry_range  # Only for Random Forest

  # Set up workflow with recipe and model
  biochar_workflow <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(model_spec)
 
   # Setting defalt CV
    set.seed(123)
  cv_folds <- if (nrow(train_data) <= 10) {
    vfold_cv(train_data, v = nrow(train_data))  
  } else {
    vfold_cv(train_data, v = 10)  
  }

  if (model_type == "pls") {
    
      # Define cross-validation folds (LOOCV for <= 10 samples)
  set.seed(123)  # Ensure reproducibility
  cv_folds <- if (nrow(train_data) <= 10) {
    vfold_cv(train_data, v = nrow(train_data))  # Leave-One-Out CV
  } else {
    vfold_cv(train_data, v = 10)  # 10-fold CV
  }
      # Ensure max_comps considers both num_pca_comp and variance_threshold dynamically
    if (isTRUE(include_pca) && !is.null(n_comp_pca)) {
    max_comps <- min(25, nrow(train_data) - 1,  ncol(train_data), n_comp_pca)
  } else {
    max_comps <- min(25, nrow(train_data) - 4,  ncol(train_data))
  }

    if (tuning_goal == "rmsep") {
      # Perform 10-fold (or LOOCV) to minimize RMSEP
      num_comp_grid <- tibble(num_comp = 1:max_comps)
      tune_results <- tune_grid(
        biochar_workflow,
        resamples = cv_folds,
        grid = num_comp_grid,
        metrics = metric_set(rmse),
        control = control_grid(save_pred = TRUE)
      )

      best_params <- tune_results %>% select_best(metric = "rmse") %>% dplyr::select(num_comp)
      final_workflow <- finalize_workflow(biochar_workflow, best_params)
      
      #*Only Generate RMSEP Plot for PLS Models**
      if (!is.null(property_name)) {
        rmsep_data <- tune_results %>%
          collect_metrics() %>%
          filter(.metric == "rmse") %>%
          dplyr::select(num_comp, mean)

        rmsep_plot <- ggplot(rmsep_data, aes(x = num_comp, y = mean)) +
          geom_line() +
          geom_point() +
          theme_minimal() +
          labs(
            title = paste("PLS: Comp vs RMSEP for", property_name),
            x = "Number of Components",
            y = "RMSEP"
          )

        print(rmsep_plot)
      }  # Display the plot in R

    } else if (tuning_goal == "variance") {
      
        # Define cross-validation folds (LOOCV for <= 10 samples)
      set.seed(123)  # Ensure reproducibility
      cv_folds <- if (nrow(train_data) <= 10) {
        vfold_cv(train_data, v = nrow(train_data))  # Leave-One-Out CV
      } else {
        vfold_cv(train_data, v = 10)  # 10-fold CV
      }
      
      # Calculate cumulative explained variance
      response_var <- colnames(train_data)[[2]]  # Adjust index if response var is at a different position
      explained_variance <- calculate_explained_variance(train_data, response_var, max_comps)

      # Find the minimum number of components meeting the variance threshold
      num_comp <- which(explained_variance >= cumulative_variance_threshold)[1]
      if (is.na(num_comp) || is.null(num_comp)) {
        message(paste("Skipping property:", response_var, "due to insufficient cumulative variance."))
        return(tibble(
          Property = response_var,
          Data_Type = "Skipped",
          Model = model_type,
          N = NA,
          R2 = NA,
          RMSE = NA,
          bias = NA,
          ncomp = NA
        ))
      }

      # Validate the selected number of components with cross-validation
      num_comp_grid <- tibble(num_comp = num_comp)
      tune_results <- tune_grid(
        biochar_workflow,
        resamples = cv_folds,
        grid = num_comp_grid,
        metrics = metric_set(rmse),
        control = control_grid(save_pred = TRUE)
      )
    best_params <- tibble(num_comp = num_comp)
    final_workflow <- finalize_workflow(biochar_workflow, best_params)
    }
  } else if (model_type == "rf") {
  set.seed(123)
  
  
  # Dynamically adjust folds
  cv_folds <- if (nrow(train_data) <= 20) {
    vfold_cv(train_data, v = min(floor(nrow(train_data) / 2), 5))  # Cap folds for small datasets
  } else {
    vfold_cv(train_data, v = min(10, nrow(train_data) - 1))  # Default for larger datasets
  }
  
  # Log fold details
  #message("Number of folds: ", length(cv_folds$splits))
  #test_sizes <- purrr::map_int(cv_folds$splits, ~ nrow(assessment(.x)))
  #train_sizes <- purrr::map_int(cv_folds$splits, ~ nrow(analysis(.x)))
  #message("Test set sizes: ", paste(test_sizes, collapse = ", "))
  #message("Train set sizes: ", paste(train_sizes, collapse = ", "))
  
  # Fallback for very small datasets
  # if (nrow(train_data) < 20 || min(test_sizes) < 5) {
  #   warning("Dataset too small for cross-validation. Using fixed train-test split.")
  #   
  #   split <- initial_split(train_data, prop = 0.8)
  #   inner_train <- training(split)
  #   inner_test <- testing(split)
    
    # if (nrow(inner_train) < 5 || nrow(inner_test) < 2) {
    #   stop("Insufficient data for fixed split. Skipping property.")
    # }
    
    rf_model <- rand_forest(
      mtry = floor(sqrt(ncol(train_data) - 1)),# overridden by tuning grid
      min_n = 2,# overridden by tuning grid
      trees = 1000
    ) %>%
      set_engine("ranger") %>%
      set_mode("regression")
    
    final_workflow <- workflow() %>%
      add_recipe(recipe) %>%
      add_model(rf_model)
    
    #model_fit <- fit(final_workflow, data = train_data)
    #predictions <- predict(model_fit, inner_test)
    
    # return(list(model = model_fit, predictions = predictions))
  #}
  
  # Dynamically adjust grid size
  rf_grid <- grid_random(
    mtry_range,
    min_n(range = c(1, min(5, floor(nrow(train_data) / 3)))),  # Smaller range for min_n
    size = min(nrow(train_data) / length(cv_folds$splits), 5)  # Adjust grid size dynamically
  )
  
  # Log grid size
  #message("Grid size: ", nrow(rf_grid))
  
  # Tune the model
  tune_results <- tune_grid(
    biochar_workflow,
    resamples = cv_folds,
    grid = rf_grid,
    metrics = metric_set(rmse),
    control = control_grid(save_pred = TRUE)  # Add verbose output
  )
  
  best_params <- tune_results %>% select_best(metric = "rmse")
  final_workflow <- finalize_workflow(biochar_workflow, best_params)
  
  } else if (model_type == "brnn") {
       num_predictors <- ncol(train_data) - 2  # Adjust if more metadata columns exist
       max_hidden_units <- min(20, floor(log2(num_predictors)))  # Choose sqrt(features) but cap at 5

    brnn_grid <- grid_random(
      hidden_units(range = c(1, max_hidden_units)),
      penalty(range = c(0.001, 0.1)),
      size = 10
    )
    
    tune_results <- tune_grid(
      biochar_workflow,
      resamples = cv_folds,
      grid = brnn_grid,
      metrics = metric_set(rmse),
      control = control_grid(save_pred = TRUE)
    )

    best_params <- tune_results %>% select_best(metric = "rmse")
    final_workflow <- finalize_workflow(biochar_workflow, best_params)
    
    } else if (model_type == "gbdt") {
      
      set.seed(123)
    #cv_folds <- vfold_cv(train_data, v = min(10, nrow(train_data) - 1))  # Reduce CV folds
    cv_folds <- vfold_cv(train_data, v = ifelse(nrow(train_data) < 30, 5, min(10, nrow(train_data) - 1)))
      # Define a GBDT-specific tuning grid
      gbdt_grid <- grid_random(
        trees(range = c(50, 200)),  # Tune number of trees #tried 300
        #tree_depth(range = c(3, 10)),  # Tune tree depth
        tree_depth(range = c(2, min(4, round(log2(nrow(train_data)))))),  # Log-based depth scaling
        learn_rate(range = c(0.005, 0.03 )),  # Tune learning rate #Tried (0.1, 0.3)
        loss_reduction(range = c(0, 5)),  # Add back to control unnecessary splits
        min_n(range = c(2, min(10, round(0.05 * nrow(train_data))))),  # Bring back to prevent overfitting
        mtry(range = c(ceiling(0.05 * ncol(train_data)), min(15, floor(0.4 * ncol(train_data))))),
        # Proportion of predictors sampled at each split
        size = 10 #10
      )
      
      # Tune the GBDT model using the grid
      tune_results <- tryCatch({
        tune_grid(
          biochar_workflow,
          resamples = cv_folds,
          grid = gbdt_grid,
          metrics = metric_set(rmse),
          control = control_grid(save_pred = TRUE)
        )
      }, error = function(e) {
        warning("Error during GBDT tuning: ", e$message)
        return(NULL)
      })

      # If tuning failed, return NULL
      if (is.null(tune_results)) return(NULL)

      # Select the best parameters based on RMSE
      best_params <- tune_results %>% select_best(metric = "rmse")
      # Finalize the workflow with the best parameters
      final_workflow <- finalize_workflow(biochar_workflow, best_params)
      
    }
  
  # Fit the final model
  model_fit <- fit(final_workflow, data = train_data)
  
#   model_fit <- tryCatch(
#   fit(final_workflow, data = train_data),
#   error = function(e) {
#     message("Error in model fitting: ", e$message)
#     return(NULL)
#   }
# )

  # Predictions for train and test data
  train_predictions <- tryCatch({
    predict(model_fit, new_data = train_data) %>%
    as_tibble()  # Convert predictions to a tibble
  }, error = function(e) {
    message("Error in train_predictions: ", e)
    return(NULL)
  })

  # Add rownames from `test_data` to `test_predictions`
  if (!is.null(train_predictions)) {
    rownames(train_predictions) <- rownames(train_data)
  }

  test_predictions <- tryCatch({
    predict(model_fit, new_data = test_data) %>%
    as_tibble()  # Convert predictions to a tibble
  }, error = function(e) {
    message("Error in test_predictions: ", e)
    return(NULL)
  })

  # Add rownames from `test_data` to `test_predictions`
  if (!is.null(test_predictions)) {
    rownames(test_predictions) <- rownames(test_data)
  }

  # Ensure predictions are non-null
  if (is.null(train_predictions) || is.null(test_predictions)) {
    stop("Prediction failed: check data dimensions or NA values in predictor matrix.")
  }
  
  return(list(
    model = model_fit,
    train_predictions = train_predictions,
    test_predictions = test_predictions,
    best_params = best_params
  ))
}
```

#### PLSR

#### Tune to minimize RMSEP - MIR

selecting the number of components that minimizes the RMSEP

Total split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 19:51)
```

Source-based split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep")
```

Cornell only Predictions (focused on pH)

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep")
```

#### Tune to minimize RMSEP - MIR 80/20 Split

Need to change 0.7 to 0.8

```         
set.seed(123) 
# Split ensuring Source is represented in both subsets
trainIndex <- createDataPartition(df.sel$Source, p = 0.7, list = FALSE)

} else { set.seed(123) # Split without considering Source trainIndex <- sample(seq_len(nrow(df.sel)), size = 0.7 * nrow(df.sel))
```

selecting the number of components that minimizes the RMSEP

Total split

```{r}
# 
# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
#                    use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

Source-based split

```{r}
# 
# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_sourcesplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
#                    use_source = TRUE, tuning_goal = "rmsep")
```

Cornell only Predictions (focused on pH)

```{r}

# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_CornellOnlysplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
#                    use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep")
```

#### Lab Covariate - MIR

Zero-variance predictors donâ€™t contribute to PLS components since they hold no information.

```         
â€¢   Removing them has a marginal effect because PLSR naturally assigns zero weight to irrelevant predictors.

â€¢   Edge case: If your dataset is very small, leaving zero-variance predictors might slightly affect component selection due to noise in variance calculations.
```

Total split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_Lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep") #, property_range = 49:51)
```

Source-based split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_sourcesplit_Lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep")
```

Cornell only Predictions (focused on pH)

\*\* don't make a difference

#### PCA Components as features

```{r}

identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_PCAfeat")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", include_pca = TRUE, num_pca_comp =20) #, property_range = 2:5) 
```

\*\* Sparse Generalized Canonical Correlation Analysis (SGCCA) algorithm used in pls() modeling failed to find a stable solution within the allowed iterations.

```{r}

identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_PCAfeat_variance")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", include_pca = TRUE, variance_threshold = 0.95) #, property_range = 50:52) 
```

#### Tune to minimize RMSEP - NIR

Total split - NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_NIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep")
```

Source-based split - NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep")#, property_range = 3:4)
```

Cornell only Predictions - NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_NIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", property_range = 59:60)
```

#### Tune to minimize RMSEP - MIR + NIR

Total split - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep")
```

Source-based split - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep")
```

Cornell only Predictions - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep")
```

#### Lab Covariate - MIR + NIR

Total split - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_totsplit_Lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

Source-based split - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_sourcesplit_Lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep")
```

Cornell only Predictions - MIR + NIR

\*\* doesn't make sense to run

### RANDOM FOREST

#### Tune to minimize RMSEP - MIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

```{r}
identifier <- initialize_workspace("RF_RMSEP_MIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep") #,property_range = 25:55)

```

#### Tune to minimize RMSEP - MIR 80/20 Split

```{r}
# 
# identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
#                    use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_sourcesplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
#                    use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

```{r}
# identifier <- initialize_workspace("RF_RMSEP_MIR_CornellOnlysplit_99perc_80_20")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
#                    use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep") #,property_range = 25:55)

```

#### Lab Covariate - MIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

#### No Zero Variance - MIR

-   Identifies **columns (wavenumbers) where all values are identical** (e.g., every row has 0.5 at a certain wavenumber). These predictors provide **no information** because their standard deviation is **zero**.

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 59:63)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_sourcesplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_CornellOnlysplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)

```

#### Lab Covariate & No Zero Variance- MIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

#### PCA Components as features

Number of components

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_PCAfeat")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_pca = TRUE, num_pca_comp =15) #, property_range = 58:65) 
```

Captured variance

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_PCAfeat")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_pca = TRUE, variance_threshold = 0.95) #, property_range = 39:41 ) #property_range = 49:51,
```

#### Tune to minimize RMSEP - NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

```{r}
identifier <- initialize_workspace("RF_RMSEP_NIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep") #,property_range = 25:55)

```

#### Lab Covariate - NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

#### No Zero Variance - NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_totsplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_sourcesplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_CornellOnlysplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)

```

#### Lab Covariate & No Zero Variance - NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

#### Tune to minimize RMSEP - MIR + NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
                   use_source = FALSE, tuning_goal = "rmsep")
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep")
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell",tuning_goal = "rmsep")


#Note: skips testing of all properties - insufficient data. 
```

#### Lab Covariate - MIR + NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep")#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep") #,property_range = 25:55)
```

#### No Zero Variance - MIR +NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_totsplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_sourcesplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_CornellOnlysplit_zv")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

#### Lab Covariate & No Zero Varaiance - MIR + NIR

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_totsplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "rf", identifier,  
                   use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE)#, property_range = 49:51)
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_sourcesplit_lab")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR_lab, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", include_zv = TRUE) #,property_range = 25:55)
```

## GBDT

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("GBDT_RMSEP_MIR_totsplit")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "gbdt",  # Specify GBDT
  use_source = FALSE,
  tuning_goal = "rmsep",
  identifier,
  property_range = 52:55#length(slprptr)
)
```

```{r}

# # Run analysis for all properties
# identifier <- initialize_workspace("GBDT_RMSEP_MIR_sourcesplit") 
# 
# run_all_properties(
#   slprptr = slprptr,
#   df1 = df1,
#   spec_trt = spec_trt,
#   model_type = "gbdt",  # Specify GBDT
#   use_source = TRUE,
#   tuning_goal = "rmsep",
#   identifier,
#   property_range = 2:length(slprptr)
# )
```

```{r}

# # Run analysis for all properties
# identifier <- initialize_workspace("GBDT_RMSEP_MIR_CornellOnlysplit") 
# 
# run_all_properties(
#   slprptr = slprptr,
#   df1 = df1,
#   spec_trt = spec_trt,
#   model_type = "gbdt",  # Specify GBDT
#   use_source = FALSE,
#   source_name = "Cornell ",
#   tuning_goal = "rmsep",
#   identifier,
#   property_range = 2:length(slprptr)
# )
```

## BRNN

#### Tune to minimize RMSEP - MIR

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_totsplit")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  property_range = 52:56, #length(slprptr),
  include_pca = TRUE,
  identifier,
  num_pca_comp = 20
  #variance_threshold = 0.95
)
```

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_sourcesplit")
  
run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "brnn",
  use_source = TRUE,
  tuning_goal = "rmsep",
  property_range = 52:56, #length(slprptr),
  include_pca = TRUE,
  identifier,
  variance_threshold = 0.95
)
```

#### Lab Covariate - MIR

```{r}
identifier <- initialize_workspace("BRNN_RMSEP_MIR_totsplit_lab")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_lab,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  include_pca = TRUE
)
```

```{r}
identifier <- initialize_workspace("BRNN_RMSEP_MIR_sourcesplit_lab")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_lab,
  model_type = "brnn",
  use_source = TRUE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  identifier,
  include_pca = TRUE
)
```

#### Tune to minimize RMSEP - MIR + NIR

```{r}
identifier <- initialize_workspace("BRNN_RMSEP_MIR_NIR_totsplit")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_NIR,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  identifier,
  include_pca = TRUE
)
```

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_NIR_sourcesplit")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_NIR,
  model_type = "brnn",
  use_source = TRUE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  identifier,
  include_pca = TRUE
)
```

#### Lab Covariate - MIR

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_NIR_totsplit_lab")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_NIR_lab,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  identifier,
  include_pca = TRUE
)
```

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_NIR_sourcesplit_lab")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt_MIR_NIR_lab,
  model_type = "brnn",
  use_source = TRUE,
  tuning_goal = "rmsep",
  property_range = 2:length(slprptr),
  identifier,
  include_pca = TRUE
)
```

#### 
