---
title: "CEBUM_PLSR_Workflow"
output: html_document
date: "2024-10-14"
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries

```{r, include=FALSE}

# Define a list of required packages
required_packages <- c("writexl","readxl","randomForest", "caret", "pls", "data.table", "ithir", 
                       "dplyr", "tidyr", "prospectr", "globals", "stringr", 
                       "ggplot2", "here", "tidymodels", "parsnip", "plsmod", 
                       "mixOmics", "yardstick", "purrr", "furrr", "tibble","ranger","VIM","shiny","FNN","lightgbm", "xgboost", "dials", "tune", "brnn", "nnet", "recipes", "doParallel","progressr")

#VIM for KNN imputation
# Load all required packages quietly without outputting any list of packages
suppressPackageStartupMessages(
  invisible(lapply(required_packages, library, character.only = TRUE))
)

# Ensure dplyr functions are prioritized
conflicted::conflict_prefer("select", "dplyr")
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflicts_prefer(purrr::map)
conflicted::conflicts_prefer(parsnip::tune)
conflicted::conflicts_prefer(dplyr::summarise)

set.seed(345)

# cores <- parallel::detectCores() - 1
# cl <- makePSOCKcluster(cores)
# registerDoParallel(cl)

# progressr::handlers(
#   progressr::handler_cli(
#     format = ":message :bar (:current/:total) :percent",
#     width  = 40
#   )
# )

plan(
  multisession,
  workers  = 4
  # tweak    = list(seed = TRUE)
)

#plan(multisession, workers = 4, tweak = list(seed = TRUE))
options(future.seed = TRUE)
```

## Load reference data

```{r}

# treated spectra
spec_trt <- readRDS("spec_trt_MIR.RDS")

spec_trt_MIR_lab <- readRDS("spec_trt_MIR_lab.RDS")
spec_trt_NIR <- readRDS("/Users/soliverchefusi/Library/CloudStorage/OneDrive-Personal/R/ICRAF&Lehmann/ICRAF_IR/ICRAF_IR_R_Project/ICRAF_IR/CEBUM/CEBUM_NIR/CEBUM_NIR/spec_trt_NIR.RDS")
spec_trt_NIR_lab <- readRDS("/Users/soliverchefusi/Library/CloudStorage/OneDrive-Personal/R/ICRAF&Lehmann/ICRAF_IR/ICRAF_IR_R_Project/ICRAF_IR/CEBUM/CEBUM_NIR/CEBUM_NIR/spec_trt_NIR_lab.RDS")

spec_trt <- spec_trt[,-2] #removing the data for the first wavenumber since it's all NAN's
spec_trt_MIR_lab <- spec_trt_MIR_lab[,-2]

spec_trt_MIR_NIR <- merge(spec_trt,spec_trt_NIR, by ="SSN")
spec_trt_MIR_NIR_lab <- merge(spec_trt_MIR_lab,spec_trt_NIR_lab, by ="SSN")
rownames(spec_trt_MIR_NIR_lab) <- spec_trt_MIR_NIR_lab$SSN

# biochar property data

#df1<-readRDS("df.f.RDS")

df1 <- readRDS("df2_CHN.RDS")


drop_vars <- c(
  "C_tot_avg", "C_org_wt", "C_inorg_wt", "N_tot_avg_wt",
  "H_tot_wt", "O_ult",
  "C_tot_to_N_wt", "C_org_to_N_wt","X15N",
  "H_tot_to_C_tot_wt", "H_tot_to_C_org_wt",
  "Oult_to_C_tot_wt", "Oult_to_C_org_wt",
  "Decomposition_36_month_perc_remaining", "C_Stability_perc_C",
  "sr_02", "sr_05", "sr_2", "sr_7",
  "Tiss_N_02", "Tiss_N_05", "Tiss_N_2", "Tiss_N_7"
)

df1 <- df1 %>% dplyr::select(-any_of(drop_vars))


df1 <- df1 %>% 
  mutate(
    sum_base_cations = rowSums(
      across(c(exchg_Ca_mmol_kg,
               exchg_K_mmol_kg,
               exchg_Mg_mmol_kg,
               exchg_Na_mmol_kg,
               exchg_P_mmol_kg)),
      na.rm = TRUE          # treat missing values as 0
    )
  )

# df.f: full dataset;
# spec_trt: treated spectra

#Available properties to predict
slprptr<-names(df1[-c(2:9)]) #FUSI EDIT: removing metadata columns 

# defining identity object with names of biochar samples
pred<-as.data.frame(spec_trt[,1])
colnames(pred)<-"SSN"
```

## PLSR, RF, GBDT, BRNN Model Workflow

Models based on minimizing RMSEP (objective function)

High-Level Structure:

```         
1.  Preprocessing

2.  Setting up models 

3.  Running models (map() over models to fit them)

4.  Calculating metrics (for each model)

5.  Plotting results (for each model)
```

### Initializing Repository for Outputs

```{r}

# Initialize the workspace by creating directories for storing outputs and 
# setting up an empty tibble to accumulate performance metrics 
initialize_workspace <- function(identifier) {
  #dir.create(paste0("Plots_Validation_MIR_", identifier), showWarnings = FALSE)
  dir.create(paste0("/Volumes/Storage/CEBUM/Plots_WellFitted_Properties_", identifier), showWarnings = FALSE)
  all_metrics <<- tibble()  # Global tibble to store metrics for each property
  
  return(identifier)  # Pass the identifier for later use
}
```

### Selecting Property to then iterate over

```{r}

# slprptr: list of properties; .x: index of property in slprptr
select_property_data <- function(slprptr, .x, df1, spec_trt) {
  # Filter df1 to create df.f based on SSN present in spec_trt
    df.f <- df1 %>%
    filter(SSN %in% spec_trt$SSN) # Select only samples present in both predictor and response datasets
  # Check if the property exists in df1
  if (!slprptr[.x] %in% colnames(df.f)) {
    stop(paste("Property", slprptr[.x], "not found in df1"))
  }
  
  # Columns to select, ensuring required columns are present
  cols_to_select <- c("SSN", "Source", slprptr[.x])
  cols_to_select <- intersect(cols_to_select, colnames(df.f))
  
  if (!all(cols_to_select %in% colnames(df.f))) {
    stop(paste("Columns", paste(cols_to_select, collapse = ", "), "not found in df.f"))
  }
  
  # Filter and preprocess data, including removing outliers for the selected property
    df.sel <- df.f %>%
    dplyr::select(dplyr::all_of(cols_to_select)) %>%
    inner_join(spec_trt, by = "SSN") %>%   # Combine predictor and response variables into one df
    na.omit()#%>%
    #filter(data.table::between(!!sym(slprptr[.x]), quantile(!!sym(slprptr[.x]), 0.01), quantile(!!sym(slprptr[.x]), 0.99))) # Filter out extremes
  
  # Set SSN as rownames but keep the column
  rownames(df.sel) <- df.sel$SSN
  # Check if df.sel has rows
  if (nrow(df.sel) == 0) {
    stop(paste("No matching rows in df.sel for property:", slprptr[.x]))
  }
  
  return(list(df.f = df.f, df.sel = df.sel))
}
```

### Defining Bootstrapped Training/Test Split

```{r}

generate_bootstrap_splits <- function(df.sel, num_bootstrap, use_source = FALSE, source_name = NULL) {
  #set.seed(123)

  # If a specific lab is selected, filter data
  if (!is.null(source_name)) {
    df.sel <- df.sel %>% filter(Source == source_name)
  }

  # Ensure we have enough samples
    # If there aren't enough samples, return NULL instead of stopping
  if (nrow(df.sel) < 30) {
    message("Skipping property due to insufficient data after filtering.")
    return(NULL)
  }

  # Generate bootstrap resamples with or without stratification
  if (use_source) {
    boot_splits <- bootstraps(df.sel, times = num_bootstrap, strata = "Source")
  } else {
    boot_splits <- bootstraps(df.sel, times = num_bootstrap)
  }
  
  ##DEBUG
  #message("  ▶︎ Generated ", length(boot_splits$splits))# , " bootstraps; first split sizes: ")#,
        #nrow(analysis(boot_splits$splits[[1]])), "/", 
        #nrow(assessment(boot_splits$splits[[1]])))
  return(boot_splits)
  
}

# Function to split bootstrap resample into train/test
split_bootstrap_sample <- function(boot_sample) {
  train_data <- analysis(boot_sample)
  test_data <- assessment(boot_sample)
  
  # Tag SSN and set rownames
  
    train_data <- train_data %>%
      dplyr::mutate(SSN = paste0(SSN, "_", sprintf("%03d", row_number())))
    test_data <- test_data %>%
      dplyr::mutate(SSN = paste0(SSN, "_", sprintf("%03d", row_number())))
    rownames(train_data) <- train_data$SSN
    rownames(test_data) <- test_data$SSN
  
  #Remove Source column before model training
  train_data <- train_data %>% dplyr::select(-Source)
  test_data <- test_data %>% dplyr::select(-Source)
  
  #DEBUG
 # print(class(train_data))
  return(list(train_data = train_data, test_data = test_data))
  
}
```

### Preprocess Data/ Defining Recipe

New preprocess that swallows prepare

```{r}

preprocess_data <- function(boot_data, slprptr, p,
                            include_zv   = FALSE,
                            include_pca  = FALSE,
                            num_pca_comp = NULL,
                            variance_threshold = NULL,
                            include_pls = FALSE,
                            variance_threshold_pls = NULL
                            #forest_threshold = NULL
                            #include_rf = FALSE
                            ) {
  # boot_data: list of lists with $train_data and $test_data
  safe_map <- function(ls, fn) purrr::map(ls, ~ tryCatch(fn(.x),
                                                          error = function(e) NULL))
  
  #split train and test to verify they are sufficient to proceed 
  valid_splits <- safe_map(boot_data, function(split_pair) {
    train_data <- split_pair$train_data
    test_data <- split_pair$test_data
    
    
    # 2) Ensure response column present
    if (!slprptr[p] %in% colnames(train_data)) return(NULL)
    

    
   ## DEFINE RECIPE
    
        formula <- as.formula(paste(slprptr[p], "~ ."))
        
        recipe_base <- recipe(formula, data = train_data) %>%
            update_role(SSN, new_role = "ID")
        

        if (include_zv) {
            recipe_base <- recipe_base %>% step_zv() #%>% step_nzv()
        }

        # # Initialize feature reduction variables
        n_comp_pca <- NA_integer_
        n_comp_pls <- NA_integer_
        #n_features_rf <- NA_integer_

        # =========================== PCA ===========================
        if (include_pca) {
            if (!is.null(num_pca_comp)) {
                num_pca_comp_set <- min(
                    num_pca_comp,
                    ncol(train_data),
                    nrow(train_data) - 1,
                    max(10, floor(nrow(train_data) * 0.2))
                )
                    # *** Check whether the result is length zero
    if (length(num_pca_comp_set) == 0 || is.na(num_pca_comp_set)) {
      message("Skipping property due to inability to determine a valid PCA component count.")
      return(NULL)
    }
                if (num_pca_comp_set > (nrow(train_data) - 1) || num_pca_comp_set > (ncol(train_data) - 1)) {
                    message("Warning: num_pca_comp (", num_pca_comp, ") too high. Using fallback value of 5 components.")
                    num_pca_comp_set <- 5
                }

                recipe_base <- recipe_base %>%
                    step_pca(all_numeric_predictors(), num_comp = num_pca_comp_set)
                 # Wrap prep() in tryCatch so that if PCA fails, we return NULL
    prepped_recipe <- tryCatch({
      prep(recipe_base, training = train_data, retain = TRUE)
    }, error = function(e) {
      message("⚠️ PCA prep failed: ", e$message)
      return(NULL)
    })
    
    if (is.null(prepped_recipe)) {
      message("Skipping property due to PCA failure.")
      return(NULL)
    }
                n_comp_pca <- num_pca_comp_set
            } else if (!is.null(variance_threshold)) {
                recipe_base_pca_variance <- recipe_base %>%
                    step_pca(all_numeric_predictors(), threshold = variance_threshold)

                prepped_recipe <- prep(recipe_base_pca_variance, training = train_data, retain = TRUE)
                n_comp_pca_variance <- prepped_recipe$steps[[1]]$num_comp

                num_pca_comp_set <- max(
                    min(n_comp_pca_variance, ncol(train_data), nrow(train_data) - 1),
                    max(min(10, floor(nrow(train_data) * 0.2)), 2)
                )

                recipe_base <- recipe_base %>%
                    step_pca(all_numeric_predictors(), num_comp = num_pca_comp_set)

                n_comp_pca <- num_pca_comp_set
            } else {
                warning("PCA enabled but neither num_pca_comp nor variance_threshold provided. PCA not applied.")
            }
        }

        if (include_pca && (length(n_comp_pca) == 0 || is.na(n_comp_pca))) {
            stop("ERROR: n_comp_pca is NA despite PCA being enabled!")
        }

        # # =========================== PLS ===========================
        # if (include_pls) {
        #     if (is.null(variance_threshold_pls)) {
        #       # No threshold, use default
        #         recipe_base <- recipe_base %>%
        #             step_pls(all_numeric_predictors(), outcome = slprptr[p])
        #     } else {
        #       # Threshold given → pick the comp count that meets it
        #         max_possible_comp <- min(ncol(train_data), nrow(train_data) - 1)
        #         temp_recipe <- recipe_base %>%
        #             step_pls(all_numeric_predictors(), outcome = slprptr[p], num_comp = max_possible_comp)
        # 
        #         prepped_temp <- prep(temp_recipe, training = train_data, retain = TRUE)
        #         #pls_results <- prepped_temp$steps[[1]]$res$Xvar
        #         #cumulative_variance <- cumsum(pls_results) / sum(pls_results)
        #         var_df <- tidy(prepped_temp, number = 1)      # step_pls is the first step we just added
        #         cumulative_variance <- var_df$Xcumvar 
        #         n_comp_pls <- which(cumulative_variance >= variance_threshold_pls)[1]
        # 
        #         if (!is.na(n_comp_pls) && n_comp_pls < 2) {
        #             warning("PLS variance threshold too high, using fallback of 2 components.")
        #             n_comp_pls <- 2
        #         }
        # 
        #         recipe_base <- recipe_base %>%
        #             step_pls(all_numeric_predictors(), outcome = slprptr[p], num_comp = n_comp_pls)
        #     }
        # }
        
        #second version
        # ───────────────────── PLS dimensionality reduction ─────────────────────

# ────────── PLS dimensionality-reduction (used only when the final model ≠ parsnip::pls) ──────────

if (include_pls && model_type != "pls") {
  
  if (!is.null(variance_threshold_pls)) {                       # ── CASE 1: threshold given
    if (variance_threshold_pls <= 0 || variance_threshold_pls >= 1)
      stop("variance_threshold_pls must be a proportion between 0 and 1.")
    
    #max_comp <- min(ncol(train_data), nrow(train_data) - 2)     # keep ≥2 df
    
    tmp_rec <- recipe_base %>% 
      #step_zv(all_numeric_predictors()) %>%                     # drop flat wavelengths once
      step_pls(all_numeric_predictors(),
               outcome  = slprptr[p]) #,
               #num_comp = max_comp)
    
    tmp_prep <- tryCatch(prep(tmp_rec, training = train_data, retain = TRUE),
                         error = function(e) NULL)
    if (is.null(tmp_prep)) return(NULL)                         # skip this bootstrap split
    
    cumvar <- tidy(tmp_prep, number = 2) |>                     # step 1 = zv, step 2 = pls
              arrange(component) |> 
              pull(Xcumvar)
    
    ## -------- choose component count -----------------------------------
    target_comp  <- which(cumvar >= variance_threshold_pls)[1]  # may be integer(0)
    max_possible <- length(cumvar)                              # row-rank of X
    
    n_comp_pls <- max(2, ifelse(length(target_comp) == 0,
                                max_possible,                   # target unreachable
                                min(target_comp, max_possible)))# target reachable
    ## -------------------------------------------------------------------
    
    recipe_base <- recipe_base %>% 
      step_zv(all_numeric_predictors()) %>%                     # final recipe
      step_pls(all_numeric_predictors(),
               outcome  = slprptr[p],
               num_comp = n_comp_pls)
    
  } else {                                                      # ── CASE 2: no threshold
    recipe_base <- recipe_base %>% 
      #step_zv(all_numeric_predictors()) %>% 
      step_pls(all_numeric_predictors(), outcome = slprptr[p])  # RMSEP rule
    n_comp_pls <- 0L
  }
}
        
        
        
        
        
        # 
        # # =========================== Feature Selection (RF) ===========================
        # if (include_rf && !is.null(forest_threshold)) {
        #     message("Applying step_select_forests() with threshold: ", forest_threshold)
        #     recipe_base <- recipe_base %>% step_select_forests(all_numeric_predictors(), threshold = forest_threshold)
        # }
    
    
    
          return(list(
            recipe = recipe_base,
            train_data = train_data, #train_data,
            test_data = test_data, #test_data,
            n_comp_pca = n_comp_pca,
            n_comp_pls = n_comp_pls
            #n_features_rf = n_features_rf
        ))
    })
    
    
      # Drop any splits that failed
  valid_splits <- purrr::compact(valid_splits)
  if (length(valid_splits) == 0) {
    stop("🚨 ERROR: No valid splits after preprocess_data()!")
  }
  valid_splits #retuns 
}
```

### Tuning Models

```{r}

# Set up PLS or Random Forest model specification

setup_model <- function(model_type, preprocessed_splits, response_var = NULL) {
  
    train_data_list <- map(preprocessed_splits, "train_data")
    test_data_list <- map(preprocessed_splits, "test_data")
    
    
    # Iterate over all bootstrap splits to set up models
    models <- map(train_data_list, function(train_data) {

        #  Check if train_data exists before proceeding
        if (is.null(train_data)) {
            warning("Skipping model setup due to NULL train_data.")
            return(list(model_spec = NA)) #, mtry_range = NA))
        }

        if (nrow(train_data) == 0 || ncol(train_data) == 0) {
            warning("Skipping model setup due to empty train_data.")
            return(list(model_spec = NA)) #, mtry_range = NA))
        }
    
        if (model_type == "pls") {
            # Configure a PLS model specification
            model_spec <- parsnip::pls() %>%
                set_mode("regression") %>%
                set_engine("mixOmics") %>%
                set_args(num_comp = tune())
           # mtry_range <- NULL
        } else if (model_type == "rf") {
            # Configure a Random Forest model with tuning for mtry and min_n
            #if (is.null(response_var)) stop("response_var must be specified for Random Forest.")
            
            # Remove SSN and response columns to get only predictor columns
           # predictors_only <- train_data %>%
           #     dplyr::select(-SSN, -all_of(response_var))
            
            # # Check if predictors are sufficient
            # if (ncol(predictors_only) < 2) stop("Random Forest requires at least 2 predictor variables.")
            # 
            # # Finalize mtry_range
            # mtry_range <- dials::mtry(range = c(2, round(sqrt(ncol(predictors_only)))))
            # if (is.null(mtry_range)) stop("Failed to initialize mtry_range in setup_model(). Check predictor columns in train_data.")
            
            model_spec <- parsnip::rand_forest(mtry = tune(), min_n = tune()) %>%
                set_mode("regression") %>%
                set_engine("ranger")
        } else if (model_type == "gbdt") {
            # Gradient-Boosted Decision Tree setup
            # predictors_only <- train_data %>%
               # dplyr::select(-SSN, -all_of(response_var))
            
            model_spec <- parsnip::boost_tree(
                trees = tune(),  # Number of trees to tune
                tree_depth = tune(),  # Depth of each tree
                learn_rate = tune(),  # Learning rate
                loss_reduction = tune(),  # Minimum gain for split
                min_n = tune(),  # Tune minimal node size
                mtry = tune()  # Tune predictor sampling
            ) %>%
                set_mode("regression") %>%
                set_engine("xgboost")
            #mtry_range <- NULL  # Not applicable for GBDT
        } else if (model_type == "brnn") {
            model_spec <- parsnip::mlp(hidden_units = tune(), penalty = tune()) %>%
                set_mode("regression") %>%
                set_engine("nnet")
            #mtry_range <- NULL
        } else {
            stop("Unsupported model type: ", model_type)
        }

        return(list(model_spec = model_spec)) #, mtry_range = mtry_range))
    })

    #return(models)  # Return list of models, one per bootstrap split
    return(list(
  models = models
))
  
    
}
```

### Defining/ Calculating Model Metrics

```{r}

calculate_metrics <- function(results_list, data_type, slprptr, .x, model_type) {

  response_var <- slprptr[.x]  # Define response variable
  
    if (is.null(results_list) || length(results_list) == 0) {
    message("⚠No valid results for ", response_var, " in ", data_type)
    return(tibble(
      Mean_R2 = NA, SD_R2 = NA, 
      Mean_RMSE = NA, SD_RMSE = NA, 
      Mean_Bias = NA, SD_Bias = NA,
      Mean_MAE = NA, SD_MAE = NA,
      Mean_RPD= NA, SD_RPD = NA,
      Mean_RPIQ = NA,SD_RPIQ = NA,
      Mean_slope = NA, SD_slope = NA,
      Mean_int = NA, SD_int  = NA,
      Property = response_var, 
      Data_Type = data_type, 
      Model = model_type
    ))
  }
  
   # Extract metrics for each bootstrap iteration
  metrics_list <- map(results_list, function(result) {
    
    # Assign correct truth and prediction sets based on data_type
    if (data_type == "Calibration") {
      truth <- result$model$pre$mold$outcomes[[response_var]]  # Training data
      predictions <- result$train_predictions
    } else if (data_type == "Validation") {
      truth <- result$test_truth[[response_var]]  # Use explicitly stored test values

      predictions <- result$test_predictions
    } else {
      stop("Invalid data_type specified.")
    }

    # Ensure truth and predictions are aligned
    if (length(truth) != nrow(predictions)) {
      message("Skipping metric calculation: Length mismatch between truth and predictions")
      return(tibble(R_sq = NA, MAE = NA, RPD = NA, RPIQ = NA, slope = NA, intercept = NA, RMSE = NA, bias = NA))
    }

    # Safe computation function with error handling
    safe_compute <- function(expression) {
      tryCatch(eval(expression), error = function(e) NA)
    }

    # Calculate performance metrics
    #r2_value <- safe_compute(rsq_vec(truth = truth, estimate = predictions$.pred))
        r2_value <- (rsq_vec(truth = truth, estimate = predictions$.pred))
    rmse_value_plain <- safe_compute(rmse_vec(truth = truth, estimate = predictions$.pred))
    bias_value <- safe_compute(mean(predictions$.pred - truth, na.rm = TRUE) / mean(truth, na.rm = TRUE) * 100)
    
   
    mae_value  <- safe_compute(mae_vec(truth  = truth,
                                   estimate = predictions$.pred))
 ##ADDING
    # scale-dependent quality indices
sd_y   <- safe_compute(sd(truth,  na.rm = TRUE))
iqr_y  <- safe_compute(IQR(truth, na.rm = TRUE))

rpd_val  <- if (!is.na(sd_y)  & !is.na(rmse_value_plain) && rmse_value_plain > 0)
              sd_y  / rmse_value_plain else NA
rpiq_val <- if (!is.na(iqr_y) & !is.na(rmse_value_plain) && rmse_value_plain > 0)
              iqr_y / rmse_value_plain else NA

# calibration-line coefficients on this bootstrap
lm_fit   <- safe_compute(lm(.pred ~ truth, data = predictions))
slope_v  <- if (!is.na(lm_fit[1])) coef(lm_fit)[2] else NA
int_v    <- if (!is.na(lm_fit[1])) coef(lm_fit)[1] else NA
    
    
    
      # Calculate the range of the response variable from the test_data and RMSEP/range
  response_range <- safe_compute(max(truth, na.rm = TRUE) - min(truth, na.rm = TRUE))
  
  rmse_value <- if (!is.na(response_range) && response_range > 0) rmse_value_plain / (response_range / 10) else NA
  
    return(tibble(R_sq = r2_value, RMSE = rmse_value, bias = bias_value, MAE = mae_value, RPD  = rpd_val,
              RPIQ = rpiq_val,
              slope = slope_v,
              intercept = int_v))
  })
  
  
  

    # 🔍 **Remove NULL results before summarization**
  metrics_list <- purrr::compact(metrics_list)
  # Combine results and calculate mean & standard deviation across bootstrap iterations
  # Check if all iterations failed
   # Convert list of tibbles into a single tibble
  metrics_df <- bind_rows(metrics_list)  # Merge all iterations
  
    # Compute means and standard deviations across bootstrap iterations
  
    # **Check if we have valid metrics**
  if (length(metrics_list) == 0) {
    message("🚨 No valid results for ", response_var, " in ", data_type)
    return(tibble(
      Property = response_var, 
      Data_Type = data_type, 
      Model = model_type,
      Mean_R2 = NA, SD_R2 = NA, 
      Mean_RMSE = NA, SD_RMSE = NA, 
      Mean_Bias = NA, SD_Bias = NA,
      Mean_MAE = NA, SD_MAE = NA,
      Mean_RPD= NA, SD_RPD = NA,
      Mean_RPIQ = NA,SD_RPIQ = NA,
      Mean_slope = NA, SD_slope = NA,
      Mean_int = NA, SD_int  = NA
    ))
  }
  
  metrics_summary <- metrics_df %>%
    summarise(
      Mean_R2 = mean(R_sq, na.rm = TRUE),
      SD_R2 = sd(R_sq, na.rm = TRUE),
      Mean_RMSE = mean(RMSE, na.rm = TRUE),
      SD_RMSE = sd(RMSE, na.rm = TRUE),
      Mean_Bias = mean(bias, na.rm = TRUE),
      SD_Bias = sd(bias, na.rm = TRUE),
      Mean_MAE  = mean(MAE,  na.rm = TRUE),
      SD_MAE    = sd(MAE,    na.rm = TRUE),
      Mean_RPD  = mean(RPD,  na.rm = TRUE),
      SD_RPD    = sd(RPD,    na.rm = TRUE),
      Mean_RPIQ = mean(RPIQ, na.rm = TRUE),
      SD_RPIQ   = sd(RPIQ,   na.rm = TRUE),
      Mean_slope = mean(slope, na.rm = TRUE),
      SD_slope   = sd(slope,   na.rm = TRUE),
      Mean_int   = mean(intercept, na.rm = TRUE),
      SD_int     = sd(intercept,   na.rm = TRUE)
    ) %>%
    mutate(Property = response_var, Data_Type = data_type, Model = model_type)

  return(metrics_summary)
}
```

### Defining Models (PLSR, RF, GBDT, BRNN)

```{r}

# Wrapper function to run analysis for each property in slprptr with specified model_type
run_all_properties <- function(slprptr, df1, spec_trt, model_type, identifier,
                               num_bootstrap = 50, use_source = FALSE, source_name = NULL, 
                               tuning_goal = "rmsep", property_range = 2:length(slprptr),
                                include_zv = FALSE, 
                                include_pca = FALSE, num_pca_comp = NULL, 
                               variance_threshold = NULL,
                              include_pls = FALSE,
                              variance_threshold_pls = NULL
                               # include_rf = FALSE, forest_threshold = NULL
                               ) {

  output_dir <- paste0("Plots_WellFitted_Properties_", identifier)

  # Store model results for debugging and future analysis
  model_results <- list()

  # Loop through each property and collect metrics
  all_metrics <- 
  #   with_progress({
  # p <- progressor(along = property_range)
    furrr::future_map_dfr(property_range, function(.x) {
    tryCatch({
      message("Processing property: ", .x, " : ", slprptr[.x])
     
    #library(plsmod) 
    #set.seed(345)

    # Extract property-specific data 
    property_data <- select_property_data(slprptr, .x, df1, spec_trt)
    df.f <- property_data$df.f
    df.sel <- property_data$df.sel

    # Check if there are enough observations to proceed 
    if (nrow(df.sel) <= 20) {
      message(paste("Skipping property:", slprptr[.x], "due to insufficient data"))
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = model_type, R_sq = NA, MAE = NA, RPD = NA, RPIQ = NA, slope = NA, intercept = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
  # Generate bootstrap resamples
    boot_splits <- generate_bootstrap_splits(df.sel, num_bootstrap, use_source, source_name)
    
    # If boot_splits is NULL, skip this property
if (is.null(boot_splits)) {
  message("Skipping property due to insufficient data.")
  return(tibble(
    Property = slprptr[.x],
    Data_Type = "Skipped",
    Model = model_type,
    R_sq = NA,
    MAE = NA,
    RPD = NA, RPIQ = NA, slope = NA, intercept = NA,
    RMSE = NA,
    bias = NA,
    ncomp = NA
  ))
}

      boot_data <- tryCatch({
  map(boot_splits$splits, split_bootstrap_sample)  # Pass rsplit objects directly
}, error = function(e) {
  message("Error in mapping split_bootstrap_sample: ", e$message)
  NULL
}) 
      
      
          # If boot_data is NULL, skip processing
    if (is.null(boot_data)) {
      message("Skipping property due to bootstrap failure: ", slprptr[.x])
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = model_type, R_sq = NA, MAE = NA, RPD = NA, RPIQ = NA, slope = NA, intercept = NA, RMSE = NA, bias = NA, ncomp = NA))
    }

    # Prepare bootstrap train/test splits
    # valid_splits <- prepare_model_data(boot_data, slprptr, .x)
    
    # if (is.null(valid_splits)) {
    # stop("Error: valid_splits is NULL for property ", slprptr[.x])
    # }
    
    
   preprocessed_splits <- tryCatch({
    preprocess_data(boot_data, slprptr, .x,
                    include_zv = include_zv,
                    include_pca = include_pca,
                    num_pca_comp = num_pca_comp, 
                    variance_threshold = variance_threshold,
                    include_pls = include_pls,
                    variance_threshold_pls = variance_threshold_pls
                    # include_rf = include_rf, forest_threshold = forest_threshold
                    )
}, error = function(e) {
    message(" Error in preprocess_data() for ", slprptr[.x], ": ", e$message)
    return(NULL)
})
   if (is.null(preprocessed_splits)) {
      return(tibble(Property = slprptr[.x],
                    Data_Type = "Skipped",
                    Model = model_type,
                    R_sq = NA, MAE = NA, RPD = NA, RPIQ = NA, slope = NA, intercept = NA, RMSE = NA, bias = NA, ncomp = NA))
    }


# 🔍 **Check if preprocessed_splits is NULL**
# if (is.null(preprocessed_splits)) {
#     stop(" ERROR: preprocessed_splits is NULL after preprocess_data()!")
# }
    
    # # Ensure train/test data is not empty after preprocessing 
    # if (nrow(preprocessed_splits$train_data) == 0 || nrow(preprocessed_splits$test_data) == 0) {
    #   message(paste("Skipping property:", slprptr[.x], "due to empty train/test data after preprocessing."))
    #   return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = model_type, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    # }

    # # Skip properties with insufficient train/test data
    # if (nrow(valid_splits$train_data) <= 10 || nrow(valid_splits$test_data) <= 10) {
    #   message(paste("Skipping property:", slprptr[.x], "due to insufficient train/test data"))
    #   return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = model_type, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    # }
   
       # Set up the model specification 
    if (model_type %in% c("pls", "rf", "gbdt", "brnn")) {
      model_spec_list <- setup_model(model_type, preprocessed_splits, response_var = slprptr[.x])$models
    } else {
      stop(paste("Unsupported model type:", model_type))
    }
   
   # Compute the global minimum PCA component count across iterations,
   if (include_pca) {
  n_comp_pca_list <- purrr::map_dbl(preprocessed_splits, "n_comp_pca")
  min_n_comp_pca  <- min(n_comp_pca_list, na.rm = TRUE)
} else {
  min_n_comp_pca <- NULL  # or however you signal “no PCA”
}

   
# n_comp_pca_list <- purrr::map_dbl(preprocessed_splits, "n_comp_pca")
# 
# min_n_comp_pca <- min(n_comp_pca_list, na.rm = TRUE)
# min_n_comp_pca

   
   tuned_models <- tryCatch({
     fit_model(
  model_spec_list = model_spec_list,
  #model_spec_list = purr::map(model_spec_list ... over both elements)
  train_data_list = map(preprocessed_splits, "train_data"),
  test_data_list = map(preprocessed_splits, "test_data"),
  recipe_list = map(preprocessed_splits, "recipe"),
  preprocessed_splits = preprocessed_splits,
  model_type = model_type,
  tuning_goal = tuning_goal,
  property_name = slprptr[.x],
           include_zv = include_zv,
           include_pca = include_pca,
           num_pca_comp = num_pca_comp,
           variance_threshold = variance_threshold,
           include_pls = include_pls,
           variance_threshold_pls = variance_threshold_pls,
          # n_comp_pca = map(preprocessed_splits, "n_comp_pca"),
          # n_comp_pca = preprocessed_splits[[1]]$n_comp_pca
          n_comp_pca = min_n_comp_pca
          # n_comp_pls = map(preprocessed_splits, "n_comp_pls"),
          # include_rf = include_rf,
          # forest_threshold = forest_threshold
        )
   }, error = function(e) {
       message("Model fitting failed for property: ", slprptr[.x],
          "\nError: ", e$message)
              return(list(model = NA, train_predictions = NA, test_predictions = NA, best_params = NA))
            })
   
      # Debug dump of the entire tuned_models list
#message("==== tuned_models dump for property ", slprptr[.x], " ====")
# What kind of object is it?
#cat("Class of tuned_models:", class(tuned_models), "\n")
# What slots/elements does it have?
#print(names(tuned_models))

 # print(str(tuned_models))
   
     ##DEBUG with Kostia 
   
  #  if (is.null(tuned_models) ||
  #   !("model" %in% names(tuned_models))  {
  #     ##||
  #  ## length(tuned_models$model) == 0 ||
  #  ## is.na(tuned_models$model[1]))
  # message("Skipping property: ", slprptr[.x], " due to model fitting failure.")
  # return(tibble(
  #   Property  = slprptr[.x],
  #   Data_Type = "Skipped",
  #   Model     = model_type,
  #   R_sq      = NA,
  #   RMSE      = NA,
  #   bias      = NA,
  #   ncomp     = NA
  # ))
  #  }

          
    # Compute predictions for training and test data
    cal_predictions <- map(tuned_models, "train_predictions")
    val_predictions <- map(tuned_models, "test_predictions")
  
    # # Calculate performance metrics 
    
  cal_metrics <- calculate_metrics(
  results_list = tuned_models, 
  data_type = "Calibration", 
  slprptr = slprptr, 
  .x = .x, 
  model_type = model_type
)
   
  val_metrics <- calculate_metrics(
  results_list = tuned_models, 
  data_type = "Validation", 
  slprptr = slprptr, 
  .x = .x, 
  model_type = model_type
  # Extract test data for all bootstrap iterations
)

    # Combine Calibration and Validation metrics
    return(bind_rows(cal_metrics, val_metrics))
  }, error = function(e) {
    message("Error processing property ", slprptr[.x], ": ", e$message)
    tibble(Property = slprptr[.x],
           Data_Type = "Skipped",
           Model = model_type,
           R_sq = NA, MAE = NA, RPD = NA, RPIQ = NA, slope = NA, intercept = NA, RMSE = NA, bias = NA, ncomp = NA)
  })
      #p()
},
  .options = furrr_options(seed = TRUE,
                           packages = c(
        "plsmod", "ranger", "dials", "tune", "workflows", "rsample", "yardstick")
)
  )
    # ensure the output directory exists
  if (!dir.exists(output_dir)) {
    dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)
  }
  #}) ### close progress here
    output_file <- file.path(
    output_dir,
    paste0("bootstrap_metrics_", identifier, ".xlsx")
  )
  writexl::write_xlsx(all_metrics, path = output_file)
  message("Metrics saved to: ", output_file)
  
  return(all_metrics)
  }

```

### Fitting Model (tuning hyperparameters)

```{r}

fit_model <- function(model_spec_list,
                      preprocessed_splits,
                      train_data_list,
                      test_data_list,
                      recipe_list,
                      model_type,
                      tuning_goal = "rmsep",
                      cumulative_variance_threshold = 0.85,
                      property_name = NULL,
                      include_zv = FALSE,
                      variance_threshold = NULL,
                      include_pca = FALSE,
                      include_pls = FALSE,
                      # n_comp_pca_variance = NULL,
                      num_pca_comp = NULL,
                      n_comp_pca = NULL,
                      n_comp_pls = NULL,
                      variance_threshold_pls = NULL
                      # include_rf = FALSE,
                      # forest_threshold = NULL
) {

  # Iterate over bootstrap samples
  results_list <- map2(
    seq_along(train_data_list),
    train_data_list,
    function(iter, train_data) {
      test_data <- test_data_list[[iter]]
      recipe    <- recipe_list[[iter]]

      # Skip if train_data or test_data is NULL or empty
      if (is.null(train_data) ||
          is.null(test_data) ||
          nrow(train_data) == 0 ||
          nrow(test_data) == 0) {
        message("Skipping iteration ", iter, " due to missing or empty data")
        return(NULL)
      }

      model_spec <- model_spec_list[[iter]]$model_spec

      # Set up workflow
      biochar_workflow <- workflow() %>%
        add_recipe(recipe) %>%
        add_model(model_spec)

      # Cross‐validation splits
      cv_folds <- if (nrow(train_data) <= 10) {
        vfold_cv(train_data, v = nrow(train_data))
      } else {
        vfold_cv(train_data, v = 10)
      }

      if (model_type == "pls") {
        # Determine max_comps
        if (isTRUE(include_pca) && !is.null(n_comp_pca)) {
          max_comps <- min(25, nrow(train_data) - 1, ncol(train_data), n_comp_pca)
        } else {
          max_comps <- min(25, nrow(test_data) - 4, ncol(test_data))
        }
        if (max_comps < 1) {
          message("Skipping property ", slprptr[.x], " because max_comps < 1")
          return(tibble(
            Property  = slprptr[.x],
            Data_Type = "Skipped",
            Model     = model_type,
            R_sq      = NA,
            MAE       = NA,
            RPD = NA, RPIQ = NA, slope = NA, intercept = NA,
            RMSE      = NA,
            bias      = NA,
            ncomp     = NA
          ))
        }

        if (tuning_goal == "rmsep") {
          num_comp_grid <- tibble(num_comp = 1:max_comps)
          tune_results  <- tryCatch({
            tune_grid(
              biochar_workflow,
              resamples = cv_folds,
              grid      = num_comp_grid,
              metrics   = metric_set(rmse),
              control   = control_grid(
                save_pred      = TRUE,
                parallel_over  = "everything"
              )
            )
          }, error = function(e) {
            warning("⚠️ Error in PLS tuning: ", e$message)
            return(NULL)
          })
          if (is.null(tune_results) || nrow(tune_results) == 0) {
            message("⚠️ No PLS tuning results; skipping ", property_name)
            return(list(
              model             = NA,
              train_predictions = NA,
              test_predictions  = NA,
              best_params       = NA,
              test_truth        = test_data
            ))
          }
        # Extract the best number of components from the tuning results**
          best_params <- tune_results %>% select_best(metric = "rmse")
          final_workflow <- finalize_workflow(biochar_workflow, best_params)

        } else if (tuning_goal == "variance") {
          response_var       <- colnames(train_data)[[2]]
          explained_variance <- calculate_explained_variance(train_data, response_var, max_comps)
          num_comp           <- which(cumsum(explained_variance)/sum(explained_variance) >= cumulative_variance_threshold)[1]
          num_comp_grid      <- tibble(num_comp = num_comp)
          tune_results       <- tune_grid(
            biochar_workflow,
            resamples = cv_folds,
            grid      = num_comp_grid,
            metrics   = metric_set(rmse),
            control   = control_grid(
              save_pred     = TRUE,
              parallel_over = "everything"
            )
          )
          best_params    <- tibble(num_comp = num_comp)
          final_workflow <- finalize_workflow(biochar_workflow, best_params)
        }

      } else if (model_type == "rf") {
        cv_folds <- vfold_cv(train_data, v = min(10, nrow(train_data) - 1))
        response_var    <- colnames(train_data)[[2]]
        predictors_only <- train_data %>% select(-SSN, -all_of(response_var))
        n_feats <- if (include_pca) {
          preprocessed_splits[[iter]]$n_comp_pca
        } else if (include_pls) {
          preprocessed_splits[[iter]]$n_comp_pls
        } else {
          ncol(predictors_only)
        }
        
        rf_grid <- grid_random(
          mtry(range = c(1, max(1, floor(sqrt(n_feats))))),
          min_n(range = c(1, min(5, floor(nrow(train_data)/3)))),
          size = min(nrow(train_data)/length(cv_folds$splits), 5)
        )
        
        tune_results  <- tryCatch({
          tune_grid(
            biochar_workflow,
            resamples = cv_folds,
            grid      = rf_grid,
            metrics   = metric_set(rmse),
            control   = control_grid(
              save_pred     = FALSE,
              parallel_over = "everything"
            )
          )
        }, error = function(e) {
          message("❌ RF tuning error (iter ", iter, "): ", e$message)
          NULL
        })
        
        best_params    <- tune_results %>% select_best(metric = "rmse")
        
        final_workflow <- finalize_workflow(biochar_workflow, best_params)

      } else if (model_type == "brnn") {
        brnn_grid <- grid_random(hidden_units(range = c(1,20)),
                                 penalty(range = c(0.001,0.1)),
                                 size = 10)
        tune_results <- tune_grid(
          biochar_workflow,
          resamples = cv_folds,
          grid      = brnn_grid,
          metrics   = metric_set(rmse),
          control   = control_grid(
            save_pred     = FALSE,
            parallel_over = "resamples"
          )
        )
        best_params    <- tune_results %>% select_best(metric = "rmse")
        final_workflow <- finalize_workflow(biochar_workflow, best_params)

      } else if (model_type == "gbdt") {
        gbdt_grid <- grid_random(
          trees(range = c(50,200)),
          tree_depth(range = c(2, round(log2(nrow(train_data))))),
          learn_rate(range = c(0.005,0.03)),
          loss_reduction(range = c(0,5)),
          min_n(range = c(2, round(0.05 * nrow(train_data)))),
          mtry(range = c(ceiling(0.05 * ncol(train_data)), floor(0.4 * ncol(train_data)))),
          size = 10
        )
        tune_results  <- tune_grid(
          biochar_workflow,
          resamples = cv_folds,
          grid      = gbdt_grid,
          metrics   = metric_set(rmse),
          control   = control_grid(
            save_pred     = TRUE,
            parallel_over = "resamples"
          )
        )
        best_params    <- tune_results %>% select_best(metric = "rmse")
        final_workflow <- finalize_workflow(biochar_workflow, best_params)

      } else {
        stop("Unsupported model type: ", model_type)
      }

      # ────────────────────────────────────────────────────────────────────────
      # Fit final model (outside the if/else chain)
      model_fit <- fit(final_workflow, data = train_data)

      train_predictions <- predict(model_fit, new_data = train_data) %>% as_tibble()
      test_predictions  <- predict(model_fit, new_data = test_data)  %>% as_tibble()

      list(
        model             = model_fit,
        train_predictions = train_predictions,
        test_predictions  = test_predictions,
        best_params       = best_params,
        test_truth        = test_data
      )
    }  # ← end map2 anonymous function
  )    # ← end map2

  if (all(map_lgl(results_list, is.null))) {
    message("🚨 All bootstrap iterations failed for property: ", property_name)
    return(NULL)
  }

  results_list
}
```

## PLSR

#### Tune to minimize RMSEP - MIR

selecting the number of components that minimizes the RMSEP

Total split

```{r}

# # # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_99perc_AlFeSiCa")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier,
#                    use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 20, property_range = c(55,58,65:67))

```

### CHN as Features

```{r}
# spec_trt_MIR_CHN <- readRDS("spec_trt_MIR_CHN.RDS")
# 
# # # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_CHN_spec")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_MIR_CHN, model_type = "pls", identifier,
#                    use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 20, property_range = c(1:112,117))
##cant predict CHN using CHN! 
```

## CHN to predict

Change df1 to reference df2

```{r}

#df1 <- readRDS("df2_CHN.RDS")


# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_CHN")  # For RMSEP
# run_all_properties(slprptr,df1, spec_trt, model_type = "pls", identifier,
#                    use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 20, property_range = c(113:116))

```

Source-based split

```{r}

# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_sourcesplit_99perc")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
#                    use_source = TRUE, tuning_goal = "rmsep", num_bootstrap = 20) #, property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

#### Tune to minimize RMSEP - NIR

Total split - NIR

```{r}

# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_NIR_totsplit_99perc")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier, 
#                    use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 20) #, property_range = c(40:42))# :39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))library(callr)

```

Source-based split - NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", num_bootstrap = 20)# , property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

#### Tune to minimize RMSEP - MIR + NIR

Total split - MIR + NIR

```{r}

# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_totsplit_99perc")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "pls", identifier, 
#                    use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 20) #, property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

Source-based split - MIR + NIR

```{r}

# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_NIR_sourcesplit_99perc")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "pls", identifier, 
#                    use_source = TRUE, tuning_goal = "rmsep", num_bootstrap = 20)# , property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

## RANDOM FOREST

#### Tune to minimize RMSEP - MIR

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_99perc")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
#                    use_source = FALSE, tuning_goal = "rmsep",
#                    num_bootstrap = 20)#, property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_99perc_AlFeSiCa")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
#                    use_source = FALSE, tuning_goal = "rmsep",
#                    num_bootstrap = 20, property_range = c(55,58,65:67))
```

### CHN as Features

```{r}
# spec_trt_MIR_CHN <- readRDS("spec_trt_MIR_CHN.RDS")
# 
# # # Run analysis for all properties
# identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_CHN_spec")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_MIR_CHN, model_type = "rf", identifier,
#                    use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 20, property_range = c(1:112,117))

##cant predict CHN using CHN! 
```

## CHN to predict

Change df1 to reference df2

```{r}

#df1 <- readRDS("df2_CHN.RDS")


# # # Run analysis for all properties
# identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_CHN")  # For RMSEP
# run_all_properties(slprptr,df1, spec_trt, model_type = "rf", identifier,
#                    use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 40, property_range = c(113:116))

```

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_sourcesplit_99perc")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
#                    use_source = TRUE, tuning_goal = "rmsep", num_bootstrap = 20)#, property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

#### Tune to minimize RMSEP - NIR

```{r}

# identifier <- initialize_workspace("RF_RMSEP_NIR_totsplit_99perc")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,  
#                    use_source = FALSE, tuning_goal = "rmsep",num_bootstrap = 20)#, property_range = 49:51)
```

Source-based split - NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("RF_RMSEP_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", num_bootstrap = 20)# , property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

#### Tune to minimize RMSEP - MIR + NIR

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_totsplit_99perc")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
#                    use_source = FALSE, tuning_goal = "rmsep",num_bootstrap = 20)
```

#### Tune to minimize RMSEP - MIR + NIR

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_NIR_sourcesplit_99perc")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "rf", identifier,
#                    use_source = TRUE, tuning_goal = "rmsep", num_bootstrap = 20)#, property_range = c(2:3))
```

## PLSR

Cornell only Predictions

```{r}

# Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_CornellOnlysplit_99perc")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
#                    use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", num_bootstrap = 20, include_zv = TRUE)#, property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

## RF

Cornell only Predictions

```{r}
# identifier <- initialize_workspace("RF_RMSEP_MIR_CornellOnlysplit_99perc")  # For RMSEP
# 
# run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
#                    use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", num_bootstrap = 20,include_zv = TRUE) #, property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))

```

Cont form here

#### No Zero Variance - MIR

Zero-variance predictors don’t contribute to PLS components since they hold no information.

• Removing them has a marginal effect because PLSR naturally assigns zero weight to irrelevant predictors.

• Edge case: If your dataset is very small, leaving zero-variance predictors might slightly affect component selection due to noise in variance calculations.

```{r}


# identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_zv")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier,  
#                    use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE, num_bootstrap = 20)# , property_range =c(25:26))#39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

(not really needed for tree based models)

-   Identifies **columns (wavenumbers) where all values are identical** (e.g., every row has 0.5 at a certain wavenumber). These predictors provide **no information** because their standard deviation is **zero**.

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_zv")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,  
#                    use_source = FALSE, tuning_goal = "rmsep", include_zv = TRUE, num_bootstrap = 20) #, property_range =c(25:26))#39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

#### PLSR Lab Covariate - MIR

Total split

\*warnings simply mean “I found a constant column in this fold.” - likely due to lab covariate column duuring splits.

```{r}

# Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_Lab")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "pls", identifier, 
#                    use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 20, include_zv = TRUE) #, property_range = c(25:26)) #:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

#### RF Lab Covariate - MIR

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_lab")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_MIR_lab, model_type = "rf", identifier,  
#                    use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 20, include_zv = TRUE) #, property_range = c(25:26)) #39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

## Filtering out part of spectra

```{r}

# First, convert names to numeric (excluding "SSN")
wavenumbers <- names(spec_trt)
wavenumbers_num <- suppressWarnings(as.numeric(wavenumbers))  # will produce NA for "SSN"

# Identify which column names are <= 4001.3 or "SSN"
keep_cols <- wavenumbers_num <= 4001.3 | is.na(wavenumbers_num)  # keep "SSN"

# Subset the data frame
spec_trt_filtered <- spec_trt[, keep_cols]
```

```{r}
# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_SpecSamp")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_filtered, model_type = "pls", identifier,
#                    use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 20) #, property_range = c(39,41:72,87,102))

```

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_SpecSamp")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_filtered, model_type = "rf", identifier,
#                    use_source = FALSE, tuning_goal = "rmsep",
#                    num_bootstrap = 20)#, property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

```{r}

# identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_SpecSamp_CHN")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_filtered, model_type = "pls", identifier,
#                    use_source = FALSE, tuning_goal = "rmsep", num_bootstrap = 40, property_range = c(113:116))
```

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_SpecSamp_CHN")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_filtered, model_type = "rf", identifier,
#                    use_source = FALSE, tuning_goal = "rmsep",
#                    num_bootstrap = 40, property_range = c(113:116))
```

## PLSR

Cornell only Predictions

```{r}

# # Run analysis for all properties
# identifier <- initialize_workspace("PLSR_RMSEP_MIR_CornellOnlysplit_SpecSamp")  # For RMSEP
# run_all_properties(slprptr, df1, spec_trt_filtered, model_type = "pls", identifier,
#                    use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", num_bootstrap = 20, include_zv = TRUE)#, property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

## RF

Cornell only Predictions

```{r}

# identifier <- initialize_workspace("RF_RMSEP_MIR_CornellOnlysplit_SpecSamp")  # For RMSEP
# # 
# run_all_properties(slprptr, df1, spec_trt_filtered, model_type = "rf", identifier,
#                    use_source = FALSE, source_name = "Cornell ", tuning_goal = "rmsep", num_bootstrap = 20, include_zv = TRUE) #, property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))

```

#\*\*\* cont from here

## Sourcesplit

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", num_bootstrap = 20)# , property_range = c(25:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

```{r}

identifier <- initialize_workspace("RF_RMSEP_NIR_sourcesplit_99perc")  # #For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "rf", identifier,
                   use_source = TRUE, tuning_goal = "rmsep", num_bootstrap = 20) #,property_range = 25:55)
```

# PCA Reduction

## PLSR

#### PCA Components as features

Captured variance

```{r}

identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_PCAfeat_variance")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", include_pca = TRUE, variance_threshold = 0.95, num_bootstrap = 20) #, property_range = c(5:6))# 39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

## RF

#### RF PCA Components as features

Captured variance

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_PCAfeat_Var")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", include_pca = TRUE, variance_threshold = 0.95, num_bootstrap = 20) #, property_range = c(4:5))#5:39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

#### RF PLS Components as features

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totspslit_PLSfeat_RMSEP")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier,
                   use_source = FALSE, tuning_goal = "rmsep", include_pls = TRUE, num_bootstrap = 20)#, property_range = c(5:6))# 39,41:46,48,50,51,53,55,57,65,67,69,71,73,75,77))
```

## BRNN

#### Tune to minimize RMSEP - MIR

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_totsplit_PCAfeat")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  #property_range = 52:53, #length(slprptr),
  num_bootstrap = 20,
  #include_zv = TRUE,
  include_pca = TRUE,
  identifier,
  #num_pca_comp = 20
  variance_threshold = 0.95
)
```

#### PCA Components as Features

```{r}

identifier <- initialize_workspace("BRNN_RMSEP_MIR_totsplit_PCAfeat")
  
run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  #property_range = 52:56, #length(slprptr),
  num_bootstrap = 20,
  include_pca = TRUE,
  identifier,
  variance_threshold = 0.95
)
```

#### PLS Components as Features

```{r}
# 
identifier <- initialize_workspace("BRNN_RMSEP_MIR_totsplit_PLSfeat")

run_all_properties(
  slprptr = slprptr,
  df1 = df1,
  spec_trt = spec_trt,
  model_type = "brnn",
  use_source = FALSE,
  tuning_goal = "rmsep",
  #property_range = 52:53, #length(slprptr),
  num_bootstrap = 20,
  include_pls = TRUE,
  #variance_threshold_pls = 0.85,
  identifier
  #variance_threshold = 0.95
)
```
