---
title: "CEBUM_PLSR_Workflow"
output: html_document
date: "2024-10-14"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load Libraries

```{r, include=FALSE}

# Define a list of required packages
required_packages <- c("randomForest", "caret", "pls", "data.table", "ithir", 
                       "dplyr", "tidyr", "prospectr", "globals", "stringr", 
                       "ggplot2", "here", "tidymodels", "parsnip", "plsmod", 
                       "mixOmics", "yardstick", "purrr", "tibble","ranger")

# Load all required packages quietly without outputting any list of packages
suppressPackageStartupMessages(
  invisible(lapply(required_packages, library, character.only = TRUE))
)
```

## Load reference data

```{r}

# treated spectra
spec_trt <- readRDS("spec_trt_MIR.RDS")
spec_trt <- spec_trt[,-2] #removing the data for the first wavenumber since it's all NAN's

# biochar property data
df.f<-readRDS("df.f.RDS")

# df.f: full dataset;
# spec_trt: treated spectra

#Available properties to predict
slprptr<-names(df.f[-c(2:9)]) #FUSI EDIT: removing metadata columns 

# defining identity object with names of biochar samples
pred<-as.data.frame(spec_trt[,1])
colnames(pred)<-"SSN"
 
```

## PLSR ( & RF) Base model

1.  definine a 'base' model based on minimizing RMSEP

High-Level Structure:

```         
1.  Preprocessing

2.  Setting up models (PLS and Random Forest)

3.  Running models (map() over models to fit them)

4.  Calculating metrics (for each model)

5.  Plotting results (for each model)

6.  Comparing the models (optional step for comparing PLS vs Random Forest)
```

```{r}


# Initialize the workspace by creating directories for storing outputs and 
# setting up an empty tibble to accumulate performance metrics 
initialize_workspace <- function(identifier) {
  dir.create(paste0("Plots_Validation_MIR_", identifier), showWarnings = FALSE)
  dir.create(paste0("Plots_WellFitted_Properties_", identifier), showWarnings = FALSE)
  all_metrics <<- tibble()  # Global tibble to store metrics for each property
  
  return(identifier)  # Pass the identifier for later use
}

# Select and filter data based on the specified property:
# slprptr: list of properties; .x: index of property in slprptr
select_property_data <- function(slprptr, .x, df.f, spec_trt) {
  cols_to_select <- c("SSN", "Source", slprptr[.x])  # Including 'Source' column to ensure training and test set are split by lab
  cols_to_select <- intersect(cols_to_select, colnames(df.f))
  
  # Verify that required columns exist in the dataset
  if (!all(cols_to_select %in% colnames(df.f))) {
    stop(paste("Columns", paste(cols_to_select, collapse = ", "), "not found in df.f"))
  }
  
  # Filter and preprocess data, including removing outliers for the selected property
  df.sel <- df.f %>%
    filter(SSN %in% spec_trt$SSN) %>% # Select only samples present in both predictor and response datasets
    dplyr::select(dplyr::all_of(cols_to_select)) %>%
    left_join(spec_trt, by = "SSN") %>%   # Combine predictor and response variables into one df
    na.omit() %>%
    filter(data.table::between(.[[3]], quantile(.[[3]], 0.05), quantile(.[[3]], 0.95))) %>% # Filter out extremes
    tibble::column_to_rownames(var = "SSN")  # Set SSN as rownames
  
  return(df.sel)
}

# Split data into calibration (training) and validation (test) sets, ensuring both subsets are representative of the distribution of data across different labs
split_data <- function(df.sel) {
  trainIndex <- createDataPartition(df.sel$Source, p = 0.7, list = FALSE)
  df.sel <- df.sel %>% dplyr::select(-Source)  # Remove Source column post-split
  cal_ids <- rownames(df.sel)[trainIndex]
  val_ids <- rownames(df.sel)[-trainIndex]
  cal_df <- df.sel %>% filter(rownames(.) %in% cal_ids) %>% arrange(rownames(.))
  val_df <- df.sel %>% filter(rownames(.) %in% val_ids) %>% arrange(rownames(.))
  return(list(cal_df = cal_df, val_df = val_df))
}

# Convert calibration and validation datasets into train/test formats, adding SSN column
prepare_model_data <- function(cal_df, val_df) {
  train_data <- cal_df %>% tibble::rownames_to_column(var = "SSN")
  test_data <- val_df %>% tibble::rownames_to_column(var = "SSN")
  return(list(train_data = train_data, test_data = test_data))
}

# Create a recipe for data preprocessing, with optional custom steps
preprocess_data <- function(train_data, test_data, slprptr, p, recipe_steps = NULL) {
  formula <- as.formula(paste(slprptr[p], "~ ."))  # Define formula for model
  
  # Define base recipe with ID role (column) for SSN
  recipe_base <- recipe(formula, data = train_data) %>%
    update_role(SSN, new_role = "ID")
  
  # Apply additional preprocessing steps as needed
  if (!is.null(recipe_steps)) {
    for (step in recipe_steps) {
      recipe_base <- recipe_base %>% step
    }
  }
  
  return(list(recipe = recipe_base, train_data = train_data, test_data = test_data))
}

# Set up PLS or Random Forest model specification, including tuning parameters
setup_model <- function(model_type, train_data = NULL, response_var = NULL) {
  if (model_type == "pls") {
    # Configure a PLS model specification
    model_spec <- parsnip::pls() %>%
      set_mode("regression") %>%
      set_engine("mixOmics") %>%
      set_args(num_comp = tune())
    mtry_range <- NULL
  } else if (model_type == "rf") {
    # Configure a Random Forest model with tuning for mtry and min_n
    if (is.null(train_data)) stop("train_data must be provided for Random Forest.")
    if (is.null(response_var)) stop("response_var must be specified for Random Forest.")
    
    # Remove SSN and response columns to get only predictor columns
    predictors_only <- train_data %>%
      dplyr::select(-SSN, -all_of(response_var))
    
    # Finalize mtry_range
    mtry_range <- dials::finalize(dials::mtry(), predictors_only)
    
    if (is.null(mtry_range)) stop("Failed to initialize mtry_range in setup_model(). Check predictor columns in train_data.")
    
    model_spec <- parsnip::rand_forest(mtry = tune(), min_n = tune()) %>%
      set_mode("regression") %>%
      set_engine("ranger")
  }
      
  # Debugging output
  message("Debugging by printing mtry_range:")
  print(mtry_range)
  return(list(model_spec = model_spec, mtry_range = mtry_range))
}
#***** 

# Function to fit both models (PLS and Random Forest) for comparison
fit_both_models <- function(train_data, test_data, recipe) {
  model_list <- list(
    pls = setup_model("pls"),
    rf = setup_model("rf", train_data = train_data)
  )
  model_types <- names(model_list)
  
  # Fit each model type and return results
  results <- purrr::map2(model_list, model_types, ~ fit_model(.x, train_data, test_data, recipe, .y))
  return(results)
}

# Calculate evaluation metrics (R2, RMSE, Bias, etc.) for model predictions
calculate_metrics <- function(data, predictions, data_type, slprptr, p, best_params, model_type) {
  response_var <- slprptr[p]  # Define response variable
  
  # Check if response variable is present and .pred column exists in predictions
  if (!response_var %in% colnames(data) || !".pred" %in% colnames(predictions)) {
    message(paste("Skipping metrics calculation due to missing columns."))
    return(tibble(Property = response_var, Data_Type = data_type, Model = model_type, R2 = NA, RMSE = NA))
  }

  # Safe computation function with error handling for each metric
  safe_compute <- function(expression, metric_name) {
    tryCatch(eval(expression), error = function(e) NA)
  }

  # Calculate performance metrics
  r2_value <- safe_compute(rsq_vec(truth = data[[response_var]], estimate = predictions$.pred))
  rmse_value <- safe_compute(rmse_vec(truth = data[[response_var]], estimate = predictions$.pred))
  bias_value <- safe_compute(mean(data[[response_var]] - predictions$.pred, na.rm = TRUE))
  
  metrics_tibble <- tibble(
    Property = response_var,
    Data_Type = data_type,
    Model = model_type,
    Comps = if (model_type == "pls") best_params$num_comp else NA,
    mtry = if (model_type == "rf") best_params$mtry else NA,
    N = nrow(data),
    R2 = r2_value,
    RMSE = rmse_value,
    bias = bias_value
  )
  return(metrics_tibble)
}

# Function to plot measured vs. predicted values and save in the default directory
plot_predictions <- function(val_plot_data, val_df, slprptr, p, combined_metrics, model_type, identifier) {
  if (!(slprptr[p] %in% colnames(val_plot_data))) return(NULL)
  
  val_metrics <- combined_metrics %>% filter(Data_Type == "Validation", Model == model_type)
  label_text <- paste("Comps:", val_metrics$Comps, "R2:", round(val_metrics$R2, 3))
  
  validation_plot <- ggplot(val_plot_data, aes_string(x = slprptr[p], y = ".pred")) +
    geom_point(color = "blue", size = 2) +
    geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
    labs(title = paste("Measured vs Predicted for", slprptr[p], "-", model_type),
         x = paste("Measured", slprptr[p]),
         y = "Predicted") +
    theme_minimal() +
    annotate("text", x = Inf, y = -Inf, hjust = 1.1, vjust = -0.5, label = label_text, size = 4)
  
  ggsave(paste0("Plots_Validation_PLSR_MIR_", identifier, "/", slprptr[p], "_", model_type, ".png"),
         plot = validation_plot, width = 7, height = 7, dpi = 300)
}

# Plot R² values and differences for well-fitted properties based on model criteria
plot_well_fitted_r2 <- function(all_metrics, identifier) {
  all_r2_values <- all_metrics %>%
    group_by(Property) %>%
    reframe(
      R2_cal = R2[Data_Type == "Calibration"],
      R2_val = R2[Data_Type == "Validation"],
      R2_diff = abs((R2_cal - R2_val) / R2_cal)
    )
  
  well_fitted_properties <- all_r2_values %>%
    filter(R2_diff <= 0.15, R2_cal > 0.6, R2_val > 0.6) %>%
    pull(Property)
  
  plot_data <- all_metrics %>%
    filter(Property %in% well_fitted_properties, Data_Type %in% c("Calibration", "Validation")) %>%
    dplyr::select(Property, Data_Type, R2, N, RMSE, bias) %>%
    rename(Value = R2)
  
  well_fitted_plot <- ggplot(plot_data, aes(x = Property, y = Value, fill = Data_Type)) +
    geom_bar(stat = "identity", position = "dodge") +
    scale_fill_manual(values = c("Calibration" = "purple", "Validation" = "darkgreen")) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
      panel.grid = element_blank(),
      panel.border = element_rect(color = "black", fill = NA)
    ) +
    labs(title = "R² Comparison for Well-Fitted Properties",
         subtitle = "Purple: Calibration | Green: Validation",
         y = "R² Value",
         x = "Property") +
    geom_text(
      aes(label = ifelse(Data_Type == "Calibration", paste0("N=", N),
                         paste0("N=", N, "\nRMSEP=", round(RMSE, 2), "\nBias=", round(bias, 2)))),
      position = position_dodge(width = 0.9), 
      vjust = -0.5, size = 3
    )
  
  print(well_fitted_plot)
  
  ggsave(filename = paste0("Plots_WellFitted_Properties_", identifier, "/well_fitted_properties_plot.png"), 
         plot = well_fitted_plot, width = 10, height = 7, dpi = 300)
}

# Wrapper function to run analysis for each property in slprptr with specified model_type (no default)
run_all_properties <- function(slprptr, df.f, spec_trt, model_type, identifier) {
  all_metrics <<- tibble()  # Initialize global metrics storage
  
  # Loop through each property and collect metrics
  all_metrics <- purrr::map_dfr(102:length(slprptr), function(index) {
    df.sel <- select_property_data(slprptr, index, df.f, spec_trt)
    split <- split_data(df.sel)
    cal_df <- split$cal_df
    val_df <- split$val_df
    
    if (nrow(cal_df) <= 4 || nrow(val_df) <= 4) {
      message(paste("Skipping property:", slprptr[index], "due to insufficient data"))
      return(tibble(Property = slprptr[index], Data_Type = "Skipped", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
    data_prep <- prepare_model_data(cal_df, val_df)
    recipe_data <- preprocess_data(data_prep$train_data, data_prep$test_data, slprptr, index)
    
    if (model_type == "pls") {
      model_spec_list <- setup_model(model_type, train_data = data_prep$train_data)
    } else if (model_type == "rf") {
      model_spec_list <- setup_model(model_type, train_data = data_prep$train_data, response_var = slprptr[index])
    }
    
    # Use both model_spec and mtry_range in fit_model call
    model_fit <- fit_model(model_spec_list, recipe_data$train_data, recipe_data$test_data, recipe_data$recipe, model_type)
    
    if ("Data_Type" %in% colnames(model_fit) && model_fit$Data_Type[1] == "Skipped") {
      return(model_fit)
    }
    
    cal_predictions <- predict(model_fit$model, new_data = recipe_data$train_data)
    val_predictions <- predict(model_fit$model, new_data = recipe_data$test_data)
    
    cal_metrics <- calculate_metrics(recipe_data$train_data, cal_predictions, "Calibration", slprptr, index, best_params = model_fit$best_params, model_type = model_type)
    val_metrics <- calculate_metrics(recipe_data$test_data, val_predictions, "Validation", slprptr, index, best_params = model_fit$best_params, model_type = model_type)
    
    combined_metrics <- bind_rows(cal_metrics, val_metrics)
    
    plot_predictions(recipe_data$test_data %>% 
                       dplyr::select(slprptr[index]) %>%
                       bind_cols(val_predictions), 
                     val_df, slprptr, index, combined_metrics, model_type, identifier)
    
    return(combined_metrics)
  })
  
  all_metrics <<- all_metrics
  plot_well_fitted_r2(all_metrics, identifier)
}
```

### Base model 1

#### Tune to minimize RMSEP

selecting the number of components that minimizes the RMSEP

```{r}


# Function to fit and tune the model (PLS or Random Forest) based on RMSE
fit_model <- function(model_spec_list, train_data, test_data, recipe, model_type) {
  model_spec <- model_spec_list$model_spec
  mtry_range <- model_spec_list$mtry_range  # Only for Random Forest
  
  # Set up workflow with recipe and model
  biochar_workflow <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(model_spec)
  
  # Define cross-validation folds based on sample size
  cv_folds <- if (nrow(train_data) <= 10) {
    vfold_cv(train_data, v = nrow(train_data))  # Leave-One-Out CV if samples are <= 10
  } else {
    vfold_cv(train_data, v = 10)  # 10-fold CV for > 10 samples
  }
  
  # Model-specific tuning grid and selection based on RMSE
  if (model_type == "pls") {
    # Tuning grid for PLS components
    num_comp_grid <- tibble(num_comp = 1:min(nrow(test_data) - 1, 25)) # test between 1 and the number of validation samples -1 or 25
    tune_results <- tune_grid(
      biochar_workflow,
      resamples = cv_folds,
      grid = num_comp_grid,
      metrics = metric_set(rmse),
      control = control_grid(save_pred = TRUE)
    )
    best_params <- tune_results %>% select_best(metric = "rmse") %>% dplyr::select(num_comp)
    final_workflow <- finalize_workflow(biochar_workflow, best_params)
  } else if (model_type == "rf") {
    # Tuning grid for Random Forest model parameters
    if (is.null(mtry_range)) stop("mtry_range is NULL for Random Forest.")  # Check for NULL mtry_range
    rf_grid <- dials::grid_regular(mtry_range, dials::min_n(), levels = 5)
    tune_results <- tune_grid(
      biochar_workflow,
      resamples = cv_folds,
      grid = rf_grid,
      metrics = metric_set(rmse),
      control = control_grid(save_pred = TRUE)
    )
    best_params <- tune_results %>% select_best(metric = "rmse")
    final_workflow <- finalize_workflow(biochar_workflow, best_params)
  }
  
  # Fit the model with optimal parameters and handle any prediction errors
  model_fit <- fit(final_workflow, data = train_data)
  train_predictions <- tryCatch({
    predict(model_fit, new_data = train_data)
  }, error = function(e) {
    message("Error in train_predictions: ", e)
    return(NULL)
  })
  
  test_predictions <- tryCatch({
    predict(model_fit, new_data = test_data)
  }, error = function(e) {
    message("Error in test_predictions: ", e)
    return(NULL)
  })
  
  # Ensure predictions are non-null; otherwise, return error message
  if (is.null(train_predictions) || is.null(test_predictions)) {
    stop("Prediction failed: check data dimensions or NA values in predictor matrix.")
  }
  
  return(list(model = model_fit,
              train_predictions = train_predictions,
              test_predictions = test_predictions,
              best_params = best_params))
}


```

PLSR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP")  # For RMSEP
run_all_properties(slprptr, df.f, spec_trt, model_type = "pls", identifier)
```

RF

```{r}

identifier <- initialize_workspace("RF_RMSEP")  # For RMSEP
run_all_properties(slprptr, df.f, spec_trt, model_type = "rf", identifier)
```

### Base model 2:

#### Tune to capture explained variance

-   Selecting the number of components to capture 90% and 95% of the explained variance in the predictor and response variables respectively.
-   Note: ' prop_expl_var\$X and prop_expl_var\$Y represent the proportion of explained covariance for each component in the predictor (X) and response (Y) matrices.' - same could be achieved using

To Do: Need to verify what prop_expl_var is actually calculating - unclear if it's variance.

```{r}


calculate_explained_variance <- function(train_data, response_var, max_comps) {
  # Prepare X and Y matrices
  X <- train_data %>%
    dplyr::select(-c(SSN, all_of(response_var))) %>%
    dplyr::mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  Y <- train_data %>%
    dplyr::select(all_of(response_var)) %>%
    dplyr::mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  variance_list <- purrr::map_dbl(1:max_comps, function(num_comp) {
    model <- tryCatch(
      mixOmics::pls(X = X, Y = Y, ncomp = num_comp),
      error = function(e) {
        message("Error in fitting PLS model: ", e)
        return(NULL)
      }
    )
    
    if (is.null(model)) {
      return(0)
    }
    
    # Calculate cumulative explained variance for X up to num_comp
    cumulative_variance_X <- sum(model$prop_expl_var$X[1:num_comp])
    
    cumulative_variance_X
  })
  
  # Display final cumulative variance list for debugging
  print("Cumulative variance list:")
  print(variance_list)
  
  return(variance_list)
}


fit_model <- function(model_spec_list, train_data, test_data, recipe, model_type, cumulative_variance_threshold = 0.85) {
  model_spec <- model_spec_list$model_spec
  mtry_range <- model_spec_list$mtry_range  # Only for Random Forest
  
  # Set up workflow with recipe and model
  biochar_workflow <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(model_spec)
  
  # Check if PLS model, then calculate explained variance
  if (model_type == "pls") {
    # Set max_comps to 25 or one less than the number of samples in test_data
    max_comps <- min(25, nrow(test_data) - 1)
    
    # Calculate cumulative explained variance up to max_comps
    response_var <- colnames(train_data)[[2]]  # Adjust index if response var is at a different position
    explained_variance <- calculate_explained_variance(train_data, response_var, max_comps)
    
    # Find the minimum number of components meeting the variance threshold
    num_comp <- which(explained_variance >= cumulative_variance_threshold)[1]
    
    # Check if num_comp is missing or NULL and handle it
    if (is.na(num_comp) || is.null(num_comp)) {
      message(paste("Skipping property:", response_var, "due to insufficient cumulative variance."))
      # Return a tibble indicating the property was skipped
      return(tibble(
        Property = response_var, 
        Data_Type = "Skipped", 
        Model = model_type, 
        N = NA,
        R2 = NA, 
        RMSE = NA, 
        bias = NA,
        ncomp = NA  # Include ncomp to maintain compatibility with downstream processing
      ))
    }
    
    # Set num_comp in your workflow's tuning grid
    final_workflow <- finalize_workflow(biochar_workflow, tibble(num_comp = num_comp))
  } else if (model_type == "rf") {
    # Set up Random Forest tuning grid if applicable
    rf_grid <- grid_regular(mtry_range, min_n(), levels = 5)
    tune_results <- tune_grid(
      biochar_workflow,
      resamples = vfold_cv(train_data, v = 10),  # 10-fold CV
      grid = rf_grid,
      metrics = metric_set(rmse),
      control = control_grid(save_pred = TRUE)
    )
    best_params <- tune_results %>% select_best(metric = "rmse")
    final_workflow <- finalize_workflow(biochar_workflow, best_params)
  }
  
  # Fit and predict using the final workflow
  model_fit <- fit(final_workflow, data = train_data)
  train_predictions <- tryCatch({
    predict(model_fit, new_data = train_data)
  }, error = function(e) {
    message("Error in train_predictions: ", e)
    return(NULL)
  })
  
  test_predictions <- tryCatch({
    predict(model_fit, new_data = test_data)
  }, error = function(e) {
    message("Error in test_predictions: ", e)
    return(NULL)
  })
  
  # Ensure predictions are non-null; otherwise, return error message
  if (is.null(train_predictions) || is.null(test_predictions)) {
    stop("Prediction failed: check data dimensions or NA values in predictor matrix.")
  }
  
  return(list(
    model = model_fit,
    train_predictions = train_predictions,
    test_predictions = test_predictions,
    best_params = tibble(num_comp = num_comp)
  ))
}

# Run analysis for all properties
# Run analysis for all properties
identifier <- initialize_workspace("PLSR_Variance")  # For RMSEP
run_all_properties(slprptr, df.f, spec_trt, model_type = "pls", identifier)
```
