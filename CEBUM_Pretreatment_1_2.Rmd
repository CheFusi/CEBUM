---
title: "CEBUM"
output: html_document
date: "2023-12-27"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Installing Packages

```{r}

#if (!require("remotes")) install.packages("remotes")
#remotes::install_github("philipp-baumann/simplerspec")

#Info on package: 
  #https://github.com/philipp-baumann/simplerspec/ & 
  #https://github.com/philipp-baumann/simplerspec-read-filter-transform/blob/master/README.Rmd 

#install.packages(c("simplerspec","ggfortify"))
library(devtools)
#install_github("vqv/ggbiplot")
#library(simplerspec)
# Simplerspec is a universal file reader that allows one to read selected parameters instrument, optic and acquisition parameters)



suppressPackageStartupMessages({library(readxl)
                 library(plyr)
                 library(dplyr)
                 library(tidyr)
                 library(ggfortify)
                 library(tibble)
                 library(here)
                 library(ggbiplot)
                 library(tidyverse)
                 library(reshape)
                 library(reshape2)})
```

```{r}
files <- list.files(full.names = TRUE)
str(files)
```

The object `files` has the data structure *atomic vector*. *Atomic vectors* have six possible basic (*atomic*) vector types. These are *logical*, *integer*, *real*, *complex*, *string* (or *character*) and *mir*. Vector types can be returned by the R base function `typeof(x)`, which returns the type or internal storage mode an object `x`. For the `files` object it is

```{r}
# Check type of files object
typeof(files)
```

# Section 02

---
title: "02Spectra_Data_Cleaning_Exploration"
output: html_document
date: "2023-07-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### OBJECTIVE of SCRIPT

### Loading Libraries

```{r}
library(prospectr)
library(ggplot2)
library(dplyr)
library(reshape)
library(here)

#setwd("/Users/soils/Library/CloudStorage/OneDrive-CIFOR-ICRAF/Documents/0All_Training/Spectroscopy Data Training-17th May 2023/Data")
```

## Extracting spectra, removing meta data, and briefly visualizing

NOTES

-   Melt functionally takes a dataframe from wide to long format

```{r}

flnm<-"Raw_spectra-MIR.csv"

# Read spectra
raw <- read.csv(flnm)
raw[1:2,1:20]
names(raw)[1:20]
dim(raw)

############ Remove spectra metadata ######
 
raw0 <- raw[-c(1:36),-c(2:17)]
## since the MIR samples included some blanks up to row 37
raw0[1:4,1:4]

wavenumbers <- round(as.numeric(substr(colnames(raw0[,-1]),2,100)),1)
##FUSI: this is another way to remove the X that appears in the column names since the values are numeric

colnames(raw0) <- c("SSN", wavenumbers)

raw0[1:5,c(1:4,1700:1704)]

# Create temporary SSNs incase there are repeat scans. The flat csv file retains the original assigned SSNs whether with repeats or not.

length(unique(raw0$SSN))

relabel <- length(grep('TRUE',duplicated(raw0[,1])))

ifelse(relabel>0, raw0[,1] <- paste(raw0$SSN, 1:nrow(raw0),sep = "-"),raw0[,1] <- raw0[,1])

#adds - 1, 2, 3 ... to id so each sample is unique

spec.m <- melt(as.data.frame(raw0), id="SSN")


p <- ggplot(data = spec.m, aes(x = as.numeric(as.vector(variable)),y = value,group = SSN)) +
  
  geom_line(size = 0.5, alpha = 0.1, col = 'brown') +
  
  ggtitle(strsplit(flnm,".csv")[[1]][1]) +
  
  xlim(rev(range(as.numeric(as.vector(spec.m$variable))))) +
  
  #note that the variable column refers to the wavenumber and the value column to the absorbance
  ylim(range(spec.m$value)) + 
  
  # ylim(c(0,1.3)) +
  
  xlab(expression("Wavenumbers (cm)"^-1)) +
  
  ylab("Aborbance units") + 
  #theme with white background
  theme_bw() +
  #eliminates background, gridlines, and chart border
  theme(
    plot.background = element_blank()
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
  )
p <- p + theme(plot.title = element_text(hjust = 0.5))

p <- p + theme(legend.position = "none")

p <- p + theme(panel.background = element_rect(fill = "white"))

p

ggsave("Raw_spectra_plot_CEBUM.png")
################################################################################
```

### Cleaning Spectra

NOTES

-   The 'up' and 'down' cleanups are based on:
    -   So thinking about outliers: this is initially scanning the region between 2450-2500 - mainly this is probably a visual scan of the extracted dataset
    -   then maybe one identifies a peak with wavenumber 2498.2 and decides to remove this because it has an absorbance above 1.8
    -   the same is done for the peak at 3696.4 with absorbance below 1.1
    -   then the spectra are recombined

```{r}
################## Clean Spectra ##############

vrbs<-as.numeric(as.vector(spec.m$variable))

##### Up
spec.m[which(vrbs > 2450 & vrbs < 2500),]
bdup<-which(spec.m$variable == 2498.2 & spec.m$value > 1.8)

##### Down
spec.m[which(vrbs > 3600 & vrbs < 3700),]
bddwn<-which(spec.m$variable == 3696.4 & spec.m$value < 1.1)


#extracting only the rows/samples in the spec.m dataframe that meet the above criteria defined for the bdup and bddwn dataframes
spec.m2<-spec.m[which(spec.m$SSN %in% spec.m[c(bdup,bddwn),1]),]



bd <- ggplot(data = spec.m2, aes(x = as.numeric(as.vector(variable)),y = value,group = SSN)) +
  
  geom_line(size = 0.5, alpha = 0.1, col = 'brown') + 
  
  xlim(rev(range(as.numeric(as.vector(spec.m2$variable))))) +
  
  ylim(range(spec.m2$value)) + xlab(expression("Wavenumbers (cm)"^-1)) +
  
  ylab("Aborbance units") + theme_bw()

bd

#Now redefining the spec.m dataframe to omit the samples that meet the bdup and bddwn criteria as these are defined here as outliers

spec.m <- spec.m[which(!spec.m$SSN %in% spec.m[c(bdup,bddwn),1]),]

  
  
############## Average spectra if in duplicate

##FUSI edit 

# Remove the suffix from SSN
spec.m$SSN <- gsub("-\\d+$", "", spec.m$SSN)

# Average the replicates
avg_spec <- spec.m %>%
  group_by(SSN, variable) %>%
  summarise(value = mean(value, na.rm = TRUE))

# Cast the melted data back to wide format
raw0 <- dcast(avg_spec, SSN ~ variable, value.var = "value")

#previous version
#raw0 <- cast(spec.m, SSN~variable)
dim(raw0)

rownames(raw0) <- raw0$SSN

names(raw0) <- c("SSN", paste0("X", names(raw0)[-1]))

write.csv(raw0, paste0(getwd(),"/","model_",flnm),row.names = F)
```

```         
```

### Spectral Preprocessing

-   The first chunk is only necessary if you have a model spectra already, and you want to combine it with the new data so you can preprocess it together, you (likely) won't have this in the beginning i.e. if you are working on developing the model so you can skip this chunk

QUESTIONS

-   The following code seems to be doing the same thing as the 04Overlaying_spectra script - are there main differences?

TO DO

-   Read about the mean-centering function and why it's done here

Notes from Karari -

-   may have to look into database to see how to beef up dataset

-   if biochar does not share similar characteristics to soils data used to build ICRAF models ... ):

```{r}
#The following chunk is only necessary if you have a model spectra already, and you want to combine it with the 
# new data so you can preprocess it together, you (likely) won't have this in the beginning i.e. if you are working on developing the model 
# so you can skip this chunk 

####################### Read model data and preprocess #########################
# master<- read.csv("model-raw_spectra.csv")[,-c(2:17)]
# dim(master)
# master[1:5,1:5]
# 
# wavenumbers <- round(as.numeric(substr(colnames(master[,-1]),2,100)),1)
# 
# colnames(master) <- c("SSN", wavenumbers)
# 
# #Bind model and new data
# nwraw0 <- bind_rows(master,nwraw0)
# length(unique(nwraw0$SSN))
################################################################################



nwraw0 <- raw0

wavenumbers <- round(as.numeric(substr(colnames(nwraw0[,-1]),2,100)),1)
##FUSI: this is another way to remove the X that appears in the column names since the values are numeric

colnames(nwraw0) <- c("SSN", wavenumbers)

#FUSI EDIT: also removed bands below 650 as per Lago

##################################### Spectra pre-treatment ###################
#remove the CO2 region from all data

#FUSI EDIT: commenting out this section because for this dataset, co2 is empty 

# wavenumbers_1 <- colnames(nwraw0[,-1])
# co2 <- which(wavenumbers_1 < 2380 & wavenumbers_1 > 2350) # Get co2 bands
# nwraw0 <- nwraw0[,-c(co2+1)]
# dim(nwraw0)
# nwraw0[1:5,1:5]
```

### Savitzky Golay and Mean Centering

```{r}

#SavitzkyGolay pretreatment
#FUSI NOTES: Takes the first derivative and smooths the spectra

#often used as a preliminary preprocessing step to resolve overlapping signals, enhance signal properties, and suppress unwanted spectral features that arise due to nonideal instrument and sample properties."



spec_der<-as.data.frame(savitzkyGolay(nwraw0[,-1], w=17, p=2, m=1))

#Mean-centering function
center_colmeans <- function(x) {
  xcenter = colMeans(x)
  x - rep(xcenter, rep.int(nrow(x), ncol(x)))
}

#Mean-centering
spec_der.mc_trt<-center_colmeans(spec_der)

spec_trt<-bind_cols(SSN=nwraw0$SSN,spec_der.mc_trt)
 
```

### Plot of pretreted spectral

```{r}

spec_pretreat <- melt(as.data.frame(spec_trt), id="SSN")

plot_pretreat <- ggplot(data = spec_pretreat, aes(x = variable,y = value,group = SSN)) +
  #as.numeric(as.vector(
  geom_line(size = 0.5, alpha = 0.1, col = 'brown') +
  
  ggtitle(strsplit(flnm,".csv")[[1]][1]) +
  
 # xlim(rev(range(as.numeric(as.vector(spec_trt$variable))))) +
  
  #note that the variable column refers to the wavenumber and the value column to the absorbance
 # ylim(range(spec_trt$value)) + 
  
  # ylim(c(0,1.3)) +
  
  xlab(expression("Wavenumbers (cm)"^-1)) +
  
  ylab("Mean-Centered Derivative of Aborbance") + 
  #theme with white background
  theme_bw() +
  #eliminates background, gridlines, and chart border
  theme(
    plot.background = element_blank()
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
  )
plot_pretreat
```

### PCA Plots

TO DO

~~Re-read fundamentals of PCA~~

QUESTIONS:

-   This is part of the overlay process to see if the model data casts in the same spectral space as the second dataset, correct?
-   prcomp function
    -   default centers the data - but we've already done mean-centering - so does this do nothing?

```{r}

#Plot PCA scores plot overlay
dsnv <- spec_trt

pcs <- prcomp(dsnv[,-c(1:2)]) 
#FUSI EDIT: also removed the first data row since it's all NA's 

pcss <- pcs$x[,1:10]

pcss[1:6,]

plot(pcss)

#FUSI EDIT
#visualizing the loadins 
pcs$rotation
pcs

#FUSI EDIT: The original code was calib <- dim(master)[1]; but this refers to the master which as defined above is based on data used to develop the model
# I changed it to refer to nwraw0 here because that is the data being used to develop the model
#noting also that 'calib' has no significance to calibration here - just kept the same variable name for ease
calib <- dim(nwraw0)[1]


#calib <- dim(master)[1]

points(pcss[1:calib,1:2], col = "red")

points(pcss[-c(1:calib),1:2], col = "blue")

var <- round(summary(pcs)$importance[2,] * 100, 1)

scores <- cbind("Calib",as.data.frame(pcs$x[,1:5])) # get first 5 principal components

names(scores) <- c("set", colnames(scores[,-1]))

scores <- as.matrix(scores)

scores[-c(1:calib),1] <- "New samples"

scores <- as.data.frame(scores)

write.csv(scores, file = "./Calib and Pred scores.csv", row.names = FALSE)

scores <- read.csv("./Calib and Pred scores.csv")

#sp <- sp +  labs(color = "set")
sp <- ggplot(scores, aes(x = PC1, y =PC2, colour = set)) +
  
  geom_point(size = 0.8, alpha = 0.85 ) +
  
  ggtitle("PCA scores plot") +
  
  
  xlab(paste0("PC1 explains ", var[1], "% total variance")) +
  
  ylab(paste0("PC2 explains ", var[2], "% total variance")) +
  
  theme_bw() +
  
  theme(
    plot.background = element_blank()
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
  )
sp <- sp + theme(plot.title = element_text(hjust = 0.5))

sp <- sp + scale_color_manual(values =c("brown","orange"))

sp

ggsave(filename  = "./Biochar_scores.png", height = 6, width = 6,sp)
  
```

## Fusi Edits

### PC 3  & 4

```{r}

sp <- ggplot(scores, aes(x = PC3, y =PC4, colour = set)) +
  
  geom_point(size = 0.8, alpha = 0.85 ) +
  
  ggtitle("PCA scores plot") +
  
  
  xlab(paste0("PC2 explains ", var[3], "% total variance")) +
  
  ylab(paste0("PC3 explains ", var[4], "% total variance")) +
  
  theme_bw() +
  
  theme(
    plot.background = element_blank()
    ,panel.grid.major = element_blank()
    ,panel.grid.minor = element_blank()
  )
sp <- sp + theme(plot.title = element_text(hjust = 0.5))

sp <- sp + scale_color_manual(values =c("brown","orange"))

sp

```

Biplot: the arrows represent the original variables, and the direction and length of the arrows indicate the variable's contribution to the principal components

-   the position of a point represents the projection of the observation into the reduced-dimensional space defined by the principal components

```{r}

# Calculate the absolute values of loadings
absolute_loadings <- abs(pcs$rotation)

# Create a variable importance plot for the first few principal components
barplot(absolute_loadings[, 1:5], beside = TRUE, col = rainbow(5), 
        main = "Variable Importance for PC1-PC5", 
        names.arg = colnames(absolute_loadings[, 1:5]), 
        cex.names = 0.7, las = 2)

# Identify the top 10 loaded variables for each principal component
top_loaded_variables <- apply(absolute_loadings, 2, function(x) head(order(-x), 10))

# biplot that combines the loading plot and the score plot


suppressWarnings({
  biplot(pcs, scale = 0, cex = 0.7)
})



```

# Section 03

-   note:

    -   section 03: Kennard Stone is not used for the Cornell MIR files, because we don't need to split and determine which samples need to be sent for wet chem analysis since we already have the wet chem data (for whichever samples)

# Section 04

-   note:

    -   section 04: Overlaying, also not used here since we don't have any sort of reference model file

# Section 05

## PLS Model

---
title: "05CalibrationModels_PLS_Loop_ARC"
output: html_document
date: "2023-07-30"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\_ two ways of doing predictions (1) with an existing model that was built some time ago and (2) in this case when you have the spectral and wet chem data and you have to develop the model

Reference data = wet chemistry data of the full 100 samples (being used to develop the model)

QUESTION:

-   What does this portion of the code do?: spectraldata.fin\<-spectraldata[is.element(spectraldata\$SSN, df.f\$SSN),]

-   assuming the file spectral data is reading is referring to the selected samples from section 03?

-   or which method of splitting calibration and training data is used? KS or the random setseed123 used later in this script?

```{r}
library(dplyr)
library(prospectr)
library (globals)
library(stringr)
library(ggplot2)
```

```{r}


#FUSI EDIT
#The file being uploaded here is lilely just the cleaned spectra on the 100 biochars (since they have the reference data that is being refernced later)

# spectraldata=read.csv("/Users/vkarari/Library/CloudStorage/OneDrive-CGIAR/Documents/0All_Training/SA_ARC_Physical_Training_Aug-Sept22/ARC-MIR/ARC-MIR.csv")
# rownames(spectraldata)<-spectraldata$SSN



#FUSI EDIT: 
# since I'm combining the script into one, I don't need to reimport the data, just reference it

#Remove metadata columns when using raw file
# names(spectraldata)
# spectraldata<-spectraldata[,-2:-17]
# 
# library("stringr")
# #Ensure all the wavenumbers are rounded to one decimal place
# names(spectraldata)=round(as.numeric(str_sub(names(spectraldata),2)),1)
# names(spectraldata)[1] <- "SSN"
# 


#Read reference data
df1=read.csv("IR_Dependent_Variables_All.csv")#, skip=5)#[,c(1,8:29)]
colnames(df1)[1]<-"SSN"
colnames(df1)[3]<-"char_type"

#FUSI EDIT: put this cleanup chunk earlier to then filter out the columns where there isn't enough data, and then be able to define  slprptr based on the cleaned data 
# 

# #Remove unwanted characters in the dataa
# df.f[sapply(df.f, grepl, pattern = "<")] <- "NA"
dnmrc <- df1[,-c(1:3)] %>% mutate_if(is.character, as.numeric)
df1 <- bind_cols(df1[,c(1:3)],dnmrc)

#FUSI EDIT - errors out becauseo of duplicated? REVISIT
#Revisited - added back after selecting only the samples with ref data 
#KARARI

#rownames(df1)<-df1$SSN


# 
# 
# ##################################### Spectra pre-treatment ###################
# #remove the CO2 region from all data
# co2rem<-which((as.numeric(colnames(spectraldata[-1]))) < 2380 & (as.numeric(colnames(spectraldata[-1]))) > 2350)
# spectraldata.f<-spectraldata[,-co2rem]
# 
# #Mean-centering function
# center_colmeans <- function(x) {
#   xcenter = colMeans(x)
#   x - rep(xcenter, rep.int(nrow(x), ncol(x)))
# }
# 
# #SavitzkyGolay pretreatment
# spec_der<-as.data.frame(savitzkyGolay(spectraldata.f[,-1], w=17, p=2, m=1))
# 
# #Mean-centering
# spec_der.mc_trt<-center_colmeans(spec_der)
# 
# spec_trt<-bind_cols(SSN=spectraldata$SSN,spec_der.mc_trt)

###############################################################################

#FUSI EDIT: ... define what raw0 is
#Also: this variable isn't used really
spectraldata<- raw0


 #select only the common soil samples in spectral and wet chemie data
df.f<-df1[is.element(df1$SSN, spectraldata$SSN),]

#FUSI EDIT
#KARARI
rownames(df.f)<-df.f$SSN

#FUSI EDIT: put this cleanup chunk earlier to then filter out the columns where there isn't enough data, and then be able to define  slprptr based on the cleaned data 
# 
# # #Remove unwanted characters in the dataa
# # df.f[sapply(df.f, grepl, pattern = "<")] <- "NA"
# dnmrc <- df.f[,-1] %>% mutate_if(is.character, as.numeric)
# df.f <- bind_cols(SSN = df.f[,1],dnmrc)

#FUSI EDIT: 
#filtering and removing the columns (reference variables) where more than 70% of data is missing/NA

threshold_na <- 0.65 #for a 70% cut-off

df.f<-
  df.f %>% select(where(~mean(is.na(.)) < threshold_na))

spectraldata.fin<-spectraldata[is.element(spectraldata$SSN, df.f$SSN),]

#Set splitting proportion for the calibration and validation data
set.seed(123)
pool=df.f[sample(nrow(df.f), round(0.3*nrow(df.f),0)), ]
pool<-pool[order(pool$SSN),]
poolid<-pool$SSN  
```

## Partial Least Squares Regression

Loading libraries

```{r}
##################### Random Forest Modelling #################################
library(randomForest)
library(caret)
library(pls)
library(data.table)
#If package ithir is not available for your version of R
#Use the 3 below lines to install package ithir
# install.packages("devtools") 
# library(devtools)
# install_bitbucket("brendo1001/ithir/pkg") or devtools::install_bitbucket("brendo1001/ithir/pkg")

library(ithir)
#used to extract model statistics like RMSE etc.
#using a function called: goof: see here: https://rdrr.io/rforge/ithir/src/R/goof.R 
```

### PLSR Model

```{r}

#Available properties to predict
#Incase you want to predict only selected properties,
#get property position by running line 66. Remove hash sign between ")" and "["
#symbols on line 67. Edit properties position and run line 67.
#Always ensure position 1 in always included.

#FUSI EDIT 
# changed the call to reference df.f and not df1 because df.f has columns removed for samples that don't have enough data 
# also removed the columns for char and char type

names(df.f)
slprptr<-names(df.f[-c(2:3)])

pred<-as.data.frame(spec_trt[,1])
colnames(pred)<-"SSN"

mdl.stats<-NULL#Model stats container

#FUSI EDIT: started at 3, instead of 2 because the first column in the char type
for(p in 4:length(slprptr)){

  
#Select properties to predict one at a time and remove NAs  
df.sel<-df.f %>% select(SSN, slprptr[p]) %>% na.omit


#Plot and print soil properties boxplots
boxplot(df.sel[,slprptr[p]], las=2, xlab = slprptr[p], ylab = "")
dir.create("Plots_Boxplots")
png(paste0(getwd(),"/Plots_Boxplots/",slprptr[p],".png"))
print(boxplot(df.sel[,slprptr[p]], las=2, xlab = slprptr[p], ylab = ""))
dev.off()

# #Split samples inside loop for variables with many NAs
# #Set splitting proportion for the calibration and validation data
# set.seed(123)
# pool=df.f[sample(nrow(df.sel), round(0.2*nrow(df.sel),0)), ]
# pool<-pool[order(pool$SSN),]
# poolid<-pool$SSN


#Get calibration and validation datasets
val_df<-pool
cal_df1 <-subset(df.sel, !(df.sel$SSN %in% val_df$SSN))

# threshold to exclude the extreme 5% values
#KARARI UPDATE; 95% loses too many samples, changed to 99% 

cal_df <-subset(cal_df1, cal_df1[,2]>quantile(cal_df1[,2], 0.01)&cal_df1[,2] <quantile(cal_df1[,2], 0.99))
val_df1 <-subset(df.sel, (df.sel$SSN %in% val_df$SSN))
val_df <-subset(val_df1, val_df1[,2]>quantile(val_df1[,2], 0.01)&val_df1[,2] <quantile(val_df1[,2], 0.99))
#FUSI EDIT: first chunk is initial code - for some reason wasn't actually orering the dataframe. second chunk is my edit
#renames the non-working version with suffice _or
val_df_or<-val_df[order(rownames(val_df)),]
cal_df_or<-cal_df[order(rownames(cal_df)),]

val_df<-setorder(val_df)
cal_df<-setorder(cal_df)

#Subset pre-treated spectra by available reference data
val_spec<-spec_trt[is.element(spec_trt$SSN, val_df$SSN),]
cal_spec<-spec_trt[is.element(spec_trt$SSN, cal_df$SSN),]
cal_spec<-cal_spec[order(cal_spec$SSN),]
val_spec<-val_spec[order(val_spec$SSN),]


#Get no of calibration and validation datasets
N_cal<-nrow(cal_spec)
N_val<-nrow(val_spec)

#Model data
Xcal.f=cal_spec[,-1]
Xval.f=val_spec[,-1]
dfcal.f=cal_df[,-1]
dfval.f=val_df[,-1]

###### PLSR SOC
maxc <- 25  ## number of max components
#FUSI EDIT: max was 25
pls.md <- plsr(dfcal.f ~ ., data = Xcal.f, maxc, validation = "CV", segments = 10)#10-fold CV

## plot RMSEP vs. number of components
plot(pls.md, "val", main=slprptr[p]) 

dir.create("Components_plots")
png(paste0(getwd(),"/Components_plots/",slprptr[p],".png"))
print(plot(pls.md, "val", main=slprptr[p]))
dev.off()

## no. components to use, the one with the smallest adj RMSEP value
RMSEP.obj<-RMSEP(pls.md)
str(RMSEP.obj)

RMSEP.obj$val[1:2,1,]
nc <- as.numeric(sub("comps", "", names(which.min(RMSEP.obj$val[1,1,2:dim(RMSEP.obj$val[1:2,1,])[2]]))))
nc

#Generate relevant model name
md.nm<-paste0("pls.md.", slprptr[p], ".nc", nc)

#Rename model with the looped soil property
assign(x = md.nm, value = get("pls.md", pos = .GlobalEnv), pos = .GlobalEnv)

## predict to validation dataset
pls.prd <- predict(pls.md, ncomp = nc, newdata = Xval.f)

## Return prediction statistics
val.stats=round(goof(dfval.f, pls.prd, type = "spec"),3)
val.stats<-bind_cols(Property=paste0(Property=slprptr[p],"_val"), Comps="", N=N_val, val.stats)
val.stats

## calibration statistics
pls.pc <- predict(pls.md, ncomp = nc, newdata = Xcal.f)

pls.cal=round(goof(dfcal.f, pls.pc, type = "spec"),3)
cal.stats<-bind_cols(Property=paste0(Property=slprptr[p],"_cal"), Comps=as.character(nc), N=N_cal, pls.cal)
cal.stats

################### Get model statistics #########################
mdstats<-bind_rows(cal.stats, val.stats)

#Create model stats labels for the plot
#FUSI EDIT: Removed comps from before N because plot was crowded should add back in and only remove in plot below 
slct.stats<-as.data.frame(t(mdstats[,c("Property","N","R2","RMSE","bias","RPIQ" )])) 
names(slct.stats)<-NULL
slct.stats<-bind_cols(rownames(slct.stats),slct.stats[,2])

#FUSI EDIT: added "comps" because it is in slct.stats 
valbls<-paste0(c("N","R2","RMSE","bias","RPIQ"), "\n")
valsts<-paste0(c(slct.stats[2,2],slct.stats[3,2],slct.stats[4,2],slct.stats[5,2],slct.stats[6,2]))
valstats<-paste(valbls,valsts)

#Bind all looped properties model stats
mdl.stats<-bind_rows(mdl.stats,mdstats)


lgth<-length(sort(dfval.f,decreasing=F))

seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)

#Plot validation plot
plot(dfval.f,pls.prd,pch=10,
     xlab=paste('Measured',names(val_df)[2],sep="_"),
     ylab=paste('Predicted',names(val_df)[2],sep="_"), 
     xlim = range(c(dfval.f,pls.prd)),
     ylim = range(c(dfval.f,pls.prd)),
     mtext(valstats[-1],side=3, at=c(seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)))
     )   ## plot the predicted vs. measured in the validation
abline(a = 0, b = 1)


dir.create("Plots_Validationplots")
png(paste0(getwd(),"/Plots_Validationplots/",slprptr[p],".png"))
print(plot(dfval.f,pls.prd,pch=10,
           xlab=paste('Measured',names(val_df)[2],sep="_"),
           ylab=paste('Predicted',names(val_df)[2],sep="_"), 
           xlim = range(c(dfval.f,pls.prd)),
           ylim = range(c(dfval.f,pls.prd)),
           mtext(valstats[-1],side=3, at=c(seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)))
           ))
abline(a = 0, b = 1)
dev.off()


################### Predict all samples #########################
prd.smpls <- predict(pls.md, spec_trt[,-1])

prd<-as.data.frame(prd.smpls)
df.prd<-bind_cols(SSN=rownames(prd),prd[,nc])
colnames(df.prd)<-c("SSN",slprptr[p])

pred<-merge(pred, df.prd, by="SSN", all.x = T)
}
#FUSI EDIT: End of for loop


#Remove the least reliably predicted texture data (Clay,Sand or Silt)
#Recalculate the removed texture data to make Clay+Sand+Silt=100% content

#Write model statistics and predicted values to the local drive
write.csv(mdl.stats, paste0(getwd(),"/Model_Statistics_PLSR.csv"),row.names = F)
write.csv(pred, paste0(getwd(),"/Predicted_Soil_Properties.csv"),row.names = F)
getwd()

```

```{r}

# ############# Predicting New Spectra Using Existing Models #####################
# 
# new_spectra<-read.csv("/Users/vkarari/Library/CloudStorage/OneDrive-CGIAR/Documents/0All_Training/SA_ARC_Physical_Training_Aug-Sept22/ARC-MIR/Argd_Raw_spectra.csv")
# 
# wavenumbers <- as.numeric(str_sub(names(new_spectra),2))
# names(new_spectra) <- wavenumbers
# names(new_spectra)[1] <- "SSN"
# rownames(new_spectra)<-new_spectra$SSN
# 
# 
# #Pretreat new data as the calibration data was pretreated
# co2rem.n<-which((as.numeric(colnames(new_spectra[-1]))) < 2380 & (as.numeric(colnames(new_spectra[-1]))) > 2350)
# newspectra.fin<-new_spectra [,-co2rem.n]
# 
# spec_vnew.p<-as.data.frame(savitzkyGolay(newspectra.fin[,-1], w=17, p=2, m=1))
# spec_vnew.pm<-center_colmeans(spec_vnew.p)
# 
# 
# ##### Predict new dataset using PLS model: Use the model name suffix as ncomp #####
# Clay_pNEW_pls <- as.data.frame(predict(pls.md.Clay.nc11, ncomp = 11, newdata = spec_vnew.pm))
# 
# SOC_pNEW_pls <- as.data.frame(predict(pls.md.Carbon.nc14, ncomp = 14, newdata = spec_vnew.pm))
# 
# CEC_pNEW_pls <- as.data.frame(predict(pls.md.CEC.nc23, ncomp = 23, newdata = spec_vnew.pm))
# 
# PH_H2O_pNEW_pls <- as.data.frame(predict(pls.md.pH_H2O.nc14, ncomp = 14, newdata = spec_vnew.pm))
# 
# ##################### Predicted dataset using PLSR model
# predNEW<-cbind(new_spectra$SSN, Clay_pNEW_pls, SOC_pNEW_pls, CEC_pNEW_pls, PH_H2O_pNEW_pls)
# 
# colnames(predNEW) <- c("SSN", "Clay", "SOC", "CEC", "PH_H2O")
# 
# write.csv(predNEW,"predicted_pls_all.csv", row.names = F)

```

## Random Forest

```{r}

#Available properties to predict
#Incase you want to predict only selected properties,
#get property position by running line 66. Remove hash sign between ")" and "["
#symbols on line 67. Edit properties position and run line 67.
#Always ensure position 1 in always included.

pred<-as.data.frame(spec_trt[,1])
colnames(pred)<-"SSN"

mdl.stats<-NULL#Model stats container

for(p in 4:length(slprptr)){
  
#Select properties to predict one at a time and remove NAs  
df.sel<-df.f %>% select(SSN, slprptr[p]) %>% na.omit

#Plot and print soil properties boxplots
boxplot(df.sel[,slprptr[p]], las=2, xlab = slprptr[p], ylab = "")
dir.create("Plots_Boxplots")
png(paste0(getwd(),"/Plots_Boxplots/",slprptr[p],".png"))
print(boxplot(df.sel[,slprptr[p]], las=2, xlab = slprptr[p], ylab = ""))
dev.off()

# #Split samples inside loop for variables with many NAs
# #Set splitting proportion for the calibration and validation data
# set.seed(123)
# pool=df.f[sample(nrow(df.sel), round(0.2*nrow(df.sel),0)), ]
# pool<-pool[order(pool$SSN),]
# poolid<-pool$SSN


#Get calibration and validation datasets
val_df<-pool
cal_df1 <-subset(df.sel, !(df.sel$SSN %in% val_df$SSN))
# threshold to exclude the extreme 5% values
cal_df <-subset(cal_df1, cal_df1[,2]>quantile(cal_df1[,2], 0.05)&cal_df1[,2] <quantile(cal_df1[,2], 0.95))
val_df1 <-subset(df.sel, (df.sel$SSN %in% val_df$SSN))
val_df <-subset(val_df1, val_df1[,2]>quantile(val_df1[,2], 0.05)&val_df1[,2] <quantile(val_df1[,2], 0.95))


#FUSI EDIT: first chunk is initial code - for some reason wasn't actually orering the dataframe. second chunk is my edit
#renames the non-working version with suffice _or
val_df_or<-val_df[order(rownames(val_df)),]
cal_df_or<-cal_df[order(rownames(cal_df)),]

val_df<-setorder(val_df)
cal_df<-setorder(cal_df)


#Subset pre-treated spectra by available reference data
val_spec<-spec_trt[is.element(spec_trt$SSN, val_df$SSN),]
cal_spec<-spec_trt[is.element(spec_trt$SSN, cal_df$SSN),]
cal_spec<-cal_spec[order(cal_spec$SSN),]
val_spec<-val_spec[order(val_spec$SSN),]


#Get no of calibration and validation datasets
N_cal<-nrow(cal_spec)
N_val<-nrow(val_spec)

#Model data
Xcal.f=cal_spec[,-1]
Xval.f=val_spec[,-1]
dfcal.f=cal_df[,-1]
dfval.f=val_df[,-1]

rf.md <- randomForest(Xcal.f, dfcal.f, ntree=500, mtry=10, importance=TRUE) #500 10

#Generate relevant model name
md.nm<-paste("rf.md",slprptr[p],sep=".")

#Rename model with the looped soil property
assign(x = md.nm, value = get("rf.md", pos = .GlobalEnv), pos = .GlobalEnv)

## predict to validation dataset
rf.prd <- predict(rf.md, Xval.f)


## Return prediction statistics
val.stats=round(goof(dfval.f,rf.prd, type = "spec"),3)
val.stats<-bind_cols(Property=paste0(Property=slprptr[p],"_val"),N=N_val,val.stats)
val.stats
## calibration statistics
rf_pc <- predict(rf.md, Xcal.f)
rf.cal=round(goof(dfcal.f,rf_pc, type = "spec"),3)
cal.stats<-bind_cols(Property=paste0(Property=slprptr[p],"_cal"),N=N_cal,rf.cal)
cal.stats

################### Get model statistics #########################
mdstats<-bind_rows(cal.stats,val.stats)

#Create model stats labels for the plot
slct.stats<-as.data.frame(t(mdstats[,c("Property","N","R2","RMSE","bias","RPIQ" )]))
names(slct.stats)<-NULL
slct.stats<-bind_cols(rownames(slct.stats),slct.stats[,2])
valbls<-paste0(c("N","R2","RMSE","bias","RPIQ"), "\n")
valsts<-paste0(c(slct.stats[2,2],slct.stats[3,2],slct.stats[4,2],slct.stats[5,2],slct.stats[6,2]))
valstats<-paste(valbls,valsts)

#Bind all looped properties model stats
mdl.stats<-bind_rows(mdl.stats,mdstats)


lgth<-length(sort(dfval.f,decreasing=F))

seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)

#Plot validation plot
plot(dfval.f,rf.prd,pch=10,
     xlab=paste('Measured',names(val_df)[2],sep="_"),
     ylab=paste('Predicted',names(val_df)[2],sep="_"), 
     xlim = range(c(dfval.f,rf.prd)),
     ylim = range(c(dfval.f,rf.prd)),
     mtext(valstats[-1],side=3, at=c(seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)))
     )   ## plot the predicted vs. measured in the validation
abline(a = 0, b = 1)


dir.create("Plots_Validationplots")
png(paste0(getwd(),"/Plots_Validationplots/",slprptr[p],".png"))
print(plot(dfval.f,rf.prd,pch=10,
           xlab=paste('Measured',names(val_df)[2],sep="_"),
           ylab=paste('Predicted',names(val_df)[2],sep="_"), 
           xlim = range(c(dfval.f,rf.prd)),
           ylim = range(c(dfval.f,rf.prd)),
           mtext(valstats[-1],side=3, at=c(seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)))
           ))
abline(a = 0, b = 1)
dev.off()


################### Predict all samples #########################
prd.smpls <- predict(rf.md, spec_trt[,-1])

prd<-as.data.frame(prd.smpls)
df.prd<-bind_cols(SSN=rownames(prd),prd)
colnames(df.prd)<-c("SSN",slprptr[p])

pred<-merge(pred,df.prd,by="SSN", all.x = T)
}

#Remove the least reliably predicted texture data (Clay,Sand or Silt)
#Recalculate the removed texture data to make Clay+Sand+Silt=100% content
# if(which(colnames(pred) %in% "Silt")>1){
#   pred<-pred[,-which(colnames(pred) %in% "Silt")]
# }else{}

#Write model statistics and predicted values to the local drive
write.csv(mdl.stats, paste0(getwd(),"/Model_Statistics_RF.csv"),row.names = F)
write.csv(pred, paste0(getwd(),"/Predicted_Soil_Properties.csv"),row.names = F)
getwd()

```

### Grouping Reference Data

Grouping reference data by temperature

```{r}

#Creating a new column 

# df.f<-df.f %>%
#   dplyr::mutate(Temp_factor = case_when(Temp.>=200 & Temp.<300 ~ 2,
#                                   Temp.>=300 & Temp.<400 ~ 3,
#                                   Temp.>=400 & Temp.<500 ~ 4,
#                                   Temp.>=500 & Temp.<600 ~ 5,
#                                   Temp.>=600 & Temp.<1000 ~ 6))

#first attempt with only two factor types

df.f<-df.f %>%
  dplyr::mutate(Temp_factor = case_when(Temp.>=200 & Temp.<400 ~ 1,
                                        Temp.>=400 & Temp.<=550 ~ 2,
                                  Temp.>550 & Temp.<1000 ~ 3))


df.f$Temp_factor<-as.factor(df.f$Temp_factor)
##

df.f<-df.f %>%
  dplyr::mutate(H_C_factor = case_when(Htot_to_Ctot_.molar.>=0 & Htot_to_Ctot_.molar.<=0.5 ~ 1,
                                       Htot_to_Ctot_.molar.>0.5 & Htot_to_Ctot_.molar.<=0.99 ~ 2))


df.f$Temp_factor<-as.factor(df.f$Temp_factor)
df.f$H_C_factor<-as.factor(df.f$H_C_factor)
df.f$Temp.<-as.numeric(df.f$Temp.)
#Need to consider including NA as a level (mutate doesn't do it)

df.f_split<-split(df.f,df.f$Temp_factor)
```

## PLSR per Temperature

```{r}

#Available properties to predict
#Incase you want to predict only selected properties,
#get property position by running line 66. Remove hash sign between ")" and "["
#symbols on line 67. Edit properties position and run line 67.
#Always ensure position 1 in always included.


 #select only the common soil samples in spectral and wet chemie data
#df.f<-df1[is.element(df1$SSN, spectraldata$SSN),]

spectraldata <-spec_trt
df.f<-df.f_split[[1]][is.element(df.f_split[[1]]$SSN, spectraldata$SSN),]



#FUSI EDIT
#KARARI
rownames(df.f)<-df.f$SSN

#FUSI EDIT: put this cleanup chunk earlier to then filter out the columns where there isn't enough data, and then be able to define  slprptr based on the cleaned data 
# 
# # #Remove unwanted characters in the dataa
# # df.f[sapply(df.f, grepl, pattern = "<")] <- "NA"
# dnmrc <- df.f[,-1] %>% mutate_if(is.character, as.numeric)
# df.f <- bind_cols(SSN = df.f[,1],dnmrc)

#FUSI EDIT: 
#filtering and removing the columns (reference variables) where more than 70% of data is missing/NA

threshold_na <- 0.65 #for a 70% cut-off

df.f<-
  df.f %>% select(where(~mean(is.na(.)) < threshold_na))

spectraldata.fin<-spectraldata[is.element(spectraldata$SSN, df.f$SSN),]

#Set splitting proportion for the calibration and validation data
set.seed(1234)
pool=df.f[sample(nrow(df.f), round(0.3*nrow(df.f),0)), ]
pool<-pool[order(pool$SSN),]
poolid<-pool$SSN  

#FUSI EDIT 
# changed the call to reference df.f and not df1 because df.f has columns removed for samples that don't have enough data 
# also removed the columns for char and char type

names(df.f_split[[1]])
slprptr<-names(df.f[-c(2:3)])

entries_to_remove<-c("Temp.","Temp_factor","H_C_factor")
slprptr <- slprptr[!(slprptr %in% entries_to_remove)]



pred<-as.data.frame(spec_trt[,1])
colnames(pred)<-"SSN"

mdl.stats<-NULL#Model stats container

#FUSI EDIT: started at 3, instead of 2 because the first column in the char type
for(p in 4:length(slprptr)){

  
#Select properties to predict one at a time and remove NAs  
df.sel<-df.f %>% select(SSN, slprptr[p]) %>% na.omit


#Plot and print soil properties boxplots
boxplot(df.sel[,slprptr[p]], las=2, xlab = slprptr[p], ylab = "")
dir.create("Plots_Boxplots")
png(paste0(getwd(),"/Plots_Boxplots/",slprptr[p],".png"))
print(boxplot(df.sel[,slprptr[p]], las=2, xlab = slprptr[p], ylab = ""))
dev.off()

# #Split samples inside loop for variables with many NAs
# #Set splitting proportion for the calibration and validation data
# set.seed(123)
# pool=df.f[sample(nrow(df.sel), round(0.2*nrow(df.sel),0)), ]
# pool<-pool[order(pool$SSN),]
# poolid<-pool$SSN


#Get calibration and validation datasets
val_df<-pool
cal_df1 <-subset(df.sel, !(df.sel$SSN %in% val_df$SSN))

# threshold to exclude the extreme 5% values
#KARARI UPDATE; 95% loses too many samples, changed to 99% 

cal_df <-subset(cal_df1, cal_df1[,2]>quantile(cal_df1[,2], 0.01)&cal_df1[,2] <quantile(cal_df1[,2], 0.99))
val_df1 <-subset(df.sel, (df.sel$SSN %in% val_df$SSN))
val_df <-subset(val_df1, val_df1[,2]>quantile(val_df1[,2], 0.01)&val_df1[,2] <quantile(val_df1[,2], 0.99))
#FUSI EDIT: first chunk is initial code - for some reason wasn't actually orering the dataframe. second chunk is my edit
#renames the non-working version with suffice _or
val_df_or<-val_df[order(rownames(val_df)),]
cal_df_or<-cal_df[order(rownames(cal_df)),]

val_df<-setorder(val_df)
cal_df<-setorder(cal_df)

#Subset pre-treated spectra by available reference data
val_spec<-spec_trt[is.element(spec_trt$SSN, val_df$SSN),]
cal_spec<-spec_trt[is.element(spec_trt$SSN, cal_df$SSN),]
cal_spec<-cal_spec[order(cal_spec$SSN),]
val_spec<-val_spec[order(val_spec$SSN),]


#Get no of calibration and validation datasets
N_cal<-nrow(cal_spec)
N_val<-nrow(val_spec)

#Model data
Xcal.f=cal_spec[,-1]
Xval.f=val_spec[,-1]
dfcal.f=cal_df[,-1]
dfval.f=val_df[,-1]

###### PLSR SOC
maxc <- 5  ## number of max components
#FUSI EDIT: max was 25
pls.md <- plsr(dfcal.f ~ ., data = Xcal.f, maxc, validation = "CV", segments = 3)#10-fold CV

## plot RMSEP vs. number of components
plot(pls.md, "val", main=slprptr[p]) 

dir.create("Components_plots")
png(paste0(getwd(),"/Components_plots/",slprptr[p],".png"))
print(plot(pls.md, "val", main=slprptr[p]))
dev.off()

## no. components to use, the one with the smallest adj RMSEP value
RMSEP.obj<-RMSEP(pls.md)
str(RMSEP.obj)

RMSEP.obj$val[1:2,1,]
nc <- as.numeric(sub("comps", "", names(which.min(RMSEP.obj$val[1,1,2:dim(RMSEP.obj$val[1:2,1,])[2]]))))
nc

#Generate relevant model name
md.nm<-paste0("pls.md.", slprptr[p], ".nc", nc)

#Rename model with the looped soil property
assign(x = md.nm, value = get("pls.md", pos = .GlobalEnv), pos = .GlobalEnv)

## predict to validation dataset
pls.prd <- predict(pls.md, ncomp = nc, newdata = Xval.f)

## Return prediction statistics
val.stats=round(goof(dfval.f, pls.prd, type = "spec"),3)
val.stats<-bind_cols(Property=paste0(Property=slprptr[p],"_val"), Comps="", N=N_val, val.stats)
val.stats

## calibration statistics
pls.pc <- predict(pls.md, ncomp = nc, newdata = Xcal.f)
pls.cal=round(goof(dfcal.f, pls.pc, type = "spec"),3)
cal.stats<-bind_cols(Property=paste0(Property=slprptr[p],"_cal"), Comps=as.character(nc), N=N_cal, pls.cal)
cal.stats

################### Get model statistics #########################
mdstats<-bind_rows(cal.stats, val.stats)

#Create model stats labels for the plot
#FUSI EDIT: Removed comps from before N because plot was crowded should add back in and only remove in plot below 
slct.stats<-as.data.frame(t(mdstats[,c("Property","N","R2","RMSE","bias","RPIQ" )]))
names(slct.stats)<-NULL
slct.stats<-bind_cols(rownames(slct.stats),slct.stats[,2])

#FUSI EDIT: added "comps" because it is in slct.stats 
valbls<-paste0(c("N","R2","RMSE","bias","RPIQ"), "\n")
valsts<-paste0(c(slct.stats[2,2],slct.stats[3,2],slct.stats[4,2],slct.stats[5,2],slct.stats[6,2]))
valstats<-paste(valbls,valsts)

#Bind all looped properties model stats
mdl.stats<-bind_rows(mdl.stats,mdstats)


lgth<-length(sort(dfval.f,decreasing=F))

seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)

#Plot validation plot
plot(dfval.f,pls.prd,pch=10,
     xlab=paste('Measured',names(val_df)[2],sep="_"),
     ylab=paste('Predicted',names(val_df)[2],sep="_"), 
     xlim = range(c(dfval.f,pls.prd)),
     ylim = range(c(dfval.f,pls.prd)),
     mtext(valstats[-1],side=3, at=c(seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)))
     )   ## plot the predicted vs. measured in the validation
abline(a = 0, b = 1)


dir.create("Plots_Validationplots")
png(paste0(getwd(),"/Plots_Validationplots/",slprptr[p],".png"))
print(plot(dfval.f,pls.prd,pch=10,
           xlab=paste('Measured',names(val_df)[2],sep="_"),
           ylab=paste('Predicted',names(val_df)[2],sep="_"), 
           xlim = range(c(dfval.f,pls.prd)),
           ylim = range(c(dfval.f,pls.prd)),
           mtext(valstats[-1],side=3, at=c(seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)))
           ))
abline(a = 0, b = 1)
dev.off()


################### Predict all samples #########################
#prd.smpls <- predict(pls.md, spec_trt[,-1])

#FUSI EDIT
prd.smpls <- predict(pls.md, spec_trt[,-1])

prd<-as.data.frame(prd.smpls)
df.prd<-bind_cols(SSN=rownames(prd),prd[,nc])
colnames(df.prd)<-c("SSN",slprptr[p])

pred<-merge(pred, df.prd, by="SSN", all.x = T)
}
#FUSI EDIT: End of for loop


#Remove the least reliably predicted texture data (Clay,Sand or Silt)
#Recalculate the removed texture data to make Clay+Sand+Silt=100% content

#Write model statistics and predicted values to the local drive
write.csv(mdl.stats, paste0(getwd(),"/Model_Statistics_PLSR_Temp_1.csv"),row.names = F)
write.csv(pred, paste0(getwd(),"/Predicted_Soil_Properties_Temp_1.csv"),row.names = F)
getwd()
```

For entire list of temperature ranges

```{r}

#Available properties to predict
#Incase you want to predict only selected properties,
#get property position by running line 66. Remove hash sign between ")" and "["
#symbols on line 67. Edit properties position and run line 67.
#Always ensure position 1 in always included.


 #select only the common soil samples in spectral and wet chemie data
#df.f<-df1[is.element(df1$SSN, spectraldata$SSN),]

spectraldata <-spec_trt

df.f_temp <- lapply(df.f_split, function(df) {
  df[is.element(df$SSN, spectraldata$SSN),]
})

#FUSI EDIT
#KARARI
df.f_temp <- lapply(df.f_temp, function(df){
  rownames(df.f)<-df.f$SSN
  return(df)
})

#FUSI EDIT: put this cleanup chunk earlier to then filter out the columns where there isn't enough data, and then be able to define  slprptr based on the cleaned data 
# 
# # #Remove unwanted characters in the dataa
# # df.f[sapply(df.f, grepl, pattern = "<")] <- "NA"
# dnmrc <- df.f[,-1] %>% mutate_if(is.character, as.numeric)
# df.f <- bind_cols(SSN = df.f[,1],dnmrc)

#FUSI EDIT: 
#filtering and removing the columns (reference variables) where more than 70% of data is missing/NA

threshold_na <- 0.65 #for a 70% cut-off

df.f_temp <- lapply(df.f_split, function(df) {
  df %>% select(where(~mean(is.na(.)) < threshold_na))
})

df.f_temp <- lapply(df.f_temp, function(df) {
  df[is.element(df$SSN, spectraldata$SSN),]
})

#spectraldata.fin<-spectraldata[is.element(spectraldata$SSN, df.f$SSN),]

#Set splitting proportion for the calibration and validation data
set.seed(1234)

 pool<- lapply(df.f_temp, function(df) {
  pool_df=df[sample(nrow(df), round(0.3*nrow(df),0)), ]
  pool_df<-pool_df[order(pool_df$SSN),]
  return(pool_df)
})

poolid<- lapply(pool, function(df) {
  data.frame(SSN =df$SSN)
  })

#FUSI EDIT 
# changed the call to reference df.f and not df1 because df.f has columns removed for samples that don't have enough data 
# also removed the columns for char and char type

names(df.f_split[[1]])

slprptr<-names(df.f[-c(2:3)])

slprptr<- lapply(df.f_temp, function(df) {
  names(df[-c(2:3)])
  })

entries_to_remove<-c("Temp.","Temp_factor","H_C_factor")

slprptr <- lapply(slprptr, function(df) {
  slprptr_df <- df[!(df %in% entries_to_remove)]
  return(slprptr_df)
})

pred<-as.data.frame(spec_trt[,1])
colnames(pred)<-"SSN"

mdl.stats<-NULL#Model stats container

##PAUSED CONVERSION FROM SINGLE DATFRAME TO LIST OPERATIONS HERE

#FUSI EDIT: started at 3, instead of 2 because the first column in the char type
for(p in 4:length(slprptr)){

  
#Select properties to predict one at a time and remove NAs  
df.sel<-df.f %>% select(SSN, slprptr[p]) %>% na.omit


#Plot and print soil properties boxplots
boxplot(df.sel[,slprptr[p]], las=2, xlab = slprptr[p], ylab = "")
dir.create("Plots_Boxplots")
png(paste0(getwd(),"/Plots_Boxplots/",slprptr[p],".png"))
print(boxplot(df.sel[,slprptr[p]], las=2, xlab = slprptr[p], ylab = ""))
dev.off()

# #Split samples inside loop for variables with many NAs
# #Set splitting proportion for the calibration and validation data
# set.seed(123)
# pool=df.f[sample(nrow(df.sel), round(0.2*nrow(df.sel),0)), ]
# pool<-pool[order(pool$SSN),]
# poolid<-pool$SSN


#Get calibration and validation datasets
val_df<-pool
cal_df1 <-subset(df.sel, !(df.sel$SSN %in% val_df$SSN))

# threshold to exclude the extreme 5% values
#KARARI UPDATE; 95% loses too many samples, changed to 99% 

cal_df <-subset(cal_df1, cal_df1[,2]>quantile(cal_df1[,2], 0.01)&cal_df1[,2] <quantile(cal_df1[,2], 0.99))
val_df1 <-subset(df.sel, (df.sel$SSN %in% val_df$SSN))
val_df <-subset(val_df1, val_df1[,2]>quantile(val_df1[,2], 0.01)&val_df1[,2] <quantile(val_df1[,2], 0.99))
#FUSI EDIT: first chunk is initial code - for some reason wasn't actually orering the dataframe. second chunk is my edit
#renames the non-working version with suffice _or
val_df_or<-val_df[order(rownames(val_df)),]
cal_df_or<-cal_df[order(rownames(cal_df)),]

val_df<-setorder(val_df)
cal_df<-setorder(cal_df)

#Subset pre-treated spectra by available reference data
val_spec<-spec_trt[is.element(spec_trt$SSN, val_df$SSN),]
cal_spec<-spec_trt[is.element(spec_trt$SSN, cal_df$SSN),]
cal_spec<-cal_spec[order(cal_spec$SSN),]
val_spec<-val_spec[order(val_spec$SSN),]


#Get no of calibration and validation datasets
N_cal<-nrow(cal_spec)
N_val<-nrow(val_spec)

#Model data
Xcal.f=cal_spec[,-1]
Xval.f=val_spec[,-1]
dfcal.f=cal_df[,-1]
dfval.f=val_df[,-1]

###### PLSR SOC
maxc <- 5  ## number of max components
#FUSI EDIT: max was 25
pls.md <- plsr(dfcal.f ~ ., data = Xcal.f, maxc, validation = "CV", segments = 3)#10-fold CV

## plot RMSEP vs. number of components
plot(pls.md, "val", main=slprptr[p]) 

dir.create("Components_plots")
png(paste0(getwd(),"/Components_plots/",slprptr[p],".png"))
print(plot(pls.md, "val", main=slprptr[p]))
dev.off()

## no. components to use, the one with the smallest adj RMSEP value
RMSEP.obj<-RMSEP(pls.md)
str(RMSEP.obj)

RMSEP.obj$val[1:2,1,]
nc <- as.numeric(sub("comps", "", names(which.min(RMSEP.obj$val[1,1,2:dim(RMSEP.obj$val[1:2,1,])[2]]))))
nc

#Generate relevant model name
md.nm<-paste0("pls.md.", slprptr[p], ".nc", nc)

#Rename model with the looped soil property
assign(x = md.nm, value = get("pls.md", pos = .GlobalEnv), pos = .GlobalEnv)

## predict to validation dataset
pls.prd <- predict(pls.md, ncomp = nc, newdata = Xval.f)

## Return prediction statistics
val.stats=round(goof(dfval.f, pls.prd, type = "spec"),3)
val.stats<-bind_cols(Property=paste0(Property=slprptr[p],"_val"), Comps="", N=N_val, val.stats)
val.stats

## calibration statistics
pls.pc <- predict(pls.md, ncomp = nc, newdata = Xcal.f)
pls.cal=round(goof(dfcal.f, pls.pc, type = "spec"),3)
cal.stats<-bind_cols(Property=paste0(Property=slprptr[p],"_cal"), Comps=as.character(nc), N=N_cal, pls.cal)
cal.stats

################### Get model statistics #########################
mdstats<-bind_rows(cal.stats, val.stats)

#Create model stats labels for the plot
#FUSI EDIT: Removed comps from before N because plot was crowded should add back in and only remove in plot below 
slct.stats<-as.data.frame(t(mdstats[,c("Property","N","R2","RMSE","bias","RPIQ" )]))
names(slct.stats)<-NULL
slct.stats<-bind_cols(rownames(slct.stats),slct.stats[,2])

#FUSI EDIT: added "comps" because it is in slct.stats 
valbls<-paste0(c("N","R2","RMSE","bias","RPIQ"), "\n")
valsts<-paste0(c(slct.stats[2,2],slct.stats[3,2],slct.stats[4,2],slct.stats[5,2],slct.stats[6,2]))
valstats<-paste(valbls,valsts)

#Bind all looped properties model stats
mdl.stats<-bind_rows(mdl.stats,mdstats)


lgth<-length(sort(dfval.f,decreasing=F))

seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)

#Plot validation plot
plot(dfval.f,pls.prd,pch=10,
     xlab=paste('Measured',names(val_df)[2],sep="_"),
     ylab=paste('Predicted',names(val_df)[2],sep="_"), 
     xlim = range(c(dfval.f,pls.prd)),
     ylim = range(c(dfval.f,pls.prd)),
     mtext(valstats[-1],side=3, at=c(seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)))
     )   ## plot the predicted vs. measured in the validation
abline(a = 0, b = 1)


dir.create("Plots_Validationplots")
png(paste0(getwd(),"/Plots_Validationplots/",slprptr[p],".png"))
print(plot(dfval.f,pls.prd,pch=10,
           xlab=paste('Measured',names(val_df)[2],sep="_"),
           ylab=paste('Predicted',names(val_df)[2],sep="_"), 
           xlim = range(c(dfval.f,pls.prd)),
           ylim = range(c(dfval.f,pls.prd)),
           mtext(valstats[-1],side=3, at=c(seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)))
           ))
abline(a = 0, b = 1)
dev.off()


################### Predict all samples #########################
#prd.smpls <- predict(pls.md, spec_trt[,-1])

#FUSI EDIT
prd.smpls <- predict(pls.md, spec_trt[,-1])

prd<-as.data.frame(prd.smpls)
df.prd<-bind_cols(SSN=rownames(prd),prd[,nc])
colnames(df.prd)<-c("SSN",slprptr[p])

pred<-merge(pred, df.prd, by="SSN", all.x = T)
}
#FUSI EDIT: End of for loop


#Remove the least reliably predicted texture data (Clay,Sand or Silt)
#Recalculate the removed texture data to make Clay+Sand+Silt=100% content

#Write model statistics and predicted values to the local drive
write.csv(mdl.stats, paste0(getwd(),"/Model_Statistics_PLSR_Temp_1.csv"),row.names = F)
write.csv(pred, paste0(getwd(),"/Predicted_Soil_Properties_Temp_1.csv"),row.names = F)
getwd()
```

attempt 2

```{r}

#Available properties to predict
#Incase you want to predict only selected properties,
#get property position by running line 66. Remove hash sign between ")" and "["
#symbols on line 67. Edit properties position and run line 67.
#Always ensure position 1 in always included.


 #select only the common soil samples in spectral and wet chemie data
#df.f<-df1[is.element(df1$SSN, spectraldata$SSN),]

spectraldata <-spec_trt

df.f_temp <- lapply(df.f_split, function(df) {
  df[is.element(df$SSN, spectraldata$SSN),]
})

#FUSI EDIT
#KARARI
df.f_temp <- lapply(df.f_temp, function(df){
  rownames(df.f)<-df.f$SSN
  return(df)
})

#FUSI EDIT: put this cleanup chunk earlier to then filter out the columns where there isn't enough data, and then be able to define  slprptr based on the cleaned data 
# 
# # #Remove unwanted characters in the dataa
# # df.f[sapply(df.f, grepl, pattern = "<")] <- "NA"
# dnmrc <- df.f[,-1] %>% mutate_if(is.character, as.numeric)
# df.f <- bind_cols(SSN = df.f[,1],dnmrc)

#FUSI EDIT: 
#filtering and removing the columns (reference variables) where more than 70% of data is missing/NA

threshold_na <- 0.65 #for a 70% cut-off

df.f_temp <- lapply(df.f_split, function(df) {
  df %>% select(where(~mean(is.na(.)) < threshold_na))
})

df.f_temp <- lapply(df.f_temp, function(df) {
  df[is.element(df$SSN, spectraldata$SSN),]
})

#spectraldata.fin<-spectraldata[is.element(spectraldata$SSN, df.f$SSN),]

#Set splitting proportion for the calibration and validation data
set.seed(1234)

 pool<- lapply(df.f_temp, function(df) {
  pool_df=df[sample(nrow(df), round(0.3*nrow(df),0)), ]
  pool_df<-pool_df[order(pool_df$SSN),]
  return(pool_df)
})

poolid<- lapply(pool, function(df) {
  data.frame(SSN =df$SSN)
  })

#FUSI EDIT 
# changed the call to reference df.f and not df1 because df.f has columns removed for samples that don't have enough data 
# also removed the columns for char and char type

names(df.f_split[[1]])

slprptr<-names(df.f[-c(2:3)])

slprptr<- lapply(df.f_temp, function(df) {
  names(df[-c(2:3)])
  })

entries_to_remove<-c("Temp.","Temp_factor","H_C_factor")

slprptr <- lapply(slprptr, function(df) {
  slprptr_df <- df[!(df %in% entries_to_remove)]
  return(slprptr_df)
})

pred<-as.data.frame(spec_trt[,1])
colnames(pred)<-"SSN"

mdl.stats<-NULL#Model stats container


# Create lists for df.f, pool, poolid, and slprptr
df.f_list <- df.f_temp 
pool_list <- pool
poolid_list <- list()
slprptr_list <- slprptr


# Loop through slprptr_list

df.sel_list <- list()

# Loop through slprptr list
for (p in 1:length(slprptr)) {
  property_names <- slprptr[[p]]  # Get the list of property names for this iteration
  
  df.sel <- lapply(property_names, function(prop) {
    df_subset <- df.f %>% select(SSN, !!sym(prop)) %>% na.omit
    return(df_subset)
  })
#sym creates a symbol, !! unquotes the symbol and together this allows the column to be selected
  df.sel_list[[p]] <- df.sel
}



  # Plot and print soil properties boxplots for each property in df.sel_list
  dir.create("Plots_Boxplots")
  for (i in seq_along(df.sel_list)) {
    df.sel <- df.sel_list[[i]]
    
    if (length(property_name) == 1 && property_name %in% colnames(df.sel)) {
      property_data <- df.sel[[property_name]]  # Extract the property data
      
      if (is.numeric(property_data)) {
        png(paste0(getwd(), "/Plots_Boxplots/", property_name, "_", i, ".png"))
        print(boxplot(property_data, las = 2, xlab = property_name, ylab = ""))
        dev.off()
      } else {
        cat("Property", property_name, "is not numeric in dataframe", i, "\n")
      }
    } else {
      cat("Property", property_name, "not found in dataframe", i, "\n")
    }
  }
  
    
# Rest of your code for splitting, modeling, and statistics goes here for each df.sel
    
# #Get calibration and validation datasets
# val_df<-pool
# cal_df1 <-subset(df.sel, !(df.sel$SSN %in% val_df$SSN))
# 
# # threshold to exclude the extreme 5% values
# #KARARI UPDATE; 95% loses too many samples, changed to 99% 
# 
# cal_df <-subset(cal_df1, cal_df1[,2]>quantile(cal_df1[,2], 0.01)&cal_df1[,2] <quantile(cal_df1[,2], 0.99))
# val_df1 <-subset(df.sel, (df.sel$SSN %in% val_df$SSN))
# val_df <-subset(val_df1, val_df1[,2]>quantile(val_df1[,2], 0.01)&val_df1[,2] <quantile(val_df1[,2], 0.99))
# #FUSI EDIT: first chunk is initial code - for some reason wasn't actually orering the dataframe. second chunk is my edit
# #renames the non-working version with suffice _or
# val_df_or<-val_df[order(rownames(val_df)),]
# cal_df_or<-cal_df[order(rownames(cal_df)),]
# 
# val_df<-setorder(val_df)
# cal_df<-setorder(cal_df)
# 
# #Subset pre-treated spectra by available reference data
# val_spec<-spec_trt[is.element(spec_trt$SSN, val_df$SSN),]
# cal_spec<-spec_trt[is.element(spec_trt$SSN, cal_df$SSN),]
# cal_spec<-cal_spec[order(cal_spec$SSN),]
# val_spec<-val_spec[order(val_spec$SSN),]
# 
# 
# #Get no of calibration and validation datasets
# N_cal<-nrow(cal_spec)
# N_val<-nrow(val_spec)
# 
# #Model data
# Xcal.f=cal_spec[,-1]
# Xval.f=val_spec[,-1]
# dfcal.f=cal_df[,-1]
# dfval.f=val_df[,-1]

#------------
# Initialize empty lists to store the results
val_df_list <- list()
cal_df_list <- list()
val_spec_list <- list()
cal_spec_list <- list()
N_cal_list <- list()
N_val_list <- list()
Xcal_f_list <- list()
Xval_f_list <- list()
dfcal_f_list <- list()
dfval_f_list <- list()

# Function to process each dataframe in the pool list
process_dataframe <- function(df, pool) {
  df <- as.data.frame(df)  # Ensure df is treated as a dataframe
  # Subset the dataframes based on the current pool
  val_df <- df[!(df$SSN %in% pool[[1]]$SSN), ]
  cal_df1 <- df[df$SSN %in% pool[[1]]$SSN, ]
  
  # val_df<-pool
  # cal_df1 <-subset(df.sel, !(df.sel$SSN %in% val_df$SSN))
  
  # Apply the threshold to exclude extreme values
  cal_df <- cal_df1[cal_df1[, 2] > quantile(cal_df1[, 2], 0.01) & cal_df1[, 2] < quantile(cal_df1[, 2], 0.99), ]
  val_df1 <- cal_df1[cal_df1[, 2] <= quantile(cal_df1[, 2], 0.01) | cal_df1[, 2] >= quantile(cal_df1[, 2], 0.99), ]
  
  # Rename dataframes
  val_df_or <- val_df1[order(rownames(val_df1)), ]
  cal_df_or <- cal_df[order(rownames(cal_df)), ]
  
  # Sort dataframes
  val_df <- val_df_or[order(val_df_or$SSN), ]
  cal_df <- cal_df_or[order(cal_df_or$SSN), ]
  
  # Subset pre-treated spectra by available reference data
  val_spec <- spec_trt[is.element(spec_trt$SSN, val_df$SSN), ]
  cal_spec <- spec_trt[is.element(spec_trt$SSN, cal_df$SSN), ]
  cal_spec <- cal_spec[order(cal_spec$SSN), ]
  val_spec <- val_spec[order(val_spec$SSN), ]
  
  # Get the number of calibration and validation datasets
  N_cal <- nrow(cal_spec)
  N_val <- nrow(val_spec)
  
  # Model data
  Xcal_f <- cal_spec[, -1]
  Xval_f <- val_spec[, -1]
  dfcal_f <- cal_df[, -1]
  dfval_f <- val_df[, -1]
  
  # Return the results as a list
  return(list(
    val_df = val_df,
    cal_df = cal_df,
    val_spec = val_spec,
    cal_spec = cal_spec,
    N_cal = N_cal,
    N_val = N_val,
    Xcal_f = Xcal_f,
    Xval_f = Xval_f,
    dfcal_f = dfcal_f,
    dfval_f = dfval_f
  ))
}

# Use lapply to process each dataframe in the df.sel_list
results <- lapply(df.sel_list, process_dataframe, pool = pool_list)

# Unlist the results if needed
val_df_list <- lapply(results, function(result) result$val_df)
cal_df_list <- lapply(results, function(result) result$cal_df)
val_spec_list <- lapply(results, function(result) result$val_spec)
cal_spec_list <- lapply(results, function(result) result$cal_spec)
N_cal_list <- lapply(results, function(result) result$N_cal)
N_val_list <- lapply(results, function(result) result$N_val)
Xcal_f_list <- lapply(results, function(result) result$Xcal_f)
Xval_f_list <- lapply(results, function(result) result$Xval_f)
dfcal_f_list <- lapply(results, function(result) result$dfcal_f)
dfval_f_list <- lapply(results, function(result) result$dfval_f)
#-----------



###### PLSR SOC
maxc <- 5  ## number of max components
#FUSI EDIT: max was 25

for (property_idx in seq_along(slprptr_list)) {
  
  property_name <- slprptr_list[[property_idx]]
  pls.md <- plsr(dfcal_f_list[[property_idx]] ~ ., data = Xcal_f_list[[property_idx]], maxc, validation = "CV", segments = 3)#10-fold CV

  
## plot RMSEP vs. number of components
plot(pls.md, "val", main=slprptr[p]) 

dir.create("Components_plots")
png(paste0(getwd(),"/Components_plots/",slprptr[p],".png"))
print(plot(pls.md, "val", main=slprptr[p]))
dev.off()

## no. components to use, the one with the smallest adj RMSEP value
RMSEP.obj<-RMSEP(pls.md)
str(RMSEP.obj)

RMSEP.obj$val[1:2,1,]
nc <- as.numeric(sub("comps", "", names(which.min(RMSEP.obj$val[1,1,2:dim(RMSEP.obj$val[1:2,1,])[2]]))))
nc

#Generate relevant model name
md.nm<-paste0("pls.md.", slprptr[p], ".nc", nc)

#Rename model with the looped soil property
assign(x = md.nm, value = get("pls.md", pos = .GlobalEnv), pos = .GlobalEnv)

## predict to validation dataset
pls.prd <- predict(pls.md, ncomp = nc, newdata = Xval.f)

## Return prediction statistics
val.stats=round(goof(dfval.f, pls.prd, type = "spec"),3)
val.stats<-bind_cols(Property=paste0(Property=slprptr[p],"_val"), Comps="", N=N_val, val.stats)
val.stats

## calibration statistics
pls.pc <- predict(pls.md, ncomp = nc, newdata = Xcal.f)
pls.cal=round(goof(dfcal.f, pls.pc, type = "spec"),3)
cal.stats<-bind_cols(Property=paste0(Property=slprptr[p],"_cal"), Comps=as.character(nc), N=N_cal, pls.cal)
cal.stats

################### Get model statistics #########################
mdstats<-bind_rows(cal.stats, val.stats)
model_results[[property_idx]] <- list(val_stats = val_stats, cal_stats = cal_stats)
}
#Create model stats labels for the plot
#FUSI EDIT: Removed comps from before N because plot was crowded should add back in and only remove in plot below 
slct.stats<-as.data.frame(t(mdstats[,c("Property","N","R2","RMSE","bias","RPIQ" )]))
names(slct.stats)<-NULL
slct.stats<-bind_cols(rownames(slct.stats),slct.stats[,2])

#FUSI EDIT: added "comps" because it is in slct.stats 
valbls<-paste0(c("N","R2","RMSE","bias","RPIQ"), "\n")
valsts<-paste0(c(slct.stats[2,2],slct.stats[3,2],slct.stats[4,2],slct.stats[5,2],slct.stats[6,2]))
valstats<-paste(valbls,valsts)

#Bind all looped properties model stats
mdl.stats<-bind_rows(mdl.stats,mdstats)


lgth<-length(sort(dfval.f,decreasing=F))

seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)

#Plot validation plot
plot(dfval.f,pls.prd,pch=10,
     xlab=paste('Measured',names(val_df)[2],sep="_"),
     ylab=paste('Predicted',names(val_df)[2],sep="_"), 
     xlim = range(c(dfval.f,pls.prd)),
     ylim = range(c(dfval.f,pls.prd)),
     mtext(valstats[-1],side=3, at=c(seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)))
     )   ## plot the predicted vs. measured in the validation
abline(a = 0, b = 1)


dir.create("Plots_Validationplots")
png(paste0(getwd(),"/Plots_Validationplots/",slprptr[p],".png"))
print(plot(dfval.f,pls.prd,pch=10,
           xlab=paste('Measured',names(val_df)[2],sep="_"),
           ylab=paste('Predicted',names(val_df)[2],sep="_"), 
           xlim = range(c(dfval.f,pls.prd)),
           ylim = range(c(dfval.f,pls.prd)),
           mtext(valstats[-1],side=3, at=c(seq.int(sort(dfval.f,decreasing=F)[1], sort(dfval.f,decreasing=F)[lgth],length.out=4)))
           ))
abline(a = 0, b = 1)
dev.off()


################### Predict all samples #########################
#prd.smpls <- predict(pls.md, spec_trt[,-1])

#FUSI EDIT
prd.smpls <- predict(pls.md, spec_trt[,-1])

prd<-as.data.frame(prd.smpls)
df.prd<-bind_cols(SSN=rownames(prd),prd[,nc])
colnames(df.prd)<-c("SSN",slprptr[p])

pred<-merge(pred, df.prd, by="SSN", all.x = T)
#  }

```

## PCA with Reference Data for Grouping

```{r}

 #Merging spectral (spec_trt) used for PCA earlier with ref data (df.f)

#removed samples with Temp = 950 

spec_ref1 <- merge(df.f,spec_trt)

spec_refA<-spec_ref1%>% filter(is.na(Temp.)|Temp.!=950)

#removing the ref data for the PCA
x<-as.numeric(ncol(df.f))

pcs1 <- prcomp(spec_refA[,-c(1:x)])

#?extracting the first 10 components?
pcss1 <- pcs1$x[,1:10]

pcss1[1:6,]

plot(pcss1)

g <- ggbiplot::ggbiplot(pcs1, #need to specify which library the command is coming from, gives errors otherwise
              obs.scale = 1,
              var.scale = 1 ,
              groups = spec_refA$Temp.,
              #ellipse = TRUE,
              ##circle = TRUE,
              #ellipse.prob = 0.68,
              var.axes=FALSE)

g<-g + scale_fill_discrete(name="Temp")
   #  + guides(color=guide_legend("Temp."))
g



g1 <- ggbiplot::ggbiplot(pcs1, #need to specify which library the command is coming from, gives errors otherwise
              obs.scale = 1,
              var.scale = 1 ,
              groups = spec_refA$Temp_factor,
              #ellipse = TRUE,
              ##circle = TRUE,
              #ellipse.prob = 0.68,
              var.axes=FALSE) 

g1<- g1 + labs(title = "PCA plot with Temp Factor") + 
  theme(panel.border = element_rect(color = "black",
                                    fill = NA,
                                    size = 2),
                                    plot.title = element_text(hjust = 0.5))+
  guides(color=guide_legend("Temperature Range (C)")) + 
  scale_color_manual(labels = c("200-400","400-550","550+","NA"), values=c("brown","red","orange","green")) 
g1


```

## Correlation Matrices

help from the following [link](http://www.sthda.com/english/wiki/correlation-matrix-a-quick-start-guide-to-analyze-format-and-visualize-a-correlation-matrix-using-r-software)

```{r, message=FALSE}
library(Hmisc)
library(PerformanceAnalytics)
library(gplots)
library(plotly)
```

### Hierrchical Clustering

```{r}
df.f$Temp.<-as.numeric(df.f$Temp.)
#df.f[,-1]<-as.numeric(unlist((df.f[,-1]))
cor_mat<-rcorr(as.matrix(df.f[,c(2:ncol(df.f))]))
cor_mat


#Hierrchical Clustering
cor_mat_data<-df.f[,2:(ncol(df.f)-2)]

cor_mat_2<-cor(cor_mat_data, use = "pairwise.complete.obs")
#calculates correlations pairwise between variables while excluding observations that have missing values in either of the two variables being correlated

Hierchical_cluster<-heatmap.2(cor_mat_2, 
          #col = colorRampPalette(c("blue", "white", "red"))(50), 
          dendrogram = "both",
          trace = "none",
          #margins = c(10, 10),
          main = "Correlation Heatmap")
Hierchical_cluster

```

### Interactive Heatmap

```{r}

heatmap_interactive <- plot_ly(
  x = colnames(cor_mat_2),
  y = colnames(cor_mat_2),
  z = cor_mat_2,
  type = "heatmap",
  colorscale = "Viridis"
)
heatmap_interactive

```

```{r}

filtered_cor_mat <- cor_mat_2
filtered_cor_mat[filtered_cor_mat > -0.75 & filtered_cor_mat <0.75] <- NA


#filtered_cor_mat[filtered_cor_mat < 0.75 | filtered_cor_mat > 0.99] <- NA

heatmap_interactive_2 <- plot_ly(
  x = colnames(filtered_cor_mat),
  y = colnames(filtered_cor_mat),
  z = filtered_cor_mat,
  type = "heatmap",
  colorscale = "Viridis"
)
heatmap_interactive_2
```

```{r}

df <- data.frame(
  A = c(1, 2, NA, 4),
  B = c(NA, 2, 3, 4)
)

# Remove rows containing NA values
df_clean <- df[complete.cases(df), ]
```

```{r, warning=FALSE}

corr_data<-df.f[,c(2:20)]
chart.Correlation(corr_data)#, histogram=TRUE)
```

```{r, warning=FALSE}

corr_data_1<-df.f[,c(21:40)]
chart.Correlation(corr_data_1)#, histogram=TRUE)
```

```{r, warning=FALSE}

corr_data_2<-df.f[,c(41:ncol(df.f)-2)]
chart.Correlation(corr_data_2)#, histogram=TRUE)
```
