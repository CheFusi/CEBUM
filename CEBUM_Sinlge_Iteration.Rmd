## Load Libraries

```{r, include=FALSE}

# Define a list of required packages
required_packages <- c("writexl","readxl","randomForest", "caret", "pls", "data.table", "ithir", 
                       "dplyr", "tidyr", "prospectr", "globals", "stringr", 
                       "ggplot2", "here", "tidymodels", "parsnip", "plsmod", 
                       "mixOmics", "yardstick", "purrr", "tibble","ranger","VIM","shiny","FNN","lightgbm", "xgboost", "dials", "tune", "brnn", "recipes","readr")

#VIM for KNN imputation
# Load all required packages quietly without outputting any list of packages
suppressPackageStartupMessages(
  invisible(lapply(required_packages, library, character.only = TRUE))
)
```

## Load reference data

```{r}

# treated spectra
spec_trt <- readRDS("spec_trt_MIR.RDS")
spec_trt_MIR_lab <- readRDS("spec_trt_MIR_lab.RDS")

spec_trt_NIR <- readRDS("/Users/soliverchefusi/Library/CloudStorage/OneDrive-Personal/R/ICRAF&Lehmann/ICRAF_IR/ICRAF_IR_R_Project/ICRAF_IR/CEBUM/CEBUM_NIR/CEBUM_NIR/spec_trt_NIR.RDS")
spec_trt_NIR_lab <- readRDS("/Users/soliverchefusi/Library/CloudStorage/OneDrive-Personal/R/ICRAF&Lehmann/ICRAF_IR/ICRAF_IR_R_Project/ICRAF_IR/CEBUM/CEBUM_NIR/CEBUM_NIR/spec_trt_NIR_lab.RDS")

spec_trt <- spec_trt[,-2] #removing the data for the first wavenumber since it's all NAN's
spec_trt_MIR_lab <- spec_trt_MIR_lab[,-2]

spec_trt_MIR_NIR <- merge(spec_trt,spec_trt_NIR, by ="SSN")
spec_trt_MIR_NIR_lab <- merge(spec_trt_MIR_lab,spec_trt_NIR_lab, by ="SSN")
rownames(spec_trt_MIR_NIR_lab) <- spec_trt_MIR_NIR_lab$SSN

# biochar property data
#df1<-readRDS("df1.RDS")
df1<-readRDS("df.f.RDS")

# df.f: full dataset;
# spec_trt: treated spectra

#Available properties to predict
slprptr<-names(df1[-c(2:9)]) #FUSI EDIT: removing metadata columns 

# defining identity object with names of biochar samples
pred<-as.data.frame(spec_trt[,1])
colnames(pred)<-"SSN"
 
```

## PLSR, RF, GBDT, BRNN Model Workflow

Models based on minimizing RMSEP (objective function)

High-Level Structure:

```         
1.  Preprocessing

2.  Setting up models 

3.  Running models (map() over models to fit them)

4.  Calculating metrics (for each model)

5.  Plotting results (for each model)
```

Adding feature importance:

```{r}

## Feature‐importance helpers

# 1) RF importances  
extract_rf_importance <- function(fitted_wf){
  ranger_obj <- extract_fit_parsnip(fitted_wf)$fit
  ranger_obj$variable.importance %>%
    enframe(name = "feature", value = "importance") %>%
    arrange(desc(importance))
}

# 2) PLSR “importance” via VIP scores
extract_pls_importance <- function(fitted_wf){
  vip::vi(fitted_wf) %>%
    rename(feature = Variable, importance = Importance) %>%
    arrange(desc(importance))
}
```

```{r}

# Initialize the workspace by creating directories for storing outputs and 
# setting up an empty tibble to accumulate performance metrics 
initialize_workspace <- function(identifier) {
  #dir.create(paste0("Plots_Validation_MIR_", identifier), showWarnings = FALSE)
  dir.create(paste0("Plots_WellFitted_Properties_", identifier), showWarnings = FALSE)
  all_metrics <<- tibble()  # Global tibble to store metrics for each property
  
  return(identifier)  # Pass the identifier for later use
}


# slprptr: list of properties; .x: index of property in slprptr
select_property_data <- function(slprptr, .x, df1, spec_trt) {
  # Filter df1 to create df.f based on SSN present in spec_trt
    df.f <- df1 %>%
    filter(SSN %in% spec_trt$SSN) # Select only samples present in both predictor and response datasets
  # Check if the property exists in df1
  if (!slprptr[.x] %in% colnames(df.f)) {
    stop(paste("Property", slprptr[.x], "not found in df1"))
  }
  
  # Columns to select, ensuring required columns are present
  cols_to_select <- c("SSN", "Source", slprptr[.x])
  cols_to_select <- intersect(cols_to_select, colnames(df.f))
  
  if (!all(cols_to_select %in% colnames(df.f))) {
    stop(paste("Columns", paste(cols_to_select, collapse = ", "), "not found in df.f"))
  }
  
  # Filter and preprocess data, including removing outliers for the selected property

    
    df.sel <- df.f %>%
    dplyr::select(dplyr::all_of(cols_to_select)) %>%
    inner_join(spec_trt, by = "SSN") %>%   # Combine predictor and response variables into one df
    na.omit()#%>%
    #filter(data.table::between(!!sym(slprptr[.x]), quantile(!!sym(slprptr[.x]), 0.01), quantile(!!sym(slprptr[.x]), 0.99))) # Filter out extremes
  
  # Set SSN as rownames but keep the column
  rownames(df.sel) <- df.sel$SSN
  # Check if df.sel has rows
  if (nrow(df.sel) == 0) {
    stop(paste("No matching rows in df.sel for property:", slprptr[.x]))
  }
  
  return(list(df.f = df.f, df.sel = df.sel))
}

# Split data into calibration (training) and validation (test) sets
# Allows splitting based on a specific Source if source_name is specified
split_data <- function(df.sel, use_source = FALSE, source_name = NULL) {
  # If a specific source_name is provided, filter data for that source
  
  if (!is.null(source_name)) {
    df.sel <- df.sel %>% filter(Source == source_name)
  }

  if (use_source) {
    set.seed(123) 
    # Split ensuring Source is represented in both subsets
    trainIndex <- createDataPartition(df.sel$Source, p = 0.7, list = FALSE)
  } else {
    set.seed(123) 
    # Split without considering Source
    trainIndex <- sample(seq_len(nrow(df.sel)), size = 0.7 * nrow(df.sel))
  }

  # Remove Source column
    df.sel <- df.sel %>% dplyr::select(-Source)
  
  cal_ids <- rownames(df.sel)[trainIndex]
  val_ids <- rownames(df.sel)[-trainIndex]
    
    # Filter and assign rownames directly
  cal_df <- df.sel %>%
    filter(rownames(.) %in% cal_ids) %>%
    arrange(rownames(.)) 
  
  val_df <- df.sel %>%
    filter(rownames(.) %in% val_ids) %>%
    arrange(rownames(.))

  # Save objects in the global environment
assign("cal_df_global", cal_df, envir = .GlobalEnv)
assign("val_df_global", val_df, envir = .GlobalEnv)

  return(list(cal_df = cal_df, val_df = val_df))
}

prepare_model_data <- function(cal_df, val_df, slprptr, .x) {
  if (nrow(cal_df) <= 10 || nrow(val_df) <= 10) {
    warning("Insufficient rows in cal_df or val_df. Returning NA placeholders.")
    train_data <- cal_df
    test_data <- val_df
    train_data[, ] <- NA
    test_data[, ] <- NA
  } else {
    train_data <- cal_df
    test_data <- val_df
    rownames(train_data) <- train_data$SSN  # Set SSN as rownames but keep the column
    rownames(test_data) <- test_data$SSN
  }
  
  # Save rownames of training and test data for debugging
  debug_rows <- tibble(
    Property = slprptr[.x],
    Dataset = c(rep("Training", nrow(train_data)), rep("Testing", nrow(test_data))),
    SSN = c(rownames(train_data), rownames(test_data))
  )
  
  # Save debugging dataframe globally to inspect later
  if (!exists("debug_row_summary")) {
    assign("debug_row_summary", debug_rows, envir = .GlobalEnv)
  } else {
    debug_row_summary <<- bind_rows(debug_row_summary, debug_rows)
  }
  
  return(list(train_data = train_data, test_data = test_data))
}

# Create a recipe for data preprocessing, with optional custom steps
preprocess_data <- function(train_data, test_data, slprptr, p, recipe_steps = NULL,
                            include_zv = FALSE,
                            include_pca = FALSE,
                            include_pls = FALSE,
                            include_rf = FALSE,
                            num_pca_comp = NULL, 
                            variance_threshold = NULL,
                            variance_threshold_pls = NULL,
                            forest_threshold = NULL) {
  # Check if train/test data is all NA
  if (all(is.na(train_data)) || all(is.na(test_data))) {
    warning("Preprocessing skipped due to all NA in train/test data.")
    return(list(
      recipe = NULL,
      train_data = data.frame(),  # Return an empty data frame as a placeholder
      test_data = data.frame(),
      n_comp_pca = NA,
      n_comp_pls = NA
    ))
  }
  
  formula <- as.formula(paste(slprptr[p], "~ ."))  # Define formula for model
  
  # Define base recipe with ID role (column) for SSN
  recipe_base <- recipe(formula, data = train_data) %>%
    update_role(SSN, new_role = "ID")  # Designate SSN column as an identifier
  
    # Conditionally add step_zv() only if include_zv is TRUE
  if (include_zv) {
    recipe_base <- recipe_base %>% step_zv()%>%
step_nzv()
  }
  
  #================================== PCA Reduction ================================= 
  # Conditionally add PCA only if include_pca is TRUE
  
    # Check available numeric predictors **before** applying PCA

  # Handle PCA selection
  n_comp_pca <- NA  # Default to NA if PCA is not applied
  
  if (include_pca) {
    if (!is.null(num_pca_comp)) {
      
      # Ensure num_pca_comp does not exceed the number of samples - 1
      # num_pca_comp_set <- min(num_pca_comp, ncol(train_data), nrow(train_data) - 1)
      
    num_pca_comp_set <- min(
    num_pca_comp, 
    ncol(train_data), 
    nrow(train_data) - 1,  
    max(10, floor(nrow(train_data) * 0.2))
  )  

  # Fallback to 5 components if the matrix is singular
  if (num_pca_comp_set > (nrow(train_data) - 1) || num_pca_comp_set > (ncol(train_data) - 1)) {
    message("Warning: num_pca_comp (", num_pca_comp, ") too high. Using fallback value of 5 components.")
    num_pca_comp_set <- 5
  }
      

    recipe_base <- recipe_base %>%
      step_pca(all_numeric_predictors(), num_comp = num_pca_comp_set)
     n_comp_pca <- num_pca_comp_set  # Store the manually set component count

         # Extract actual number of components chosen dynamically
    prepped_recipe <- prep(recipe_base, training = train_data, retain = TRUE)
    
    # Extract the actual number of PCA components chosen based on variance
    n_comp_pca_test <- prepped_recipe$steps[[1]]$num_comp 
     
  } else if (!is.null(variance_threshold)) {
    recipe_base_pca_variance <- recipe_base %>%
      step_pca(all_numeric_predictors(), threshold = variance_threshold)
    
    # Extract actual number of components chosen dynamically
    prepped_recipe <- prep(recipe_base_pca_variance, training = train_data, retain = TRUE)
    
    # Extract the actual number of PCA components chosen based on variance
    n_comp_pca_variance <- prepped_recipe$steps[[1]]$num_comp  # Extracts the selected num_comp

    # Ensure the number of PCA components does not exceed nrow(train_data) - 1
    #num_pca_comp_set <- min(n_comp_pca_variance, ncol(train_data), nrow(train_data) - 1)
numeric_cols <- train_data %>%
  dplyr::select(where(is.numeric)) %>%
  ncol()

num_pca_comp_set <- max(
  min(n_comp_pca_variance, numeric_cols, nrow(train_data) - 1, ncol(train_data) - 1),  
  max(min(10, floor(nrow(train_data) * 0.2)), 2)  # Dynamic lower bound  #
)

    # Apply PCA step with the selected number of components
    recipe_base <- recipe_base %>%
      step_pca(all_numeric_predictors(), num_comp = num_pca_comp_set)

    # Store the number of components for later use
    n_comp_pca <- num_pca_comp_set
    
  } else {
    warning("PCA was enabled but neither num_pca_comp nor variance_threshold was provided. PCA will not be applied.")
  }

  # Ensure valid PCA selection
  if (!is.na(n_comp_pca) && n_comp_pca < 2) {
    warning(paste("Skipping PCA for", slprptr[p], "- not enough valid components after preprocessing."))
    return(list(
      recipe = NULL,
      train_data = train_data,
      test_data = test_data,
      n_comp_pca = NA
    ))
  }

if (is.null(n_comp_pca) || is.na(n_comp_pca)) {
    stop("ERROR: n_comp_pca is NULL in preprocess_data()!")
}
  }
    
  #================================== PLSR Reduction ================================= 
 n_comp_pls <- NA
  if (include_pls) {
    if (!is.null(variance_threshold_pls)) {
      # Default: RMSEP-based selection
    recipe_base <- recipe_base %>% step_pls(all_numeric_predictors(), outcome = slprptr[p])  # Default RMSEP-based selection
    } else {
      # Apply PLS with a high number of components first
      max_possible_comp <- min(ncol(train_data), nrow(train_data) - 1)
      temp_recipe <- recipe_base %>%
        step_pls(all_numeric_predictors(), outcome = slprptr[p], num_comp = max_possible_comp)

      # Prep the recipe to extract variance explained
      prepped_temp <- prep(temp_recipe, training = train_data, retain = TRUE)

      # Extract variance explained by each component
      pls_results <- prepped_temp$steps[[1]]$res$Xvar  # Extract X variance explained
      cumulative_variance <- cumsum(pls_results) / sum(pls_results)

      # Find the smallest number of components that reaches the threshold
      n_comp_pls <- which(cumulative_variance >= variance_threshold_pls)[1]

      # Ensure a valid number of components
      if (is.na(n_comp_pls) || n_comp_pls < 2) {
        warning("PLS variance threshold too high, using fallback of 2 components.")
        n_comp_pls <- 2
      }
            # Apply PLS with the selected number of components
      recipe_base <- recipe_base %>%
        step_pls(all_numeric_predictors(), outcome = slprptr[p], num_comp = n_comp_pls)
    }
  }
   
    #============================== Feature Selection (RF) =================================
  n_features_rf <- NA  # Track number of selected features (like PCA/PLS)

    if (include_rf) {
    if (!is.null(forest_threshold)) {
      message("Applying step_select_forests() with threshold: ", forest_threshold)
      
      # Apply feature selection with default parameters (no pre-determined mtry or min_n)
      recipe_base <- recipe_base %>% step_select_forests(
        all_numeric_predictors(), 
        threshold = forest_threshold
      )
      
      # Prep to get selected feature count
      # prepped_recipe_rf <- prep(recipe_base, training = train_data, retain = TRUE)
      # selected_features <- tidy(prepped_recipe_rf, number = 1)  # Extract selected features
      # n_features_rf <- nrow(selected_features)  # Count of features after selection
      
      # if (n_features_rf < 2) {
      #   warning("Too few features selected. RF threshold may be too high. Consider lowering it.")
      #   n_features_rf <- NA  # Mark as NA to indicate failure
      # }
    } else {
      warning("include_rf = TRUE but no threshold specified. Skipping RF-based selection.")
    }
  }
      
  #======================================== END  =====================================
  
  
   # Apply additional preprocessing steps as needed
  if (!is.null(recipe_steps)) {
    for (step in recipe_steps) {
      recipe_base <- recipe_base %>% step
    }
  }
  
  return(list(recipe = recipe_base, train_data = train_data, test_data = test_data, 
              n_comp_pca = n_comp_pca, 
              n_comp_pls = n_comp_pls, 
              n_features_rf = n_features_rf))
}

# Set up PLS or Random Forest model specification, including tuning parameters
setup_model <- function(model_type, train_data = NULL, response_var = NULL) {
  if (model_type == "pls") {
    # Configure a PLS model specification
    model_spec <- parsnip::pls() %>%
      set_mode("regression") %>%
      set_engine("mixOmics") %>%
      set_args(num_comp = tune())
    mtry_range <- NULL
  } else if (model_type == "rf") {
    # Configure a Random Forest model with tuning for mtry and min_n
    if (is.null(train_data)) stop("train_data must be provided for Random Forest.")
    if (is.null(response_var)) stop("response_var must be specified for Random Forest.")
    
    # Remove SSN and response columns to get only predictor columns
    predictors_only <- train_data %>%
      dplyr::select(-SSN, -all_of(response_var))
        # Check if predictors are sufficient
    if (ncol(predictors_only) < 2) stop("Random Forest requires at least 2 predictor variables.")
    
    
    # Finalize mtry_range
    # mtry_range <- dials::finalize(dials::mtry(), predictors_only) # predictors_only is used to set the max value for the mtry range - so here mtry ranges up to the total number of predictors (e.g. number of wavenumbers).
    mtry_range <- dials::mtry(range = c(2, round(sqrt(ncol(predictors_only)))))
    
    if (is.null(mtry_range)) stop("Failed to initialize mtry_range in setup_model(). Check predictor columns in train_data.")
    
    model_spec <- parsnip::rand_forest(mtry = tune(), min_n = tune()) %>%
      set_mode("regression") %>%
      set_engine("ranger", importance = "permutation") ##edited for Feature Importance
  } else if (model_type == "gbdt") {
    # Gradient-Boosted Decision Tree setup
    predictors_only <- train_data %>%
      dplyr::select(-SSN, -all_of(response_var))
    model_spec <- parsnip::boost_tree(
      trees = tune(),  # Number of trees to tune
      tree_depth = tune(),  # Depth of each tree
      learn_rate = tune(),  # Learning rate
      loss_reduction = tune(),  # Minimum gain for split
      min_n = tune(),  # Tune minimal node size
      #sample_size = tune(),  # Tune row sampling
      mtry = tune()  # Tune predictor sampling
    ) %>%
      set_mode("regression") %>%
      set_engine("xgboost")
                 #early_stopping_rounds = 10)  #
    mtry_range <- NULL  # Not applicable for GBDT
  } else if (model_type == "brnn") {
    model_spec <- parsnip::mlp(hidden_units = tune(), penalty = tune()) %>%
      set_mode("regression") %>%
      set_engine("nnet")
    mtry_range <- NULL
  } else {
    stop("Unsupported model type: ", model_type)
  }
  return(list(model_spec = model_spec, mtry_range = mtry_range))
}

calculate_metrics <- function(data, predictions, data_type, slprptr, .x, best_params, model_type, test_data) {
  response_var <- slprptr[.x]  # Define response variable

  # Check if response variable is present and .pred column exists in predictions
  if (!response_var %in% colnames(data) || !".pred" %in% colnames(predictions)) {
    message(paste("Skipping metrics calculation due to missing columns."))
    return(tibble(Property = response_var, Data_Type = data_type, Model = model_type, R2 = NA, RMSE = NA))
  }

  # Safe computation function with error handling for each metric
  safe_compute <- function(expression) {
    tryCatch(eval(expression), error = function(e) NA)
  }

  # Calculate performance metrics
  r2_value <- safe_compute(rsq_vec(truth = data[[response_var]], estimate = predictions$.pred))
  rmse_value <- safe_compute(rmse_vec(truth = data[[response_var]], estimate = predictions$.pred))
  bias_value <- safe_compute((mean(predictions$.pred - data[[response_var]], na.rm = TRUE) / mean(data[[response_var]], na.rm = TRUE))*100)

  # Calculate the range of the response variable from the test_data and RMSEP/range
  response_range <- safe_compute(max(test_data[[response_var]], na.rm = TRUE) - min(test_data[[response_var]], na.rm = TRUE))
  rmse_range_ratio <- if (!is.na(response_range) && response_range > 0) rmse_value / (response_range / 10) else NA

  metrics_tibble <- tibble(
    Property = response_var,
    Data_Type = data_type,
    Model = model_type,
    Comps = if (model_type == "pls") best_params$num_comp else NA,
    #mtry = if (model_type == "rf") best_params$mtry else NA,
    #min_n = if (model_type == "rf") best_params$min_n else NA,
    mtry = if (model_type %in% c("rf", "gbdt")) best_params$mtry else NA,  
    min_n = if (model_type %in% c("rf", "gbdt")) best_params$min_n else NA,  
    N = nrow(data),
    R2 = r2_value,
    RMSE = rmse_value,
    RMSEP_range_ratio = rmse_range_ratio,
    bias = bias_value
    #n_comp_pca = if (include_pca) n_comp_pca else NA  # Only assign if PCA was applied
  )

  return(metrics_tibble)
}

# Define properties for core and related metrics
get_property_groups <- function() {
  core_properties <- c(
    "Temp.", "pH", "EC_uS", "Ash_avg", "exchg_Ca_mmol_kg", 
    "exchg_K_mmol_kg", "exchg_Mg_mmol_kg", "exchg_Na_mmol_kg", "exchg_P_mmol_kg", 
    "Ca_mg_kg", "K_mg_kg", "Mg_mg_kg", "C_tot_avg", "N_tot_avg",
    "C_org_wt", "H_tot_wt", "O_ult", "P_avg_mg_kg", "Fe_avg_mg_kg", "Zn_avg_mg_kg", 
    "Cu_avg_mg_kg", "Ni_avg_mg_kg", "As_avg_mg_kg", "Cd_avg_mg_kg"
  )
  
  related_properties <- c(
    "Naphthalin_mg_kg", "X2_Methylnaphthalin", "X1_Methylnaphthalin", "Sum_Naphthaline", 
    "Acenaphthylen", "Acenaphthen", "Fluoren", "Phenanthren", "Anthracen", 
    "Fluoranthen", "Pyren", "Chrysen", "Benzo_a_anthracen", 
    "Benzo_b_plus_k_fluoranthen", "Benzo_a_pyren", "Dibenzo_a_h_anthracen", 
    "Indeno_c_d_pyren", "Benzo_g_h_i_perylen", "Sum_PAH_defined_EPA_mg_kg", 
    "Bulk.Density_mg_m3", "C_tot_to_N_wt", "O_less_Al_Si_Fe_to_C", "O_less_Al_Si_Fe_Ca_to_C","O_less_Al_Si_Fe_C_inorg_to_C", "O_less_C_inorg_to_C", "C_org_to_N_wt", 
    "C_tot_to_N_molar", "Htot_to_C_tot_molar", "Oult_to_C_tot_molar", 
    "Htot_to_C_org_molar", "Oult_to_C_org_molar", "O_less_Al_Si_Fe_to_C_org_molar", 
    "O_less_Al_Si_Fe_Ca_to_C_org_molar", "O_less_Al_Si_Fe_C_inorg__to_C_org_molar", 
    "O_less_C_inorg_to_C_org_molar"
  )
  
  return(list(core_properties = core_properties, related_properties = related_properties))
}

filter_metrics_by_group <- function(all_metrics, property_groups) {
  core_metrics <- all_metrics %>% filter(Property %in% property_groups$core_properties)
  related_metrics <- all_metrics %>% filter(Property %in% property_groups$related_properties)
  return(list(core_metrics = core_metrics, related_metrics = related_metrics))
}


# Function to plot measured vs. predicted values and save in the default directory
plot_predictions <- function(val_plot_data, val_df, slprptr, .x, combined_metrics, model_type, identifier) {
  # Check if the property exists in the validation plot data
  if (!(slprptr[.x] %in% colnames(val_plot_data))) return(NULL)

  # Extract metrics for validation
  val_metrics <- combined_metrics %>% filter(Data_Type == "Validation", Model == model_type)
  
  # Add label text with components and R²
  label_text <- paste("Comps:", val_metrics$Comps, "R²:", round(val_metrics$R2, 3))

  # Generate the plot
  # validation_plot <- ggplot(val_plot_data, aes_string(x = slprptr[.x], y = ".pred")) +
  #   geom_point(color = "blue", size = 2) +
  #   geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  #   labs(
  #     title = paste("Measured vs Predicted for", slprptr[.x], "-", model_type),
  #     x = paste("Measured", slprptr[.x]),
  #     y = "Predicted"
  #   ) +
  #   theme_minimal() +
  #   annotate("text", x = Inf, y = -Inf, hjust = 1.1, vjust = -0.5, label = label_text, size = 4)

  # Save the plot
  # ggsave(
  #   filename = paste0("Plots_Validation_PLSR_MIR_", identifier, "/", slprptr[.x], "_", model_type, ".png"),
  #   plot = validation_plot, width = 7, height = 7, dpi = 300
  #)
}

# Plot R² values and differences for well-fitted properties based on model criteria
plot_well_fitted_r2 <- function(all_metrics, identifier) {
  all_r2_values <- all_metrics %>%
    group_by(Property) %>%
    reframe(
      R2_cal = R2[Data_Type == "Calibration"],
      R2_val = R2[Data_Type == "Validation"],
      R2_diff = abs((R2_cal - R2_val) / R2_cal)
    )
  
  well_fitted_properties <- all_r2_values %>%
    filter(R2_diff <= 0.15, R2_cal > 0.6, R2_val > 0.6) %>%
    pull(Property)
  
  plot_data <- all_metrics %>%
    filter(Property %in% well_fitted_properties, Data_Type %in% c("Calibration", "Validation")) %>%
    dplyr::select(Property, Data_Type, R2, N, RMSE, bias,RMSEP_range_ratio) %>%
    dplyr::rename(Value = R2)
  
  well_fitted_plot <- ggplot(plot_data, aes(x = Property, y = Value, fill = Data_Type)) +
    geom_bar(stat = "identity", position = "dodge") +
    scale_fill_manual(values = c("Calibration" = "purple", "Validation" = "darkgreen")) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
      panel.grid = element_blank(),
      panel.border = element_rect(color = "black", fill = NA)
    ) +
    labs(title = "R² Comparison for Well-Fitted Properties",
         subtitle = "Purple: Calibration | Green: Validation",
         y = "R² Value",
         x = "Property") +
    geom_text(
      aes(label = ifelse(Data_Type == "Calibration", paste0("N:", N),
                         paste0("N:", N, "\nRMSE%:\n", round(RMSEP_range_ratio, 2), "\nRel_Bias:\n", round(bias, 2)))),
      position = position_dodge(width = 0.9), 
      vjust = -0.5, size = 3
    ) +
    ylim(0, 1.35)  # Set y-axis limit to allow space for annotations
  
  print(well_fitted_plot)
  
  ggsave(filename = paste0("Plots_WellFitted_Properties_", identifier, "/well_fitted_properties_plot.png"), 
         plot = well_fitted_plot, width = 10, height = 7, dpi = 300)
}

# Wrapper function to run analysis for each property in slprptr with specified model_type (no default)
run_all_properties <- function(slprptr, df1, spec_trt, model_type, identifier, 
                                use_source = FALSE, source_name = NULL, tuning_goal = "rmsep",
                                property_range = 2:length(slprptr),
                                include_zv = FALSE, include_pca = FALSE,
                                num_pca_comp = NULL, variance_threshold = NULL,
                                include_pls = FALSE, variance_threshold_pls = NULL,
                                include_rf = FALSE, forest_threshold = NULL) {
  #all_metrics <<- tibble()  # Initialize global metrics storage
  model_results <- list()  # Store model fits for later use
  output_dir <- paste0("Plots_WellFitted_Properties_", identifier)  # Use existing directory

  
  # Validate the property_range parameter
  if (any(property_range > length(slprptr) | property_range < 1)) {
    stop("property_range contains indices outside the valid range of slprptr.")
  }
  
  # Loop through each property in the specified range and collect metrics
  all_metrics <- purrr::map_dfr(property_range, function(.x) {
    message("Processing property: ", .x, " : ", slprptr[.x])
    # Select and filter the property data
    property_data <- select_property_data(slprptr, .x, df1, spec_trt)
    df.f <- property_data$df.f
    df.sel <- property_data$df.sel
    
    # Split the data into calibration (training) and validation (test) sets
    split <- split_data(df.sel, use_source = use_source, source_name = source_name)
    cal_df <- split$cal_df
    val_df <- split$val_df
    
    # Skip if the calibration or validation datasets have insufficient rows
    if (nrow(cal_df) <= 10 || nrow(val_df) <= 10) {
      message(paste("Skipping property:", slprptr[.x], "due to insufficient data"))
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
    # Prepare train/test data and preprocess (e.g., removing outliers)
    data_prep <- prepare_model_data(cal_df, val_df, slprptr, .x)  # Converts calibration and validation datasets to train/test format
    recipe_data <- preprocess_data(data_prep$train_data, data_prep$test_data, slprptr, .x, 
      include_zv=include_zv, 
      include_pca = include_pca,  num_pca_comp = num_pca_comp, variance_threshold = variance_threshold, 
      include_pls = include_pls, variance_threshold_pls = variance_threshold_pls,
      include_rf = include_rf, forest_threshold = forest_threshold)  # Handles preprocessing steps
    
    #Ensure train_data and test_data are updated after additional step e.g. step_pca
    train_data <- recipe_data$train_data
    test_data <- recipe_data$test_data
    
    n_comp_pca <- recipe_data$n_comp_pca 
    
    # Ensure train/test data is not empty after preprocessing
    if (nrow(recipe_data$train_data) == 0 || nrow(recipe_data$test_data) == 0) {
      message(paste("Skipping property:", slprptr[.x], "due to empty train/test data after preprocessing."))
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
    # Step 3: Skip properties with insufficient train/test data
    if (nrow(data_prep$train_data) <= 10 || nrow(data_prep$test_data) <= 10) {
      message(paste("Skipping property:", slprptr[.x], "due to insufficient train/test data"))
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
    # Set up the model specification (PLS or RF) based on model_type
    if (model_type == "pls") {
      model_spec_list <- setup_model(model_type, train_data = data_prep$train_data)  # Set up PLS model
    } else if (model_type == "rf") {
      model_spec_list <- setup_model(model_type, train_data = data_prep$train_data, response_var = slprptr[.x])  # Set up RF model
    } else if (model_type == "gbdt") {
      model_spec_list <- setup_model(model_type, train_data = data_prep$train_data)  # Set up GBDT model
    } else if (model_type == "brnn") {  # 🔥 Fix: Ensure BRNN is handled
  model_spec_list <- setup_model(model_type, train_data = data_prep$train_data)  # Set up BRNN model
  } else {
      stop(paste("Unsupported model type:", model_type))  # Error for unsupported models
    }
    
    
    # Fit the model using the specified tuning goal (e.g., RMSEP) and handle errors gracefully
  
    model_fit <- tryCatch(
      {
        fit_model(
          model_spec_list, 
          train_data = recipe_data$train_data, 
          test_data = recipe_data$test_data, 
          recipe = recipe_data$recipe, 
          model_type = model_type, 
          tuning_goal = tuning_goal,
          property_name = slprptr[.x],
          include_zv = include_zv,
          include_pca = include_pca,
          include_pls = include_pls,
          include_rf = include_rf,
          variance_threshold = variance_threshold,
          num_pca_comp = num_pca_comp,
          n_comp_pca = n_comp_pca,
          n_comp_pls = recipe_data$n_comp_pls,
          variance_threshold_pls = variance_threshold_pls,
          forest_threshold = forest_threshold 
        )
      },
      error = function(e) {
        # Log detailed debugging info
        warning(paste(
          "Error with property:", slprptr[.x], "\n",
          "Training data dimensions:", dim(recipe_data$train_data), "\n",
          "Test data dimensions:", dim(recipe_data$test_data), "\n",
          "Error message:", e$message
        ))
        return(tibble(Property = slprptr[.x], Data_Type = "Error", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
      }
    )
    
    ## Feature importance
    # collect feature‐importance for this property
prop_name <- slprptr[.x]
# if (model_type == "rf") {
#   model_results[[prop_name]] <<- extract_rf_importance(model_fit$model)
# }
if(model_type=="rf") {
      model_results[[slprptr[.x]]] <<- extract_rf_importance(model_fit$model)
    }
if (model_type == "pls") {
  model_results[[prop_name]] <<- extract_pls_importance(model_fit$model)
}
    
    
    # Handle skipped properties due to model fitting issues
    if ("Data_Type" %in% colnames(model_fit) && model_fit$Data_Type[1] == "Skipped") {
      return(model_fit)
    }
    
    # Generate predictions for calibration and validation datasets
    cal_predictions <- predict(model_fit$model, new_data = recipe_data$train_data)
    val_predictions <- predict(model_fit$model, new_data = recipe_data$test_data)
    
    # Calculate performance metrics (R², RMSE, Bias, etc.) for both datasets
    cal_metrics <- calculate_metrics(
      data = recipe_data$train_data, 
      predictions = cal_predictions, 
      data_type = "Calibration", 
      slprptr = slprptr, 
      .x = .x, 
      best_params = model_fit$best_params, 
      model_type = model_type, 
      test_data = recipe_data$test_data
    )
    val_metrics <- calculate_metrics(
      data = recipe_data$test_data, 
      predictions = val_predictions, 
      data_type = "Validation", 
      slprptr = slprptr, 
      .x = .x, 
      best_params = model_fit$best_params, 
      model_type = model_type, 
      test_data = recipe_data$test_data
    )
    
    # Combine metrics for calibration and validation
    combined_metrics <- bind_rows(cal_metrics, val_metrics)
    
    # Plot predictions (commented out for now; enable as needed)
    # plot_predictions(
    #   val_plot_data = recipe_data$test_data %>% dplyr::select(slprptr[.x]) %>% bind_cols(val_predictions),
    #   val_df = val_df,
    #   slprptr = slprptr,
    #   p = .x,
    #   combined_metrics = combined_metrics,
    #   model_type = model_type,
    #   identifier = identifier
    # )

    return(combined_metrics)
  })
  
  ##Feature Importance
  
  # at the bottom of run_all_properties(), once all_metrics is written:
#── Write out the RF importances as one CSV ──#
if (model_type == "rf") {
  all_rf_imps <- dplyr::bind_rows(model_results, .id = "Property")
  write.csv(
    all_rf_imps,
    file.path(output_dir, paste0("RF_importances_", identifier, ".csv")),
    row.names = FALSE
  )
}
  
#── Write out the PLS “VIP” importances as one CSV ──#
if (model_type == "pls") {
  all_pls_imps <- dplyr::bind_rows(model_results, .id = "Property")
  write.csv(
    all_pls_imps,
    file.path(output_dir, paste0("PLSR_vip_scores_", identifier, ".csv")),
    row.names = FALSE
  )
}
  
  # Retrieve property groups for filtering metrics
  property_groups <- get_property_groups()
  
  # Filter metrics into core and related property groups
  filtered_metrics <- filter_metrics_by_group(all_metrics, property_groups)
  
  # Save filtered metrics as separate tabs in an Excel file
  output_data <- list(
    "All Metrics" = all_metrics,
    "Core Properties" = filtered_metrics$core_metrics,
    "Related Properties" = filtered_metrics$related_metrics
  )
  output_file <- paste0("Plots_WellFitted_Properties_", identifier, "/all_metrics_", identifier, ".xlsx")
  writexl::write_xlsx(output_data, path = output_file)
  

  # Plot well-fitted R² results
  plot_well_fitted_r2(all_metrics, identifier)
}
```

##  Base model 1

```{r}

# Function to calculate cumulative explained variance
calculate_explained_variance <- function(train_data, response_var, max_comps) {
  # Prepare X and Y matrices
  X <- train_data %>%
    dplyr::select(-c(SSN, all_of(response_var))) %>%
    dplyr::mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  Y <- train_data %>%
    dplyr::select(all_of(response_var)) %>%
    dplyr::mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  # Explained variance calculation
  variance_list <- purrr::map_dbl(1:max_comps, function(num_comp) {
    model <- tryCatch(
      mixOmics::pls(X = X, Y = Y, ncomp = num_comp),
      error = function(e) {
        message("Error in fitting PLS model: ", e)
        return(NULL)
      }
    )
    
    if (is.null(model)) {
      return(0)
    }
    
    # Calculate cumulative explained variance for X up to num_comp
    cumulative_variance_X <- sum(model$prop_expl_var$X[1:num_comp])
    cumulative_variance_X
  })
  
  return(variance_list)
}
```

```{r}

# Main fit_model function
fit_model <- function(model_spec_list, train_data, test_data, recipe, model_type,
                      tuning_goal = "rmsep", cumulative_variance_threshold = 0.85,property_name = NULL,
                      include_zv = FALSE, 
                      variance_threshold = NULL, 
                      include_pca = FALSE, 
                      include_pls = FALSE,  
                      n_comp_pca_variance = NULL,
                      num_pca_comp = NULL, 
                      n_comp_pca = NULL, 
                      n_comp_pls = NULL, 
                      variance_threshold_pls = NULL, 
                      include_rf = FALSE, 
                      forest_threshold = NULL) { 
  
  model_spec <- model_spec_list$model_spec
  mtry_range <- model_spec_list$mtry_range  # Only for Random Forest

  # Set up workflow with recipe and model
  biochar_workflow <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(model_spec)
 
   # Setting defalt CV
    set.seed(123)
  cv_folds <- if (nrow(train_data) <= 10) {
    vfold_cv(train_data, v = nrow(train_data))  
  } else {
    vfold_cv(train_data, v = 10)  
  }

  if (model_type == "pls") {
    
      # Define cross-validation folds (LOOCV for <= 10 samples)
  set.seed(123)  # Ensure reproducibility
  cv_folds <- if (nrow(train_data) <= 10) {
    vfold_cv(train_data, v = nrow(train_data))  # Leave-One-Out CV
  } else {
    vfold_cv(train_data, v = 10)  # 10-fold CV
  }
      # Ensure max_comps considers both num_pca_comp and variance_threshold dynamically
    if (isTRUE(include_pca) && !is.null(n_comp_pca)) {
    max_comps <- min(25, nrow(train_data) - 1,  ncol(train_data), n_comp_pca)
  } else {
    max_comps <- min(25, nrow(train_data) - 4,  ncol(train_data))
  }

    if (tuning_goal == "rmsep") {
      # Perform 10-fold (or LOOCV) to minimize RMSEP
      num_comp_grid <- tibble(num_comp = 1:max_comps)
      tune_results <- tune_grid(
        biochar_workflow,
        resamples = cv_folds,
        grid = num_comp_grid,
        metrics = metric_set(rmse),
        control = control_grid(save_pred = TRUE)
      )

      best_params <- tune_results %>% select_best(metric = "rmse") %>% dplyr::select(num_comp)
      final_workflow <- finalize_workflow(biochar_workflow, best_params)
      
      #*Only Generate RMSEP Plot for PLS Models**
      if (!is.null(property_name)) {
        rmsep_data <- tune_results %>%
          collect_metrics() %>%
          filter(.metric == "rmse") %>%
          dplyr::select(num_comp, mean)

        rmsep_plot <- ggplot(rmsep_data, aes(x = num_comp, y = mean)) +
          geom_line() +
          geom_point() +
          theme_minimal() +
          labs(
            title = paste("PLS: Comp vs RMSEP for", property_name),
            x = "Number of Components",
            y = "RMSEP"
          )

        print(rmsep_plot)
      }  # Display the plot in R

    } else if (tuning_goal == "variance") {
      
        # Define cross-validation folds (LOOCV for <= 10 samples)
      set.seed(123)  # Ensure reproducibility
      cv_folds <- if (nrow(train_data) <= 10) {
        vfold_cv(train_data, v = nrow(train_data))  # Leave-One-Out CV
      } else {
        vfold_cv(train_data, v = 10)  # 10-fold CV
      }
      
      # Calculate cumulative explained variance
      response_var <- colnames(train_data)[[2]]  # Adjust index if response var is at a different position
      explained_variance <- calculate_explained_variance(train_data, response_var, max_comps)

      # Find the minimum number of components meeting the variance threshold
      num_comp <- which(explained_variance >= cumulative_variance_threshold)[1]
      if (is.na(num_comp) || is.null(num_comp)) {
        message(paste("Skipping property:", response_var, "due to insufficient cumulative variance."))
        return(tibble(
          Property = response_var,
          Data_Type = "Skipped",
          Model = model_type,
          N = NA,
          R2 = NA,
          RMSE = NA,
          bias = NA,
          ncomp = NA
        ))
      }

      # Validate the selected number of components with cross-validation
      num_comp_grid <- tibble(num_comp = num_comp)
      tune_results <- tune_grid(
        biochar_workflow,
        resamples = cv_folds,
        grid = num_comp_grid,
        metrics = metric_set(rmse),
        control = control_grid(save_pred = TRUE)
      )
    best_params <- tibble(num_comp = num_comp)
    final_workflow <- finalize_workflow(biochar_workflow, best_params)
    }
  } else if (model_type == "rf") {
  set.seed(123)
  
  
  # Dynamically adjust folds
  cv_folds <- if (nrow(train_data) <= 20) {
    vfold_cv(train_data, v = min(floor(nrow(train_data) / 2), 5))  # Cap folds for small datasets
  } else {
    vfold_cv(train_data, v = min(10, nrow(train_data) - 1))  # Default for larger datasets
  }
  
  # Log fold details
  #message("Number of folds: ", length(cv_folds$splits))
  #test_sizes <- purrr::map_int(cv_folds$splits, ~ nrow(assessment(.x)))
  #train_sizes <- purrr::map_int(cv_folds$splits, ~ nrow(analysis(.x)))
  #message("Test set sizes: ", paste(test_sizes, collapse = ", "))
  #message("Train set sizes: ", paste(train_sizes, collapse = ", "))
  
  # Fallback for very small datasets
  # if (nrow(train_data) < 20 || min(test_sizes) < 5) {
  #   warning("Dataset too small for cross-validation. Using fixed train-test split.")
  #   
  #   split <- initial_split(train_data, prop = 0.8)
  #   inner_train <- training(split)
  #   inner_test <- testing(split)
    
    # if (nrow(inner_train) < 5 || nrow(inner_test) < 2) {
    #   stop("Insufficient data for fixed split. Skipping property.")
    # }
    
    rf_model <- rand_forest(
      mtry = floor(sqrt(ncol(train_data) - 1)),# overridden by tuning grid
      min_n = 2,# overridden by tuning grid
      trees = 1000
    ) %>%
      set_engine("ranger", importance = "permutation") %>% ##edited for Feature Importance) %>%
      set_mode("regression")
    
    final_workflow <- workflow() %>%
      add_recipe(recipe) %>%
      add_model(rf_model)
    
    #model_fit <- fit(final_workflow, data = train_data)
    #predictions <- predict(model_fit, inner_test)
    
    # return(list(model = model_fit, predictions = predictions))
  #}
  
  # Dynamically adjust grid size
  rf_grid <- grid_random(
    mtry_range,
    min_n(range = c(1, min(5, floor(nrow(train_data) / 3)))),  # Smaller range for min_n
    size = min(nrow(train_data) / length(cv_folds$splits), 5)  # Adjust grid size dynamically
  )
  
  # Log grid size
  #message("Grid size: ", nrow(rf_grid))
  
  # Tune the model
  tune_results <- tune_grid(
    biochar_workflow,
    resamples = cv_folds,
    grid = rf_grid,
    metrics = metric_set(rmse),
    control = control_grid(save_pred = TRUE)  # Add verbose output
  )
  
  best_params <- tune_results %>% select_best(metric = "rmse")
  final_workflow <- finalize_workflow(biochar_workflow, best_params)
  
  } else if (model_type == "brnn") {
       num_predictors <- ncol(train_data) - 2  # Adjust if more metadata columns exist
       max_hidden_units <- min(20, floor(log2(num_predictors)))  # Choose sqrt(features) but cap at 5

    brnn_grid <- grid_random(
      hidden_units(range = c(1, max_hidden_units)),
      penalty(range = c(0.001, 0.1)),
      size = 10
    )
    
    tune_results <- tune_grid(
      biochar_workflow,
      resamples = cv_folds,
      grid = brnn_grid,
      metrics = metric_set(rmse),
      control = control_grid(save_pred = TRUE)
    )

    best_params <- tune_results %>% select_best(metric = "rmse")
    final_workflow <- finalize_workflow(biochar_workflow, best_params)
    
    } else if (model_type == "gbdt") {
      
      set.seed(123)
    #cv_folds <- vfold_cv(train_data, v = min(10, nrow(train_data) - 1))  # Reduce CV folds
    cv_folds <- vfold_cv(train_data, v = ifelse(nrow(train_data) < 30, 5, min(10, nrow(train_data) - 1)))
      # Define a GBDT-specific tuning grid
      gbdt_grid <- grid_random(
        trees(range = c(50, 200)),  # Tune number of trees #tried 300
        #tree_depth(range = c(3, 10)),  # Tune tree depth
        tree_depth(range = c(2, min(4, round(log2(nrow(train_data)))))),  # Log-based depth scaling
        learn_rate(range = c(0.005, 0.03 )),  # Tune learning rate #Tried (0.1, 0.3)
        loss_reduction(range = c(0, 5)),  # Add back to control unnecessary splits
        min_n(range = c(2, min(10, round(0.05 * nrow(train_data))))),  # Bring back to prevent overfitting
        mtry(range = c(ceiling(0.05 * ncol(train_data)), min(15, floor(0.4 * ncol(train_data))))),
        # Proportion of predictors sampled at each split
        size = 10 #10
      )
      
      # Tune the GBDT model using the grid
      tune_results <- tryCatch({
        tune_grid(
          biochar_workflow,
          resamples = cv_folds,
          grid = gbdt_grid,
          metrics = metric_set(rmse),
          control = control_grid(save_pred = TRUE)
        )
      }, error = function(e) {
        warning("Error during GBDT tuning: ", e$message)
        return(NULL)
      })

      # If tuning failed, return NULL
      if (is.null(tune_results)) return(NULL)

      # Select the best parameters based on RMSE
      best_params <- tune_results %>% select_best(metric = "rmse")
      # Finalize the workflow with the best parameters
      final_workflow <- finalize_workflow(biochar_workflow, best_params)
      
    }
  
  # Fit the final model
  model_fit <- fit(final_workflow, data = train_data)
  
#   model_fit <- tryCatch(
#   fit(final_workflow, data = train_data),
#   error = function(e) {
#     message("Error in model fitting: ", e$message)
#     return(NULL)
#   }
# )

  # Predictions for train and test data
  train_predictions <- tryCatch({
    predict(model_fit, new_data = train_data) %>%
    as_tibble()  # Convert predictions to a tibble
  }, error = function(e) {
    message("Error in train_predictions: ", e)
    return(NULL)
  })

  # Add rownames from `test_data` to `test_predictions`
  if (!is.null(train_predictions)) {
    rownames(train_predictions) <- rownames(train_data)
  }

  test_predictions <- tryCatch({
    predict(model_fit, new_data = test_data) %>%
    as_tibble()  # Convert predictions to a tibble
  }, error = function(e) {
    message("Error in test_predictions: ", e)
    return(NULL)
  })

  # Add rownames from `test_data` to `test_predictions`
  if (!is.null(test_predictions)) {
    rownames(test_predictions) <- rownames(test_data)
  }

  # Ensure predictions are non-null
  if (is.null(train_predictions) || is.null(test_predictions)) {
    stop("Prediction failed: check data dimensions or NA values in predictor matrix.")
  }
  
  return(list(
    model = model_fit,
    train_predictions = train_predictions,
    test_predictions = test_predictions,
    best_params = best_params
  ))
}
```

#### End of code as of 18/02/2025

------------------------------------------------------------------------

## PLSR

#### Tune to minimize RMSEP - MIR

selecting the number of components that minimizes the RMSEP

Total split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep",  property_range = c(56, 59, 72)) #num_bootstrap = 5 ,
```

### Viewing Feature Importance: PLSR 

```{r}

conflicted::conflict_prefer("select", "dplyr")
conflicted::conflict_prefer("filter", "dplyr")
conflicted::conflict_prefer("lag",    "dplyr")
conflicted::conflict_prefer("lead",   "dplyr")
conflicted::conflicts_prefer(purrr::map)

# 1) read in your VIP CSV
vip_df <- read_csv(
  "Plots_WellFitted_Properties_PLSR_RMSEP_MIR_totsplit_99perc/PLSR_vip_scores_PLSR_RMSEP_MIR_totsplit_99perc.csv"
)

# 2) filter to just the properties you want, and make wavenumber numeric
vip_sel <- vip_df %>% 
  filter(Property %in% c("H_tot_molar","H_tot_to_C_tot_molar","O_less_Al_Si_Fe")) %>%
  mutate(
    wavenumber = as.numeric(feature),
    importance = as.numeric(importance)
  ) %>%
  arrange(Property, wavenumber)

# 3) detect local peaks (strictly higher than both neighbors) AND VIP > 1
peaks <- vip_sel %>%
  group_by(Property) %>%
  arrange(wavenumber) %>%
  mutate(
    prev_imp = lag(importance),
    next_imp = lead(importance)
  ) %>%
  filter(
    importance > prev_imp,
    importance > next_imp,
    importance > 1
  ) %>%
  ungroup() %>%
  select(Property, wavenumber, importance)

# 4) from those peaks, pick the top 10 by VIP for each Property
top10_peaks <- peaks %>%
  group_by(Property) %>%
  slice_max(order_by = importance, n = 10, with_ties = FALSE) %>%
  ungroup()

peak_list_10 <- split(top10_peaks, top10_peaks$Property)
# e.g. peak_list[["H_tot_molar"]] is the tibble of its top‐10 peaks





# 6) build one ggplot per property, adding dotted v-lines at those top-10 peaks

plot_list <- vip_sel %>%
  split(.$Property) %>%
  map(~{
    dat <- .
    prop <- unique(dat$Property)
    tp   <- peak_list_10[[prop]]
    ggplot(dat, aes(wavenumber, importance)) +
      geom_line(color = "#2C3E50", size = 0.3) +
      geom_hline(yintercept = 1, linetype = "dashed", color = "grey50") +
      geom_vline(data = tp, aes(xintercept = wavenumber),
                 linetype = "dotted", color = "firebrick", size = 0.5) +
      labs(
        #title = prop,
        x     = expression("Wavenumber ("*cm^-1*")"),
        y     = "VIP score"
      ) +
      theme_classic(base_size = 14) +
      theme(
        plot.title        = element_text(face = "bold", hjust = 0.5),
        panel.border      = element_rect(color = "black", fill = NA, size = 1),
        panel.grid.major  = element_blank(),
        panel.grid.minor  = element_blank()
      )
  })

# 7) now to view them one at a time:
plot_list$H_tot_molar
plot_list$H_tot_to_C_tot_molar

# optionally, save your peak table and/or each plot:
#write_csv(top10_peaks, "top10_vip_peaks.csv")

# to save plots:
# iwalk(plot_list, ~ ggsave(
#   filename = paste0(.y, "_vip_peaks.png"),
#   plot     = .x,
#   width    = 8, height = 4
# ))
```

```{r}

# assume peak_list and plot_list from above

output_dir <- "Plots_WellFitted_Properties_PLSR_RMSEP_MIR_totsplit_99perc/VIP_outputs"
dir.create(output_dir, showWarnings = FALSE)

# split into a named list
peaks_per_prop <- peaks %>%
  split(.$Property) %>%
  # drop the Property column now that it's encoded in the sheet name
  map(~ select(.x, -Property))

# write one .xlsx with one sheet per property

write_xlsx(
  peaks_per_prop,
  path = file.path(output_dir, "top_peaks_per_property.xlsx")
)



walk(names(plot_list), function(prop) {
  
    peak_tbl <- peak_list_10[[prop]]
  write.csv(
    peak_tbl,
    file = file.path(output_dir, paste0(prop, "_top_peaks.csv")),
    row.names = FALSE
  )
  
  # 2) save the corresponding VIP plot
  plt <- plot_list[[prop]]
  ggsave(
    filename = file.path(output_dir, paste0(prop, "_VIP.png")),
    plot     = plt,
    width    = 8,
    height   = 4,
    dpi      = 300
  )
})
```

### RF

```{r}

identifier <- initialize_workspace("RF_RMSEP_MIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "rf", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep",  property_range = c(56, 59, 72, 76))
```

### Viewing Feature Importance: RF

```{r}

# 0) pick a cutoff for RF importances
#    e.g. 75th percentile of all importances (you can also hardcode like 0.1)
rf_imps_all <- read_csv("Plots_WellFitted_Properties_RF_RMSEP_MIR_totsplit_99perc/RF_importances_RF_RMSEP_MIR_totsplit_99perc.csv")
cutoff_imp <- quantile(rf_imps_all$importance, 0.75, na.rm = TRUE)

# 1) filter & coerce
vip_sel_rf <- rf_imps_all %>% 
  filter(Property %in% c("H_tot_molar",
                         "H_tot_to_C_tot_molar",
                         "Oult_to_C_tot_molar",
                         "O_less_Al_Si_Fe")) %>%
  mutate(
    wavenumber = as.numeric(feature),
    importance = as.numeric(importance)
  ) %>%
  arrange(Property, wavenumber)

# 2) detect local peaks AND above cutoff_imp
peaks_rf <- vip_sel_rf %>%
  group_by(Property) %>%
  arrange(wavenumber) %>%
  mutate(
    prev_i = lag(importance),
    next_i = lead(importance)
  ) %>%
  filter(
    importance > prev_i,
    importance > next_i,
    importance > cutoff_imp
  ) %>%
  ungroup() %>%
  select(Property, wavenumber, importance)

# 3) take top 10 peaks per property by ΔRMSE
top10_peaks_rf <- peaks_rf %>%
  group_by(Property) %>%
  slice_max(importance, n = 10, with_ties = FALSE) %>%
  ungroup()

peak_list_10_rf <- split(top10_peaks_rf, top10_peaks_rf$Property)

# 4) build plots
plot_list_rf <- vip_sel_rf %>%
  split(.$Property) %>%
  map(~{
    dat <- .
    prop <- unique(dat$Property)
    tp   <- peak_list_10_rf[[prop]]
    ggplot(dat, aes(wavenumber, importance)) +
      geom_line(color = "#2C3E50", size = 0.4) +
      geom_hline(yintercept = cutoff_imp, linetype = "dashed", color = "grey50") +
      labs(
        #title = prop,
        x = expression("Wavenumber ("*cm^-1*")"),
        y = "ΔRMSE (permutation importance)"
      ) +
      theme_classic(base_size = 14) +
      theme(
        plot.title   = element_text(face = "bold", hjust = 0.5),
        panel.border = element_rect(color = "black", fill = NA, size = 1)
      )
  })

# 5) view one
plot_list_rf$H_tot_molar

# 6) optionally save all peaks > cutoff_imp to Excel, one sheet each
dir.create("Plots_WellFitted_Properties_RF_RMSEP_MIR_totsplit_99perc/VIP_outputs", showWarnings = FALSE)
peaks_per_prop <- peaks_rf %>% split(.$Property) %>% map(~select(.x, -Property))
write_xlsx(peaks_per_prop, 
           path = "Plots_WellFitted_Properties_RF_RMSEP_MIR_totsplit_99perc/VIP_outputs/RF_peaks_gt_cutoff.xlsx")

# 7) save the plots
iwalk(plot_list_rf, ~ ggsave(
  filename = paste0("Plots_WellFitted_Properties_RF_RMSEP_MIR_totsplit_99perc/VIP_outputs/", .y, "_rf_peaks.png"),
  plot     = .x,
  width    = 8, height = 4
))
```

```{r}

# 0) read in your RF importances
rf_imps_all <- read_csv("Plots_WellFitted_Properties_RF_RMSEP_MIR_totsplit_99perc/RF_importances_RF_RMSEP_MIR_totsplit_99perc.csv")

# 1) compute the 75th‐percentile cutoff **per** property
cutoffs <- rf_imps_all %>%
  group_by(Property) %>%
  summarize(cutoff_imp = quantile(importance, 0.95, na.rm = TRUE)) 

# 2) filter & coerce, then bring in the per‐property cutoff
vip_sel_rf <- rf_imps_all %>% 
  filter(Property %in% c("H_tot_molar","H_tot_to_C_tot_molar",
                         "Oult_to_C_tot_molar","O_less_Al_Si_Fe")) %>%
  mutate(
    wavenumber = as.numeric(feature),
    importance = as.numeric(importance)
  ) %>%
  left_join(cutoffs, by = "Property") %>%
  arrange(Property, wavenumber)

# 3) detect local peaks AND above the **property** cutoff_imp
peaks_rf <- vip_sel_rf %>%
  group_by(Property) %>%
  arrange(wavenumber) %>%
  mutate(
    prev_i = lag(importance),
    next_i = lead(importance)
  ) %>%
  filter(
    importance > prev_i,
    importance > next_i,
    importance > cutoff_imp
  ) %>%
  ungroup() %>%
  select(Property, wavenumber, importance, cutoff_imp)

# 4) pick the top 10 peaks per property
top10_peaks_rf <- peaks_rf %>%
  group_by(Property) %>%
  slice_max(importance, n = 10, with_ties = FALSE) %>%
  ungroup()

peak_list_10_rf <- split(top10_peaks_rf, top10_peaks_rf$Property)

# 5) build one plot per property, using its own cutoff_imp
plot_list_rf <- vip_sel_rf %>%
  split(.$Property) %>%
  map(~{
    dat <- .
    prop <- unique(dat$Property)
    cutoff <- unique(dat$cutoff_imp)
    tp   <- peak_list_10_rf[[prop]]
    ggplot(dat, aes(wavenumber, importance)) +
      geom_line(color = "#2C3E50", size = 0.4) +
      geom_hline(yintercept = cutoff, linetype = "dashed", color = "grey50") +
      labs(
        x = expression("Wavenumber ("*cm^-1*")"),
        y = "ΔRMSE (permutation importance)"
      ) +
      theme_classic(base_size = 14) +
      theme(
        plot.subtitle = element_text(face = "bold", hjust = 0.5),
        panel.border  = element_rect(color = "black", fill = NA, size = 1)
      )
  })

# 6) view a single plot
print(plot_list_rf$H_tot_molar)

# 7) save peaks > cutoff_imp into an Excel file, one sheet per property
dir.create("Plots_WellFitted_Properties_RF_RMSEP_MIR_totsplit_99perc/VIP_outputs",
           showWarnings = FALSE)
peaks_per_prop <- peaks_rf %>%
  split(.$Property) %>%
  map(~ select(.x, -Property, -cutoff_imp))
write_xlsx(peaks_per_prop, 
           path = "Plots_WellFitted_Properties_RF_RMSEP_MIR_totsplit_99perc/VIP_outputs/RF_peaks_gt_cutoff_by_property.xlsx")

# 8) save each plot
iwalk(plot_list_rf, ~ ggsave(
  filename = paste0("Plots_WellFitted_Properties_RF_RMSEP_MIR_totsplit_99perc/VIP_outputs/", .y, "_rf_peaks.png"),
  plot     = .x,
  width    = 8, height = 4
))
```
