---
title: "Working Scripts"
output: html_document
date: "2025-02-04"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Current Workflow

## Load Libraries

```{r, include=FALSE}

# Define a list of required packages
required_packages <- c("writexl","randomForest", "caret", "pls", "data.table", "ithir", 
                       "dplyr", "tidyr", "prospectr", "globals", "stringr", 
                       "ggplot2", "here", "tidymodels", "parsnip", "plsmod", 
                       "mixOmics", "yardstick", "purrr", "tibble","ranger","VIM","shiny","FNN")

#VIM for KNN imputation
# Load all required packages quietly without outputting any list of packages
suppressPackageStartupMessages(
  invisible(lapply(required_packages, library, character.only = TRUE))
)
```

## Load reference data

```{r}

# treated spectra
spec_trt <- readRDS("spec_trt_MIR.RDS")
spec_trt_MIR_lab <- readRDS("spec_trt_MIR_lab.RDS")

spec_trt_NIR <- readRDS("/Users/soliverchefusi/Library/CloudStorage/OneDrive-Personal/R/ICRAF&Lehmann/ICRAF_IR/ICRAF_IR_R_Project/ICRAF_IR/CEBUM/CEBUM_NIR/CEBUM_NIR/spec_trt_NIR.RDS")
spec_trt_NIR_lab <- readRDS("/Users/soliverchefusi/Library/CloudStorage/OneDrive-Personal/R/ICRAF&Lehmann/ICRAF_IR/ICRAF_IR_R_Project/ICRAF_IR/CEBUM/CEBUM_NIR/CEBUM_NIR/spec_trt_NIR_lab.RDS")

spec_trt <- spec_trt[,-2] #removing the data for the first wavenumber since it's all NAN's
spec_trt_MIR_lab <- spec_trt_MIR_lab[,-2]

spec_trt_MIR_NIR <- merge(spec_trt,spec_trt_NIR, by ="SSN")
spec_trt_MIR_NIR_lab <- merge(spec_trt_MIR_lab,spec_trt_NIR_lab, by ="SSN")
rownames(spec_trt_MIR_NIR_lab) <- spec_trt_MIR_NIR_lab$SSN

# biochar property data
df1<-readRDS("df1.RDS")

# df.f: full dataset;
# spec_trt: treated spectra

#Available properties to predict
slprptr<-names(df1[-c(2:9)]) #FUSI EDIT: removing metadata columns 

# defining identity object with names of biochar samples
pred<-as.data.frame(spec_trt[,1])
colnames(pred)<-"SSN"
 
```

## PLSR ( & RF) Base model

1.  definine a 'base' model based on minimizing RMSEP

High-Level Structure:

```         
1.  Preprocessing

2.  Setting up models (PLS and Random Forest)

3.  Running models (map() over models to fit them)

4.  Calculating metrics (for each model)

5.  Plotting results (for each model)

6.  Comparing the models (optional step for comparing PLS vs Random Forest)
```

```{r}

# Initialize the workspace by creating directories for storing outputs and 
# setting up an empty tibble to accumulate performance metrics 
initialize_workspace <- function(identifier) {
  #dir.create(paste0("Plots_Validation_MIR_", identifier), showWarnings = FALSE)
  dir.create(paste0("Plots_WellFitted_Properties_", identifier), showWarnings = FALSE)
  all_metrics <<- tibble()  # Global tibble to store metrics for each property
  
  return(identifier)  # Pass the identifier for later use
}

# slprptr: list of properties; .x: index of property in slprptr
select_property_data <- function(slprptr, .x, df1, spec_trt) {
  # Filter df1 to create df.f based on SSN present in spec_trt
    df.f <- df1 %>%
    filter(SSN %in% spec_trt$SSN) # Select only samples present in both predictor and response datasets
  # Check if the property exists in df1
  if (!slprptr[.x] %in% colnames(df.f)) {
    stop(paste("Property", slprptr[.x], "not found in df1"))
  }
  
  # Columns to select, ensuring required columns are present
  cols_to_select <- c("SSN", "Source", slprptr[.x])
  cols_to_select <- intersect(cols_to_select, colnames(df.f))
  
  if (!all(cols_to_select %in% colnames(df.f))) {
    stop(paste("Columns", paste(cols_to_select, collapse = ", "), "not found in df.f"))
  }
  
  # Filter and preprocess data, including removing outliers for the selected property

    
    df.sel <- df.f %>%
    dplyr::select(dplyr::all_of(cols_to_select)) %>%
    inner_join(spec_trt, by = "SSN") %>%   # Combine predictor and response variables into one df
    na.omit() %>%
    #filter(data.table::between(.[[3]], quantile(.[[3]], 0.01), quantile(.[[3]], 0.99)))
    filter(data.table::between(!!sym(slprptr[.x]), quantile(!!sym(slprptr[.x]), 0.01), quantile(!!sym(slprptr[.x]), 0.99))) # Filter out extremes
  
  # Set SSN as rownames but keep the column
  rownames(df.sel) <- df.sel$SSN
  # Check if df.sel has rows
  if (nrow(df.sel) == 0) {
    stop(paste("No matching rows in df.sel for property:", slprptr[.x]))
  }
  
  return(list(df.f = df.f, df.sel = df.sel))
}

# Split data into calibration (training) and validation (test) sets
# Allows splitting based on a specific Source if source_name is specified
split_data <- function(df.sel, use_source = FALSE, source_name = NULL) {
  # If a specific source_name is provided, filter data for that source
  
  if (!is.null(source_name)) {
    df.sel <- df.sel %>% filter(Source == source_name)
  }

  if (use_source) {
    set.seed(123) 
    # Split ensuring Source is represented in both subsets
    trainIndex <- createDataPartition(df.sel$Source, p = 0.7, list = FALSE)
  } else {
    set.seed(123) 
    # Split without considering Source
    trainIndex <- sample(seq_len(nrow(df.sel)), size = 0.7 * nrow(df.sel))
  }

  # Remove Source column
    df.sel <- df.sel %>% dplyr::select(-Source)
  
  cal_ids <- rownames(df.sel)[trainIndex]
  val_ids <- rownames(df.sel)[-trainIndex]
    
    # Filter and assign rownames directly
  cal_df <- df.sel %>%
    filter(rownames(.) %in% cal_ids) %>%
    arrange(rownames(.)) 
  
  val_df <- df.sel %>%
    filter(rownames(.) %in% val_ids) %>%
    arrange(rownames(.))

  # Save objects in the global environment
assign("cal_df_global", cal_df, envir = .GlobalEnv)
assign("val_df_global", val_df, envir = .GlobalEnv)

  return(list(cal_df = cal_df, val_df = val_df))
}

prepare_model_data <- function(cal_df, val_df, slprptr, .x) {
  if (nrow(cal_df) <= 10 || nrow(val_df) <= 10) {
    warning("Insufficient rows in cal_df or val_df. Returning NA placeholders.")
    train_data <- cal_df
    test_data <- val_df
    train_data[, ] <- NA
    test_data[, ] <- NA
  } else {
    train_data <- cal_df
    test_data <- val_df
    rownames(train_data) <- train_data$SSN  # Set SSN as rownames but keep the column
    rownames(test_data) <- test_data$SSN
  }
  
  # Save rownames of training and test data for debugging
  debug_rows <- tibble(
    Property = slprptr[.x],
    Dataset = c(rep("Training", nrow(train_data)), rep("Testing", nrow(test_data))),
    SSN = c(rownames(train_data), rownames(test_data))
  )
  
  # Save debugging dataframe globally to inspect later
  if (!exists("debug_row_summary")) {
    assign("debug_row_summary", debug_rows, envir = .GlobalEnv)
  } else {
    debug_row_summary <<- bind_rows(debug_row_summary, debug_rows)
  }
  
  return(list(train_data = train_data, test_data = test_data))
}

# Create a recipe for data preprocessing, with optional custom steps
preprocess_data <- function(train_data, test_data, slprptr, p, recipe_steps = NULL,include_zv = FALSE) {
  # Check if train/test data is all NA
  if (all(is.na(train_data)) || all(is.na(test_data))) {
    warning("Preprocessing skipped due to all NA in train/test data.")
    return(list(
      recipe = NULL,
      train_data = data.frame(),  # Return an empty data frame as a placeholder
      test_data = data.frame()
    ))
  }
  
  formula <- as.formula(paste(slprptr[p], "~ ."))  # Define formula for model
  
  # Define base recipe with ID role (column) for SSN
  recipe_base <- recipe(formula, data = train_data) %>%
    update_role(SSN, new_role = "ID")  # Designate SSN column as an identifier
  
    # Conditionally add step_zv() only if include_zv is TRUE
  if (include_zv) {
    recipe_base <- recipe_base %>% step_zv()%>%
step_nzv()
  }
  
  # Apply additional preprocessing steps as needed
  if (!is.null(recipe_steps)) {
    for (step in recipe_steps) {
      recipe_base <- recipe_base %>% step
    }
  }
  
  return(list(recipe = recipe_base, train_data = train_data, test_data = test_data))
}

# Set up PLS or Random Forest model specification, including tuning parameters
setup_model <- function(model_type, train_data = NULL, response_var = NULL) {
  if (model_type == "pls") {
    # Configure a PLS model specification
    model_spec <- parsnip::pls() %>%
      set_mode("regression") %>%
      set_engine("mixOmics") %>%
      set_args(num_comp = tune())
    mtry_range <- NULL
  } else if (model_type == "rf") {
    # Configure a Random Forest model with tuning for mtry and min_n
    if (is.null(train_data)) stop("train_data must be provided for Random Forest.")
    if (is.null(response_var)) stop("response_var must be specified for Random Forest.")
    
    # Remove SSN and response columns to get only predictor columns
    predictors_only <- train_data %>%
      dplyr::select(-SSN, -all_of(response_var))
        # Check if predictors are sufficient
    if (ncol(predictors_only) < 2) stop("Random Forest requires at least 2 predictor variables.")
    
    
    # Finalize mtry_range
    # mtry_range <- dials::finalize(dials::mtry(), predictors_only) # predictors_only is used to set the max value for the mtry range - so here mtry ranges up to the total number of predictors (e.g. number of wavenumbers).
    mtry_range <- dials::mtry(range = c(2, round(sqrt(ncol(predictors_only)))))
    
    if (is.null(mtry_range)) stop("Failed to initialize mtry_range in setup_model(). Check predictor columns in train_data.")
    
    model_spec <- parsnip::rand_forest(mtry = tune(), min_n = tune()) %>%
      set_mode("regression") %>%
      set_engine("ranger")
  }

  return(list(model_spec = model_spec, mtry_range = mtry_range))
}
#***** 

# Function to fit both models (PLS and Random Forest) for comparison
fit_both_models <- function(train_data, test_data, recipe) {
  model_list <- list(
    pls = setup_model("pls"),
    rf = setup_model("rf", train_data = train_data)
  )
  model_types <- names(model_list)
  
  # Fit each model type and return results
  results <- purrr::map2(model_list, model_types, ~ fit_model(.x, train_data, test_data, recipe, .y))
  return(results)
}


calculate_metrics <- function(data, predictions, data_type, slprptr, .x, best_params, model_type, test_data) {
  response_var <- slprptr[.x]  # Define response variable

  # Check if response variable is present and .pred column exists in predictions
  if (!response_var %in% colnames(data) || !".pred" %in% colnames(predictions)) {
    message(paste("Skipping metrics calculation due to missing columns."))
    return(tibble(Property = response_var, Data_Type = data_type, Model = model_type, R2 = NA, RMSE = NA))
  }

  # Safe computation function with error handling for each metric
  safe_compute <- function(expression) {
    tryCatch(eval(expression), error = function(e) NA)
  }

  # Calculate performance metrics
  r2_value <- safe_compute(rsq_vec(truth = data[[response_var]], estimate = predictions$.pred))
  rmse_value <- safe_compute(rmse_vec(truth = data[[response_var]], estimate = predictions$.pred))
  bias_value <- safe_compute(mean(predictions$.pred - data[[response_var]], na.rm = TRUE) / mean(data[[response_var]], na.rm = TRUE))

  # Calculate the range of the response variable from the test_data and RMSEP/range
  response_range <- safe_compute(max(test_data[[response_var]], na.rm = TRUE) - min(test_data[[response_var]], na.rm = TRUE))
  rmse_range_ratio <- if (!is.na(response_range) && response_range > 0) rmse_value / (response_range / 10) else NA

  metrics_tibble <- tibble(
    Property = response_var,
    Data_Type = data_type,
    Model = model_type,
    Comps = if (model_type == "pls") best_params$num_comp else NA,
    mtry = if (model_type == "rf") best_params$mtry else NA,
    min_n = if (model_type == "rf") best_params$min_n else NA,
    N = nrow(data),
    R2 = r2_value,
    RMSE = rmse_value,
    RMSEP_range_ratio = rmse_range_ratio,
    bias = bias_value
  )

  return(metrics_tibble)
}

# Define properties for core and related metrics
get_property_groups <- function() {
  core_properties <- c(
    "Temp.", "pH", "EC_uS", "Ash_avg", "exchg_Ca_mmol_kg", 
    "exchg_K_mmol_kg", "exchg_Mg_mmol_kg", "exchg_Na_mmol_kg", "exchg_P_mmol_kg", 
    "Ca_mg_kg", "K_mg_kg", "Mg_mg_kg", "C_tot_avg", "N_tot_avg",
    "C_org_wt", "H_tot_wt", "O_ult", "P_avg_mg_kg", "Fe_avg_mg_kg", "Zn_avg_mg_kg", 
    "Cu_avg_mg_kg", "Ni_avg_mg_kg", "As_avg_mg_kg", "Cd_avg_mg_kg"
  )
  
  related_properties <- c(
    "Naphthalin_mg_kg", "X2_Methylnaphthalin", "X1_Methylnaphthalin", "Sum_Naphthaline", 
    "Acenaphthylen", "Acenaphthen", "Fluoren", "Phenanthren", "Anthracen", 
    "Fluoranthen", "Pyren", "Chrysen", "Benzo_a_anthracen", 
    "Benzo_b_plus_k_fluoranthen", "Benzo_a_pyren", "Dibenzo_a_h_anthracen", 
    "Indeno_c_d_pyren", "Benzo_g_h_i_perylen", "Sum_PAH_defined_EPA_mg_kg", 
    "Bulk.Density_mg_m3", "C_tot_to_N_wt", "O_less_Al_Si_Fe_to_C", "O_less_Al_Si_Fe_Ca_to_C","O_less_Al_Si_Fe_C_inorg_to_C", "O_less_C_inorg_to_C", "C_org_to_N_wt", 
    "C_tot_to_N_molar", "Htot_to_C_tot_molar", "Oult_to_C_tot_molar", 
    "Htot_to_C_org_molar", "Oult_to_C_org_molar", "O_less_Al_Si_Fe_to_C_org_molar", 
    "O_less_Al_Si_Fe_Ca_to_C_org_molar", "O_less_Al_Si_Fe_C_inorg__to_C_org_molar", 
    "O_less_C_inorg_to_C_org_molar"
  )
  
  return(list(core_properties = core_properties, related_properties = related_properties))
}

filter_metrics_by_group <- function(all_metrics, property_groups) {
  core_metrics <- all_metrics %>% filter(Property %in% property_groups$core_properties)
  related_metrics <- all_metrics %>% filter(Property %in% property_groups$related_properties)
  return(list(core_metrics = core_metrics, related_metrics = related_metrics))
}


# Function to plot measured vs. predicted values and save in the default directory
plot_predictions <- function(val_plot_data, val_df, slprptr, .x, combined_metrics, model_type, identifier) {
  # Check if the property exists in the validation plot data
  if (!(slprptr[.x] %in% colnames(val_plot_data))) return(NULL)

  # Extract metrics for validation
  val_metrics <- combined_metrics %>% filter(Data_Type == "Validation", Model == model_type)
  
  # Add label text with components and R²
  label_text <- paste("Comps:", val_metrics$Comps, "R²:", round(val_metrics$R2, 3))

  # Generate the plot
  # validation_plot <- ggplot(val_plot_data, aes_string(x = slprptr[.x], y = ".pred")) +
  #   geom_point(color = "blue", size = 2) +
  #   geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  #   labs(
  #     title = paste("Measured vs Predicted for", slprptr[.x], "-", model_type),
  #     x = paste("Measured", slprptr[.x]),
  #     y = "Predicted"
  #   ) +
  #   theme_minimal() +
  #   annotate("text", x = Inf, y = -Inf, hjust = 1.1, vjust = -0.5, label = label_text, size = 4)

  # Save the plot
  # ggsave(
  #   filename = paste0("Plots_Validation_PLSR_MIR_", identifier, "/", slprptr[.x], "_", model_type, ".png"),
  #   plot = validation_plot, width = 7, height = 7, dpi = 300
  #)
}

# Plot R² values and differences for well-fitted properties based on model criteria
plot_well_fitted_r2 <- function(all_metrics, identifier) {
  all_r2_values <- all_metrics %>%
    group_by(Property) %>%
    reframe(
      R2_cal = R2[Data_Type == "Calibration"],
      R2_val = R2[Data_Type == "Validation"],
      R2_diff = abs((R2_cal - R2_val) / R2_cal)
    )
  
  well_fitted_properties <- all_r2_values %>%
    filter(R2_diff <= 0.15, R2_cal > 0.6, R2_val > 0.6) %>%
    pull(Property)
  
  plot_data <- all_metrics %>%
    filter(Property %in% well_fitted_properties, Data_Type %in% c("Calibration", "Validation")) %>%
    dplyr::select(Property, Data_Type, R2, N, RMSE, bias,RMSEP_range_ratio) %>%
    dplyr::rename(Value = R2)
  
  well_fitted_plot <- ggplot(plot_data, aes(x = Property, y = Value, fill = Data_Type)) +
    geom_bar(stat = "identity", position = "dodge") +
    scale_fill_manual(values = c("Calibration" = "purple", "Validation" = "darkgreen")) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1, size = 8),
      panel.grid = element_blank(),
      panel.border = element_rect(color = "black", fill = NA)
    ) +
    labs(title = "R² Comparison for Well-Fitted Properties",
         subtitle = "Purple: Calibration | Green: Validation",
         y = "R² Value",
         x = "Property") +
    geom_text(
      aes(label = ifelse(Data_Type == "Calibration", paste0("N:", N),
                         paste0("N:", N, "\nRMSE%:\n", round(RMSEP_range_ratio, 2), "\nRel_Bias:\n", round(bias, 2)))),
      position = position_dodge(width = 0.9), 
      vjust = -0.5, size = 3
    ) +
    ylim(0, 1.35)  # Set y-axis limit to allow space for annotations
  
  print(well_fitted_plot)
  
  ggsave(filename = paste0("Plots_WellFitted_Properties_", identifier, "/well_fitted_properties_plot.png"), 
         plot = well_fitted_plot, width = 10, height = 7, dpi = 300)
}


# Wrapper function to run analysis for each property in slprptr with specified model_type (no default)
run_all_properties <- function(slprptr, df1, spec_trt, model_type, identifier, 
                                use_source = FALSE, source_name = NULL, tuning_goal = "rmsep",
                                property_range = 2:length(slprptr),  include_zv = FALSE) {
  #all_metrics <<- tibble()  # Initialize global metrics storage
  model_results <- list()  # Store model fits for later use
  output_dir <- paste0("Plots_WellFitted_Properties_", identifier)  # Use existing directory

  
  # Validate the property_range parameter
  if (any(property_range > length(slprptr) | property_range < 1)) {
    stop("property_range contains indices outside the valid range of slprptr.")
  }
  
  # Loop through each property in the specified range and collect metrics
  all_metrics <- purrr::map_dfr(property_range, function(.x) {
    message("Processing property index: ", .x, " - Property: ", slprptr[.x])
    # Select and filter the property data
    property_data <- select_property_data(slprptr, .x, df1, spec_trt)
    df.f <- property_data$df.f
    df.sel <- property_data$df.sel
    
    # Split the data into calibration (training) and validation (test) sets
    split <- split_data(df.sel, use_source = use_source, source_name = source_name)
    cal_df <- split$cal_df
    val_df <- split$val_df
    
    # Skip if the calibration or validation datasets have insufficient rows
    if (nrow(cal_df) <= 10 || nrow(val_df) <= 10) {
      message(paste("Skipping property:", slprptr[.x], "due to insufficient data"))
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
    # Prepare train/test data and preprocess (e.g., removing outliers)
    data_prep <- prepare_model_data(cal_df, val_df, slprptr, .x)  # Converts calibration and validation datasets to train/test format
    recipe_data <- preprocess_data(data_prep$train_data, data_prep$test_data, slprptr, .x)  # Handles preprocessing steps
    
    # Ensure train/test data is not empty after preprocessing
    if (nrow(recipe_data$train_data) == 0 || nrow(recipe_data$test_data) == 0) {
      message(paste("Skipping property:", slprptr[.x], "due to empty train/test data after preprocessing."))
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
    # Step 3: Skip properties with insufficient train/test data
    if (nrow(data_prep$train_data) <= 10 || nrow(data_prep$test_data) <= 10) {
      message(paste("Skipping property:", slprptr[.x], "due to insufficient train/test data"))
      return(tibble(Property = slprptr[.x], Data_Type = "Skipped", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
    }
    
    # Set up the model specification (PLS or RF) based on model_type
    if (model_type == "pls") {
      model_spec_list <- setup_model(model_type, train_data = data_prep$train_data)  # Set up PLS model
    } else if (model_type == "rf") {
      model_spec_list <- setup_model(model_type, train_data = data_prep$train_data, response_var = slprptr[.x])  # Set up RF model
    }
    
    # Fit the model using the specified tuning goal (e.g., RMSEP) and handle errors gracefully
    model_fit <- tryCatch(
      {
        fit_model(
          model_spec_list, 
          train_data = recipe_data$train_data, 
          test_data = recipe_data$test_data, 
          recipe = recipe_data$recipe, 
          model_type = model_type, 
          tuning_goal = tuning_goal,
          property_name = slprptr[.x]
        )
      },
      error = function(e) {
        # Log detailed debugging info
        warning(paste(
          "Error with property:", slprptr[.x], "\n",
          "Training data dimensions:", dim(recipe_data$train_data), "\n",
          "Test data dimensions:", dim(recipe_data$test_data), "\n",
          "Error message:", e$message
        ))
        return(tibble(Property = slprptr[.x], Data_Type = "Error", Model = NA, R2 = NA, RMSE = NA, bias = NA, ncomp = NA))
      }
    )
    
    # Handle skipped properties due to model fitting issues
    if ("Data_Type" %in% colnames(model_fit) && model_fit$Data_Type[1] == "Skipped") {
      return(model_fit)
    }
    
    # Generate predictions for calibration and validation datasets
    cal_predictions <- predict(model_fit$model, new_data = recipe_data$train_data)
    val_predictions <- predict(model_fit$model, new_data = recipe_data$test_data)
    
    # Calculate performance metrics (R², RMSE, Bias, etc.) for both datasets
    cal_metrics <- calculate_metrics(
      data = recipe_data$train_data, 
      predictions = cal_predictions, 
      data_type = "Calibration", 
      slprptr = slprptr, 
      .x = .x, 
      best_params = model_fit$best_params, 
      model_type = model_type, 
      test_data = recipe_data$test_data
    )
    val_metrics <- calculate_metrics(
      data = recipe_data$test_data, 
      predictions = val_predictions, 
      data_type = "Validation", 
      slprptr = slprptr, 
      .x = .x, 
      best_params = model_fit$best_params, 
      model_type = model_type, 
      test_data = recipe_data$test_data
    )
    
    # Combine metrics for calibration and validation
    combined_metrics <- bind_rows(cal_metrics, val_metrics)
    
    # Plot predictions (commented out for now; enable as needed)
    # plot_predictions(
    #   val_plot_data = recipe_data$test_data %>% dplyr::select(slprptr[.x]) %>% bind_cols(val_predictions),
    #   val_df = val_df,
    #   slprptr = slprptr,
    #   p = .x,
    #   combined_metrics = combined_metrics,
    #   model_type = model_type,
    #   identifier = identifier
    # )

    return(combined_metrics)
  })
  
  # Retrieve property groups for filtering metrics
  property_groups <- get_property_groups()
  
  # Filter metrics into core and related property groups
  filtered_metrics <- filter_metrics_by_group(all_metrics, property_groups)
  
  # Save filtered metrics as separate tabs in an Excel file
  output_data <- list(
    "All Metrics" = all_metrics,
    "Core Properties" = filtered_metrics$core_metrics,
    "Related Properties" = filtered_metrics$related_metrics
  )
  output_file <- paste0("Plots_WellFitted_Properties_", identifier, "/all_metrics_", identifier, ".xlsx")
  writexl::write_xlsx(output_data, path = output_file)
  

  # Plot well-fitted R² results
  plot_well_fitted_r2(all_metrics, identifier)
}
```

## Base model 1

```{r}

# Function to calculate cumulative explained variance
calculate_explained_variance <- function(train_data, response_var, max_comps) {
  # Prepare X and Y matrices
  X <- train_data %>%
    dplyr::select(-c(SSN, all_of(response_var))) %>%
    dplyr::mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  Y <- train_data %>%
    dplyr::select(all_of(response_var)) %>%
    dplyr::mutate(across(everything(), as.numeric)) %>%
    as.matrix()
  
  # Explained variance calculation
  variance_list <- purrr::map_dbl(1:max_comps, function(num_comp) {
    model <- tryCatch(
      mixOmics::pls(X = X, Y = Y, ncomp = num_comp),
      error = function(e) {
        message("Error in fitting PLS model: ", e)
        return(NULL)
      }
    )
    
    if (is.null(model)) {
      return(0)
    }
    
    # Calculate cumulative explained variance for X up to num_comp
    cumulative_variance_X <- sum(model$prop_expl_var$X[1:num_comp])
    cumulative_variance_X
  })
  
  return(variance_list)
}
```

```{r}

# Main fit_model function
fit_model <- function(model_spec_list, train_data, test_data, recipe, model_type,
                      tuning_goal = "rmsep", cumulative_variance_threshold = 0.85,property_name = NULL) {

  model_spec <- model_spec_list$model_spec
  mtry_range <- model_spec_list$mtry_range  # Only for Random Forest

  # Set up workflow with recipe and model
  biochar_workflow <- workflow() %>%
    add_recipe(recipe) %>%
    add_model(model_spec)



  if (model_type == "pls") {
    
      # Define cross-validation folds (LOOCV for <= 10 samples)
  set.seed(123)  # Ensure reproducibility
  cv_folds <- if (nrow(train_data) <= 10) {
    vfold_cv(train_data, v = nrow(train_data))  # Leave-One-Out CV
  } else {
    vfold_cv(train_data, v = 10)  # 10-fold CV
  }
    
    max_comps <- min(25, nrow(test_data) - 1)  # Limit max components

    if (tuning_goal == "rmsep") {
      # Perform 10-fold (or LOOCV) to minimize RMSEP
      num_comp_grid <- tibble(num_comp = 1:max_comps)
      tune_results <- tune_grid(
        biochar_workflow,
        resamples = cv_folds,
        grid = num_comp_grid,
        metrics = metric_set(rmse),
        control = control_grid(save_pred = TRUE)
      )

      best_params <- tune_results %>% select_best(metric = "rmse") %>% dplyr::select(num_comp)
      final_workflow <- finalize_workflow(biochar_workflow, best_params)
      
      #*Only Generate RMSEP Plot for PLS Models**
      if (!is.null(property_name)) {
        rmsep_data <- tune_results %>%
          collect_metrics() %>%
          filter(.metric == "rmse") %>%
          dplyr::select(num_comp, mean)

        rmsep_plot <- ggplot(rmsep_data, aes(x = num_comp, y = mean)) +
          geom_line() +
          geom_point() +
          theme_minimal() +
          labs(
            title = paste("PLS: Comp vs RMSEP for", property_name),
            x = "Number of Components",
            y = "RMSEP"
          )

        print(rmsep_plot)
      }  # Display the plot in R

    } else if (tuning_goal == "variance") {
      
        # Define cross-validation folds (LOOCV for <= 10 samples)
      set.seed(123)  # Ensure reproducibility
      cv_folds <- if (nrow(train_data) <= 10) {
        vfold_cv(train_data, v = nrow(train_data))  # Leave-One-Out CV
      } else {
        vfold_cv(train_data, v = 10)  # 10-fold CV
      }
      
      # Calculate cumulative explained variance
      response_var <- colnames(train_data)[[2]]  # Adjust index if response var is at a different position
      explained_variance <- calculate_explained_variance(train_data, response_var, max_comps)

      # Find the minimum number of components meeting the variance threshold
      num_comp <- which(explained_variance >= cumulative_variance_threshold)[1]
      if (is.na(num_comp) || is.null(num_comp)) {
        message(paste("Skipping property:", response_var, "due to insufficient cumulative variance."))
        return(tibble(
          Property = response_var,
          Data_Type = "Skipped",
          Model = model_type,
          N = NA,
          R2 = NA,
          RMSE = NA,
          bias = NA,
          ncomp = NA
        ))
      }

      # Validate the selected number of components with cross-validation
      num_comp_grid <- tibble(num_comp = num_comp)
      tune_results <- tune_grid(
        biochar_workflow,
        resamples = cv_folds,
        grid = num_comp_grid,
        metrics = metric_set(rmse),
        control = control_grid(save_pred = TRUE)
      )

      best_params <- tibble(num_comp = num_comp)
      final_workflow <- finalize_workflow(biochar_workflow, best_params)
    }
  } else if (model_type == "rf") {
  set.seed(123)
  n_rows <- nrow(train_data)
  
  # Dynamically adjust folds
  cv_folds <- if (n_rows <= 20) {
    vfold_cv(train_data, v = min(floor(n_rows / 2), 5))  # Cap folds for small datasets
  } else {
    vfold_cv(train_data, v = min(10, n_rows - 1))  # Default for larger datasets
  }
  
  # Log fold details
  #message("Number of folds: ", length(cv_folds$splits))
  test_sizes <- purrr::map_int(cv_folds$splits, ~ nrow(assessment(.x)))
  train_sizes <- purrr::map_int(cv_folds$splits, ~ nrow(analysis(.x)))
  #message("Test set sizes: ", paste(test_sizes, collapse = ", "))
  #message("Train set sizes: ", paste(train_sizes, collapse = ", "))
  
  # Fallback for very small datasets
  if (n_rows < 20 || min(test_sizes) < 5) {
    warning("Dataset too small for cross-validation. Using fixed train-test split.")
    
    split <- initial_split(train_data, prop = 0.8)
    inner_train <- training(split)
    inner_test <- testing(split)
    
    if (nrow(inner_train) < 5 || nrow(inner_test) < 2) {
      stop("Insufficient data for fixed split. Skipping property.")
    }
    
    rf_model <- rand_forest(
      mtry = floor(sqrt(ncol(inner_train) - 1)),
      min_n = 2
    ) %>%
      set_engine("ranger") %>%
      set_mode("regression")
    
    final_workflow <- workflow() %>%
      add_recipe(recipe) %>%
      add_model(rf_model)
    
    model_fit <- fit(final_workflow, data = inner_train)
    predictions <- predict(model_fit, inner_test)
    
    return(list(model = model_fit, predictions = predictions))
  }
  
  # Dynamically adjust grid size
  rf_grid <- grid_random(
    mtry_range,
    min_n(range = c(1, min(5, floor(n_rows / 3)))),  # Smaller range for min_n
    size = min(n_rows / length(cv_folds$splits), 5)  # Adjust grid size dynamically
  )
  
  # Log grid size
  #message("Grid size: ", nrow(rf_grid))
  
  # Tune the model
  tune_results <- tune_grid(
    biochar_workflow,
    resamples = cv_folds,
    grid = rf_grid,
    metrics = metric_set(rmse),
    control = control_grid(save_pred = TRUE)  # Add verbose output
  )
  
  best_params <- tune_results %>% select_best(metric = "rmse")
  final_workflow <- finalize_workflow(biochar_workflow, best_params)
}
  
  
  # Fit the final model
  model_fit <- fit(final_workflow, data = train_data)

  # Predictions for train and test data
  train_predictions <- tryCatch({
    predict(model_fit, new_data = train_data) %>%
    as_tibble()  # Convert predictions to a tibble
  }, error = function(e) {
    message("Error in train_predictions: ", e)
    return(NULL)
  })

  # Add rownames from `test_data` to `test_predictions`
  if (!is.null(train_predictions)) {
    rownames(train_predictions) <- rownames(train_data)
  }

  test_predictions <- tryCatch({
    predict(model_fit, new_data = test_data) %>%
    as_tibble()  # Convert predictions to a tibble
  }, error = function(e) {
    message("Error in test_predictions: ", e)
    return(NULL)
  })

  # Add rownames from `test_data` to `test_predictions`
  if (!is.null(test_predictions)) {
    rownames(test_predictions) <- rownames(test_data)
  }

  # Ensure predictions are non-null
  if (is.null(train_predictions) || is.null(test_predictions)) {
    stop("Prediction failed: check data dimensions or NA values in predictor matrix.")
  }
  
  return(list(
    model = model_fit,
    train_predictions = train_predictions,
    test_predictions = test_predictions,
    best_params = best_params
  ))
}
```

#### 

------------------------------------------------------------------------

### Base model 2:

#### Tune to capture explained variance - MIR

-   Selecting the number of components to capture 90% and 95% of the explained variance in the predictor and response variables respectively.
-   Note: ' prop_expl_var\$X and prop_expl_var\$Y represent the proportion of explained covariance for each component in the predictor (X) and response (Y) matrices.' - same could be achieved using

To Do: Need to verify what prop_expl_var is actually calculating - unclear if it's variance.

Total split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_Variance_MIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "variance")
```

Source-based split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_Variance_MIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "variance")
```

Cornell only Predictions (focused on pH)

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_Variance_MIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt, model_type = "pls", identifier, 
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "variance")
```

#### Tune to capture explained variance - NIR

Total split - NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_Variance_NIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "variance")
```

Source-based split - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_Variance_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "variance")
```

Cornell only Predictions - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_Variance_NIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "variance")
```

#### Tune to capture explained variance - MIR + NIR

Total split - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_Variance_MIR_NIR_totsplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "variance")
```

Source-based split - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_Variance_MIR_NIR_sourcesplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "variance")
```

Cornell only Predictions - MIR + NIR

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_Variance_MIR_NIR_CornellOnlysplit_99perc")  # For RMSEP
run_all_properties(slprptr, df1, spec_trt_MIR_NIR, model_type = "pls", identifier, 
                   use_source = FALSE, source_name = "Cornell ", tuning_goal = "variance")
```

### Imputations

#### KNN - using total split

```{r}


# Perform KNN imputation
df1_KNN5_impt <- kNN(df1, k = 5,
                      #weights = "auto")#, #Control how variables contribute to the distance calculation. Use weights = "auto" to let the function automatically calculate weights based on variable importance (via random forest
                      useImputedDist = FALSE, #Control whether imputed values are used for calculating distances during the imputation of other variables.
                      #FALSE to avoid dependencies between variables.
                      #weightDist = TRUE)#, #TRUE to give closer neighbors more weight.
                      methodStand = "iqr", #) #Controls the standardization method for numeric variables in the Gower distance.
                      addRF = TRUE
                      )  
```

Impute the entire dataset

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_99perc_KNN5")  # For RMSEP
run_all_properties(slprptr, df1_KNN5_impt, spec_trt, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep")
```

#### Davis dataset

Matching the feedstock ID between 2 datasets.

```{r}

# Load datasets
user_data <- readRDS("df.f.RDS")
davis_data <- read.csv("davis_data.csv")
 
# Apply filtering
entries_to_remove <- c(
  "Enders et al., Bioresource Technology 114 (2012) 644-653.  ",
  "Enders et al., Bioresource Technology 114 (2012) 644-653. ",
  "Enders et al., Bioresource Technology 114 (2012) 644-653."
)

davis_data <- davis_data %>%
  filter(!Reference %in% entries_to_remove)

# Rename columns for consistency
names(davis_data) <- str_replace_all(names(davis_data), "\\.+", "_")
names(user_data) <- str_replace_all(names(user_data), "\\.+", "_")


# Function to simplify feedstock strings

simplify_feedstock <- function(feedstock) {
  feedstock <- str_to_lower(feedstock) %>%
    str_replace_all("sugar cane", "sugarcane") %>%  # Replace specific terms
    str_replace_all("hard wood", "hardwood") %>% 
    str_replace_all("soft wood", "softwood") %>% 
    str_remove_all("\\(.*?\\)|[0-9]+|°c|\\bshells?\\b|\\bchar\\b|\\bwith\\b|\\bw\\/\\b|\\bpaste\\b|\\+") %>%
    str_squish()  # Remove extra spaces
  return(feedstock)
}

# Simplify feedstock strings in user_data
user_data <- user_data %>%
  mutate(Simplified_Feedstock = simplify_feedstock(paste(Feedstock, Class, Char, sep = " ")))

# Simplify strings in the Davis dataset to broaden matching
davis_data <- davis_data %>%
  mutate(
    Simplified_Feedstock = str_to_lower(
      str_remove_all(
        paste(Feedstock_Composition, Biochar_Name, Optional_comments_about_Feedstock, sep = " "),
        "\\(.*?\\)|[0-9]+|°c"
      )
    ) %>%
    str_replace_all("sugar cane", "sugarcane") %>%  # Replace specific terms
    str_replace_all("hard wood", "hardwood") %>% 
    str_replace_all("soft wood", "softwood") %>%
    str_squish()  # Remove extra spaces
  )

primary_keywords <- c(
  # From shells/fruit/nuts
  "peanut", "pecan", "almond", "chestnut", "pistachio", "macadamia", "acorn", "hazelnut", "walnut", "nutshell",
  "apricot", "plum", "orange", "pineapple", "grapeseed", "pomace", "olive", "palm_kernel",

  # From wood/softwood/hardwood
  "softwood", "hardwood", "oak", "hickory", "eucalyptus", "poplar", "willow", "beech", "cedar", "pine", "spruce", 
  "fir", "bamboo", "mixed wood", "wood", "ash", "mesquite", "cottonwood",

  # From straw/agricultural/crop waste
  "corn", "wheat", "rice", "switchgrass", "miscanthus", "stover", "grass", "straw", "bagasse", "hemp", "rapeseed", 
  "cane", "sugarcane", "bamboo chips", "maize",

  # From manure/animal-based
  "manure", "cow", "poultry", "horse", "swine", "goat", "turkey", "dairy", "human", "sewage", "biosolids",

  # From sludge/industrial waste
  "sludge", "wastewater", "municipal sludge", "waste", "paper", "compost",

  # Miscellaneous
  "algae", "charcoal", "cocoa", "artichoke", "buckwheat", "spirulina", "guayule", "soybean", "coffee", "tea",
  "durian", "tomato", "waterweed", "shrimp", "bone"
)

# Secondary mapping
secondary_mapping <- list(
  # Nuts mapped to shells
  shells = c("peanut", "pecan", "almond", "chestnut", "pistachio", "nutshell", "macadamia", "acorn", "walnut", "hazelnut"),
  
  # Manure/Animal-Based
  manure = c("cow", "poultry", "horse", "swine", "goat", "turkey", "human", "dairy"),
  
  # Fruits
  fruit = c("apricot", "plum", "orange", "pineapple", "grapeseed", "pomace", "olive", "palm_kernel", "cocoa", "coffee"),
  
  # Straw/Crop Waste
  straw = c("wheat", "rice", "rapeseed", "reed", "maize straw"),
  grass = c("switchgrass", "miscanthus", "giant reed", "fodder grass", "grass"),
  maize = c("cob", "stover", "bagasse"),
  
  # Wood/Hardwood/Softwood
  hardwood = c("hardwood","oak", "hickory", "eucalyptus", "poplar", "walnut", "willow", "beech", "ash", "cottonwood", "mesquite"),
  softwood = c("softwood", "pine", "spruce", "fir", "cedar"),
  wood = c("wood", "mixed wood"),
  
  # Sludge/Industrial Waste
  sludge = c("sewage", "biosolids", "wastewater", "municipal sludge"),
  industrial = c("waste", "paper", "compost"),
  
  # Miscellaneous
  algae = c("algae", "spirulina", "waterweed"),
  
  # Default Fallback
  default = c("other")
)



tertiary_mapping <- list(
  agricultural = c("corn", "grass", "switchgrass", "wheat", "rice", "bagasse", "stover", "straw", "bamboo", "miscanthus"),
  `wood-based` = c("wood", "pine", "oak", "hardwood", "softwood", "chips", "sawdust", "pellets", "bark"),
  `animal-based` = c("manure", "cow", "bull", "poultry", "dairy", "sewage", "swine", "horse", "litter"),
  `food-based` = c("nutshell", "macadamia", "acorn", "walnut", "peanut", "pecan", "almond", "chestnut", "pistachio", 
                   "fruit", "pomace", "coconut", "plum", "orange", "pineapple", "apricot"),
  `industrial/other` = c("waste", "sludge", "biosolids", "trash", "paper", "pit", "stone", "kernel")
)

# # Matching function with Primary, Secondary, and Tertiary logic
match_feedstock <- function(feedstock, secondary_mapping, tertiary_mapping) {
  if (is.na(feedstock) || nchar(feedstock) == 0) {
    return(c(Primary = "Unknown", Secondary = "Unknown", Tertiary = "Unknown"))
  }
  
  # Tokenize feedstock
  tokens <- str_split(feedstock, "\\s+")[[1]]
  
  # Match Primary: Prioritize specific terms first
  primary_match <- NULL
  for (category in names(secondary_mapping)) {
    specific_matches <- intersect(tokens, secondary_mapping[[category]])
    if (length(specific_matches) > 0) {
      primary_match <- specific_matches[1]
      break
    }
  }
  primary_match <- ifelse(!is.null(primary_match), primary_match, "other")
  
  # Match Secondary: Ensure it aligns with the Primary category
  if (primary_match != "other") {
    secondary_match <- names(secondary_mapping)[sapply(secondary_mapping, function(x) primary_match %in% x)]
    secondary_match <- ifelse(length(secondary_match) > 0, secondary_match[1], "other")
  } else {
    # General fallback for Secondary
    secondary_match <- names(secondary_mapping)[sapply(secondary_mapping, function(x) any(tokens %in% x))]
    secondary_match <- ifelse(length(secondary_match) > 0, secondary_match[1], "other")
  }
  
  # Match Tertiary: Broad classification
  tertiary_match <- names(tertiary_mapping)[sapply(tertiary_mapping, function(x) any(x %in% tokens))]
  tertiary_match <- ifelse(length(tertiary_match) > 0, tertiary_match[1], "Other")
  
  return(c(Primary = primary_match, Secondary = secondary_match, Tertiary = tertiary_match))
}

# Apply the updated logic to user_data
user_data <- user_data %>%
  rowwise() %>%
  mutate(
    Match_Result = list(match_feedstock(Simplified_Feedstock, secondary_mapping, tertiary_mapping)),
    Primary_Feedstock = Match_Result[["Primary"]],
    Secondary_Feedstock = Match_Result[["Secondary"]],
    Tertiary_Feedstock = Match_Result[["Tertiary"]]
  ) %>%
  ungroup() %>%
  dplyr::select(-Match_Result)

# Apply the updated logic to davis_data
davis_data <- davis_data %>%
  rowwise() %>%
  mutate(
    Match_Result = list(match_feedstock(Simplified_Feedstock, secondary_mapping, tertiary_mapping)),
    Primary_Feedstock = Match_Result[["Primary"]],
    Secondary_Feedstock = Match_Result[["Secondary"]],
    Tertiary_Feedstock = Match_Result[["Tertiary"]]
  ) %>%
  ungroup() %>%
  dplyr::select(-Match_Result)


 user_data <- user_data %>%
    rename(Temperature = Temp_) 
    
    # Match columns for ash and total C between datasets
davis_data <- davis_data %>%
    rename(Temperature = Temperature_C_) %>% 
    mutate(
      SSN = paste0("davis_", row_number()), # Create unique SSN for davis_data
      Ash_avg = Total_Ash_Content_ / 100, # Convert percent to decimal
      C_tot_avg = Total_C_Used_in_Plot / 100
  )

# Save the datasets as Excel files
# write_xlsx(user_data, "user_data.xlsx")
# write_xlsx(davis_data, "davis_data.xlsx")

```

##### Imputing only pH from davis dataset

```{r}

# Combine user_data and davis_data
combined_data <- bind_rows(
  user_data %>%
    mutate(dataset = "user"),
  davis_data %>%
    mutate(dataset = "davis")
)

# Select only relevant columns for grouping and imputation
combined_data <- combined_data %>%
  dplyr::select(SSN, pH, Primary_Feedstock, Temperature, dataset)

# KNN Imputation with Temperature
knn_impute_with_temp <- function(data) {
  # Identify rows with missing pH
  missing_idx <- which(is.na(data$pH))
  
  # If no missing values, return original data
  if (length(missing_idx) == 0) return(data)
  
  # Split data into training and testing sets
  train <- data %>% filter(!is.na(pH) & !is.na(Temperature)) # Complete cases only
  test <- data %>% filter(is.na(pH) & !is.na(Temperature))   # Missing pH but with valid Temperature
  
  # If no valid training or testing data, skip imputation
  if (nrow(train) == 0 || nrow(test) == 0) return(data)
  
  # Dynamically adjust k
  k <- min(5, nrow(train)) # Use the smaller of 5 or the number of training rows
  
  # Apply KNN regression
  knn_result <- knn.reg(
    train = train %>% dplyr::select(Temperature) %>% as.matrix(), # Temperature as predictor
    test = test %>% dplyr::select(Temperature) %>% as.matrix(),  # Temperature as predictor
    y = train$pH,                                         # Non-missing pH values
    k = k
  )
  
  # Impute missing values
  data$pH[is.na(data$pH) & !is.na(data$Temperature)] <- knn_result$pred
  return(data)
}

# Group by Primary_Feedstock and apply KNN Imputation
imputed_data <- combined_data %>%
  group_by(Primary_Feedstock) %>%
  group_modify(~ knn_impute_with_temp(.x)) %>%
  ungroup()

# Replace user_data pH values with imputed values
user_data <- user_data %>%
  left_join(
    imputed_data %>%
      filter(dataset == "user") %>%
      dplyr::select(SSN, pH) %>% # Keep SSN to avoid duplicates
      rename(pH_imputed = pH), # Rename the imputed pH column
    by = "SSN"
  ) %>%
  mutate(pH = if_else(is.na(pH), pH_imputed, pH)) %>% # Replace missing pH values
  dplyr::select(-pH_imputed) # Drop the imputed column
```

#### PLSR - Davis Imputed

#### Tune to minimize RMSEP - MIR

selecting the number of components that minimizes the RMSEP

Davis imputed pH- Temp, feedstock

```{r}

# Replace `pH` in df1 with `pH` from user_data based on SSN
new_df <- df1 %>%
  left_join(user_data %>% dplyr::select(SSN, pH) %>% rename(pH_user_data = pH), by = "SSN") %>%
  mutate(pH = if_else(!is.na(pH_user_data), pH_user_data, pH)) %>%
  dplyr::select(-pH_user_data)  # Drop the temporary column
    
```

Total Split

```{r}
# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_Davis_pH")  # For RMSEP
run_all_properties(slprptr, new_df, spec_trt, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep")
```

Source Split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_sourcesplit_Davis_pH")  # For RMSEP
run_all_properties(slprptr, new_df, spec_trt, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep")
```

Imputing Temp, C from Davis dataset

```{r}


# Combine datasets for consistent processing
combined_data <- bind_rows(
  user_data %>% mutate(dataset = "user"),
  davis_data %>% mutate(dataset = "davis")
)

# Select only relevant columns for grouping and imputation
combined_data <- combined_data %>%
  dplyr::select(SSN, pH, Primary_Feedstock, Temperature, C_tot_avg, dataset)

knn_impute_with_multiple_predictors <- function(data) {
  # Identify rows with missing pH
  missing_idx <- which(is.na(data$pH))
  
  # If no missing values, return original data
  if (length(missing_idx) == 0) return(data)
  
  # Split data into training and testing sets
  train <- data %>% filter(!is.na(pH) & !is.na(Temperature) & !is.na(C_tot_avg)) # Complete cases only
  test <- data %>% filter(is.na(pH) & !is.na(Temperature) & !is.na(C_tot_avg))   # Missing pH but with valid predictors
  
  # If no valid training or testing data, skip imputation
  if (nrow(train) < 5 || nrow(test) == 0) return(data)
  
  # Dynamically adjust k
  k <- min(5, nrow(train))
  
  # Prepare training and test predictors
  train_predictors <- train %>% dplyr::select("Temperature", "C_tot_avg") %>% as.matrix()
  test_predictors <- test %>% dplyr::select("Temperature", "C_tot_avg") %>% as.matrix()
  
  # Apply KNN regression
  knn_result <- knn.reg(
    train = train_predictors,
    test = test_predictors,
    y = train$pH,
    k = k
  )
  
   
  # Impute missing values
  #data$pH[is.na(data$pH) & !is.na(Temperature) & !is.na(C_tot_avg)] <- knn_result$pred
  data$pH[is.na(data$pH) & !is.na(data[["Temperature"]]) & !is.na(data[["C_tot_avg"]])] <- knn_result$pred
  
   
  return(data)
}

# Group by Primary_Feedstock and apply KNN Imputation
imputed_data <- combined_data %>%
  group_by(Primary_Feedstock) %>%
  group_modify(~ knn_impute_with_multiple_predictors(.x)) %>%
  ungroup()

# Replace user_data pH values with imputed values
user_data <- user_data %>%
  left_join(
    imputed_data %>%
      filter(dataset == "user") %>%
      dplyr::select(SSN, pH) %>% # Keep SSN to avoid duplicates
      rename(pH_imputed = pH), # Rename the imputed pH column
    by = "SSN"
  ) %>%
  mutate(pH = if_else(is.na(pH), pH_imputed, pH)) %>% # Replace missing pH values
  dplyr::select(-pH_imputed) # Drop the imputed column
```

Davis imputed pH - Temp, feedstock, C,

```{r}

# Replace `pH` in df1 with `pH` from user_data based on SSN
new_df <- df1 %>%
  left_join(user_data %>% dplyr::select(SSN, pH) %>% rename(pH_user_data = pH), by = "SSN") %>%
  mutate(pH = if_else(!is.na(pH_user_data), pH_user_data, pH)) %>%
  dplyr::select(-pH_user_data)  # Drop the temporary column
    
```

Total Split

```{r}
# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_totsplit_Davis_pH_2")  # For RMSEP
run_all_properties(slprptr, new_df, spec_trt, model_type = "pls", identifier, 
                   use_source = FALSE, tuning_goal = "rmsep", property_range = 2:30)
```

Source Split

```{r}

# Run analysis for all properties
identifier <- initialize_workspace("PLSR_RMSEP_MIR_sourcesplit_Davis_pH_2")  # For RMSEP
run_all_properties(slprptr, new_df, spec_trt, model_type = "pls", identifier, 
                   use_source = TRUE, tuning_goal = "rmsep", property_range = 2:30)
```

```{r}

17.86+34.92+72.38

337+417+2250 +1200+500+1000+100+5200+390+180+650+100+200+2480+2350+300+2500+200+500+1000+1300+450+1700+1400+2000+560+1500+600+5600+3500+320+4000+300+250+4090+5200+400+2200+750+400+1800+780+200+450+470+5200+15000+3500+2000+1000+570+2000+235+500+500+270+320+760+600+600+280+180+280+1000+700+2400+1190+900+275+3500+
  
  3000+1000
```
